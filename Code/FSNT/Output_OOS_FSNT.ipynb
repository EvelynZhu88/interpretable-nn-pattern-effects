{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b7ac6b-ef11-4cab-a9da-eed6c5359983",
   "metadata": {
    "id": "52b7ac6b-ef11-4cab-a9da-eed6c5359983",
    "outputId": "cf1207b2-3a5b-46e9-b392-9224113611fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 13:31:17.247824: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-13 13:31:17.365216: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-13 13:31:17.399843: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import tensorflow as tf\n",
    "import innvestigate\n",
    "# This is required for innvestigate and must be called before models are loaded.\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense,LeakyReLU, Dropout, Add, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from matplotlib.colors import Normalize\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "tf.keras.utils.set_random_seed(0)\n",
    "tf.config.list_physical_devices('GPU')\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras.engine.training_v1\")\n",
    "import xarray as xr\n",
    "#import innvestigate\n",
    "import scipy.io as sio\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "from scipy.signal import butter, sosfilt\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16122489-4fa5-409b-bcf0-ce782264dc41",
   "metadata": {
    "id": "159320bc-7b0f-4ff1-ab55-88d95cd1f9d5",
    "outputId": "52319d43-65d8-42ac-e8aa-3075ab7d9fb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical dataset loaded: <xarray.Dataset>\n",
      "Dimensions:   (lon: 288, lat: 192, time: 181)\n",
      "Coordinates:\n",
      "  * lon       (lon) float64 0.0 1.25 2.5 3.75 5.0 ... 355.0 356.2 357.5 358.8\n",
      "  * lat       (lat) float64 -90.0 -89.06 -88.12 -87.17 ... 88.12 89.06 90.0\n",
      "  * time      (time) int64 1920 1921 1922 1923 1924 ... 2096 2097 2098 2099 2100\n",
      "Data variables:\n",
      "    TS        (time, lat, lon) float32 ...\n",
      "    TOA       (time) float64 ...\n",
      "    TOA_anom  (time) float64 ...\n",
      "    TS_anom   (time, lat, lon) float32 ...\n",
      "Attributes:\n",
      "    ERF:      ERF = F - F_ctrl; my old CAM5 forcing run at /glade/campaign/un...\n",
      "    script:   /glade/work/dongy24/Python/create_input_for_huaiyu.ipynb\n",
      "    author:   Y. Dong, 03/23/2025\n"
     ]
    }
   ],
   "source": [
    "In_name = \"TS_anom\" #\n",
    "Out_name = \"FSNT_anom\" #\n",
    "\n",
    "\n",
    "#====================== Here is 1D data ===============================#\n",
    "# Paths to the datasets\n",
    "# file_hist = \"E:\\\\Yue\\\\CESM2\\\\test\\\\test.CESM2-historical.ens-mean.ANN.1850-2014.new.nc\"\n",
    "# file_2xCO2 = \"E:\\\\Yue\\\\test\\\\test.2xCO2.ANN.new.nc\"\n",
    "# file_4xCO2 = \"E:\\\\Yue\\\\CESM2\\\\test\\\\test.CESM2-4xCO2.ANN.nc\"\n",
    "\n",
    "\n",
    "#C1 \n",
    "######### CESM1 #########\n",
    "file_hist = \"/ocean/projects/ees250004p/ezhu3/data/CESM1/test/test.historical.ens-mean.ANN.1920-2100.new.nc\"\n",
    "# file_2xCO2 = \"/ocean/projects/ees240007p/hwei1/Yue/test/test.2xCO2.ANN.new.nc\"\n",
    "file_4xCO2 = \"/ocean/projects/ees250004p/ezhu3/data/CESM1/test/test.4xCO2.FSNT.ANN.new.nc\"\n",
    "######### CESM1 #########\n",
    "'''\n",
    "######### CESM2 #########\n",
    "file_hist = \"/ocean/projects/ees250004p/ezhu3/data/CESM2/test/test.CESM2-historical.ens-mean.ANN.1850-2014.nc\"\n",
    "# file_2xCO2 = \"/ocean/projects/ees240007p/hwei1/Yue/test/test.2xCO2.ANN.new.nc\"\n",
    "file_4xCO2 = \"/ocean/projects/ees250004p/ezhu3/data/CESM2/test/test.CESM2-4xCO2.FSNT.ANN.nc\"\n",
    "######### CESM2 #########\n",
    "'''\n",
    "#====================== End of 1D data ================================#\n",
    "\n",
    "#====================== here is 2D data ===============================#\n",
    "\n",
    "#====================== End of 1D data ================================#\n",
    "\n",
    "# === Load historical data ===\n",
    "ds_hist = xr.open_dataset(file_hist, engine=\"netcdf4\")\n",
    "print(\"Historical dataset loaded:\", ds_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc21307d-79bf-4a45-a19e-3cfb7fae193d",
   "metadata": {
    "id": "159320bc-7b0f-4ff1-ab55-88d95cd1f9d5",
    "outputId": "52319d43-65d8-42ac-e8aa-3075ab7d9fb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4xCO2 dataset loaded: <xarray.Dataset>\n",
      "Dimensions:    (lat: 192, lon: 288, year: 150)\n",
      "Coordinates:\n",
      "  * lat        (lat) float64 -90.0 -89.06 -88.12 -87.17 ... 88.12 89.06 90.0\n",
      "  * lon        (lon) float64 0.0 1.25 2.5 3.75 5.0 ... 355.0 356.2 357.5 358.8\n",
      "  * year       (year) int64 1850 1851 1852 1853 1854 ... 1996 1997 1998 1999\n",
      "Data variables:\n",
      "    TS_anom    (year, lat, lon) float64 ...\n",
      "    FSNT_anom  (year) float64 ...\n",
      "Attributes:\n",
      "    script:   /glade/work/dongy24/Python/create_input_for_huaiyu.ipynb\n",
      "    note:     all fluxes are positive downward (TOA = FSNT + FLNT)\n",
      "TS_hist shape: (181, 192, 288)\n",
      "TS_4xCO2 shape: (150, 192, 288)\n"
     ]
    }
   ],
   "source": [
    "lat = ds_hist[\"lat\"]\n",
    "lon = ds_hist[\"lon\"]\n",
    "\n",
    "time_hist = ds_hist[\"time\"] \n",
    "\n",
    "######### CESM2 #########\n",
    "#time_hist = [date.year for date in time_hist.values] #for C2 data\n",
    "######### CESM2 #########\n",
    "\n",
    "######### CESM1 #########\n",
    "time_hist = time_hist.values #for C1 data\n",
    "######### CESM1 #########\n",
    "\n",
    "TS_hist = ds_hist[In_name]\n",
    "TOA_hist = ds_hist[\"TOA_anom\"].values\n",
    "\n",
    "# === Load 2xCO2 data ===\n",
    "# ds_2xCO2 = xr.open_dataset(file_2xCO2)\n",
    "# print(\"2xCO2 dataset loaded:\", ds_2xCO2)\n",
    "\n",
    "# time_2xCO2 = ds_2xCO2[\"year\"]\n",
    "# TS_2xCO2 = ds_2xCO2[In_name]\n",
    "# TOA_2xCO2 = ds_2xCO2[Out_name].values\n",
    "\n",
    "# === Load 4xCO2 data ===\n",
    "ds_4xCO2 = xr.open_dataset(file_4xCO2, engine=\"netcdf4\")\n",
    "print(\"4xCO2 dataset loaded:\", ds_4xCO2)\n",
    "\n",
    "\n",
    "# time_4xCO2_coords = ds_4xCO2[\"year\"]\n",
    "# time_4xCO2 = time_4xCO2_coords.values\n",
    "\n",
    "'''\n",
    "######### CESM2 #########\n",
    "time_4xCO2 = ds_4xCO2[\"time\"] \n",
    "time_4xCO2 = [date.year for date in time_4xCO2.values] #for C2 data\n",
    "######### CESM2 #########\n",
    "'''\n",
    "######### CESM1 #########\n",
    "time_4xCO2 = ds_4xCO2[\"year\"] \n",
    "time_4xCO2 = time_4xCO2.values #C1 data\n",
    "######### CESM1 #########\n",
    "\n",
    "\n",
    "TS_4xCO2 = ds_4xCO2[In_name]\n",
    "TOA_4xCO2 = ds_4xCO2[Out_name].values\n",
    "\n",
    "# Optional: Print shapes to verify\n",
    "print(\"TS_hist shape:\", TS_hist.shape)\n",
    "# print(\"TS_2xCO2 shape:\", TS_2xCO2.shape)\n",
    "print(\"TS_4xCO2 shape:\", TS_4xCO2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01c6f30e-fc4b-47e5-b910-ce53070c606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy import stats\n",
    "from scipy import ndimage # New import for spatial smoothing\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "\n",
    "def calculate_trend_and_significance_temporal_smooth(relevance_data, time_axis, p_value_threshold=0.05, window_size=5):\n",
    "    \"\"\"\n",
    "    Applies temporal smoothing (running mean) to relevance time series before\n",
    "    calculating the linear trend and significance.\n",
    "    'window_size' is the number of years for the running mean.\n",
    "    \"\"\"\n",
    "    if relevance_data.ndim != 3:\n",
    "        raise ValueError(\"Input relevance_data must be a 3D array [time, lat, lon].\")\n",
    "    if time_axis.ndim != 1 or time_axis.shape[0] != relevance_data.shape[0]:\n",
    "        raise ValueError(\"Time axis must be 1D and match the time dimension of data.\")\n",
    "\n",
    "    num_time, lat_dim, lon_dim = relevance_data.shape\n",
    "    trend_map = np.full((lat_dim, lon_dim), np.nan)\n",
    "    significance_mask = np.full((lat_dim, lon_dim), False, dtype=bool)\n",
    "\n",
    "    print(f\"    Calculating trends with temporal smoothing (window={window_size}) for {lat_dim}x{lon_dim} grid...\")\n",
    "    for y in range(lat_dim):\n",
    "        for x in range(lon_dim):\n",
    "            relevance_timeseries = relevance_data[:, y, x]\n",
    "            \n",
    "            if np.all(np.isfinite(relevance_timeseries)):\n",
    "                # Apply temporal smoothing using a rolling mean\n",
    "                # Using pandas for convenient rolling mean with handling of edges\n",
    "                series = pd.Series(relevance_timeseries)\n",
    "                smoothed_timeseries = series.rolling(window=window_size, center=True, min_periods=1).mean().to_numpy()\n",
    "                \n",
    "                # Filter out NaNs that might be introduced by rolling mean if window is large\n",
    "                valid_indices = ~np.isnan(smoothed_timeseries)\n",
    "                \n",
    "                if np.sum(valid_indices) > 1: # Need at least 2 points for regression\n",
    "                    current_time_axis = time_axis[valid_indices]\n",
    "                    current_data = smoothed_timeseries[valid_indices]\n",
    "                    \n",
    "                    if len(current_data) > 1: # Still need at least 2 points after NaN removal\n",
    "                        lin_reg_result = stats.linregress(x=current_time_axis, y=current_data)\n",
    "                        trend_map[y, x] = lin_reg_result.slope\n",
    "                        if lin_reg_result.pvalue < p_value_threshold:\n",
    "                            significance_mask[y, x] = True\n",
    "    print(\"    Trend and significance calculation with temporal smoothing complete.\")\n",
    "    return trend_map, significance_mask\n",
    "\n",
    "def calculate_trend_and_significance(relevance_data, time_axis, p_value_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Calculates the linear trend (slope) and a significance mask\n",
    "    at each grid cell over time.\n",
    "    \"\"\"\n",
    "    if relevance_data.ndim != 3:\n",
    "        raise ValueError(\"Input relevance_data must be a 3D array [time, lat, lon].\")\n",
    "    if time_axis.ndim != 1 or time_axis.shape[0] != relevance_data.shape[0]:\n",
    "        raise ValueError(\"Time axis must be 1D and match the time dimension of data.\")\n",
    "\n",
    "    _, lat_dim, lon_dim = relevance_data.shape\n",
    "    trend_map = np.full((lat_dim, lon_dim), np.nan)\n",
    "    significance_mask = np.full((lat_dim, lon_dim), False, dtype=bool) # Mask for significance\n",
    "\n",
    "    print(f\"    Calculating trends and significance for {lat_dim}x{lon_dim} grid...\")\n",
    "    for y in range(lat_dim):\n",
    "        for x in range(lon_dim):\n",
    "            relevance_timeseries = relevance_data[:, y, x]\n",
    "            if np.all(np.isfinite(relevance_timeseries)) and len(relevance_timeseries) > 1:\n",
    "                lin_reg_result = stats.linregress(x=time_axis, y=relevance_timeseries)\n",
    "                trend_map[y, x] = lin_reg_result.slope\n",
    "                if lin_reg_result.pvalue < p_value_threshold:\n",
    "                    significance_mask[y, x] = True\n",
    "    print(\"    Trend and significance calculation complete.\")\n",
    "    return trend_map, significance_mask\n",
    "\n",
    "def apply_spatial_smoothing(data2d, sigma=1):\n",
    "    \"\"\"\n",
    "    Applies a Gaussian filter to smooth a 2D map.\n",
    "    Sigma is the standard deviation for Gaussian kernel.\n",
    "    \"\"\"\n",
    "    print(f\"    Applying spatial smoothing with sigma={sigma}...\")\n",
    "    smoothed_map = ndimage.gaussian_filter(data2d, sigma=sigma)\n",
    "    print(\"    Smoothing complete.\")\n",
    "    return smoothed_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3c6e5e9-4e59-44fc-841a-055663c21dc4",
   "metadata": {
    "id": "d3c6e5e9-4e59-44fc-841a-055663c21dc4",
    "outputId": "231f725f-fe91-4c4c-b9cb-c9e1ab9c7e51"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray &#x27;TS_anom&#x27; (lat: 192, lon: 288)&gt;\n",
       "[55296 values with dtype=float32]\n",
       "Coordinates:\n",
       "  * lon      (lon) float64 0.0 1.25 2.5 3.75 5.0 ... 355.0 356.2 357.5 358.8\n",
       "  * lat      (lat) float64 -90.0 -89.06 -88.12 -87.17 ... 87.17 88.12 89.06 90.0\n",
       "    time     int64 1920\n",
       "Attributes:\n",
       "    units:    K\n",
       "    note:     TS_anom = TS_his - TS_ref\n",
       "    ref:      B1850C5CN mean state</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'>'TS_anom'</div><ul class='xr-dim-list'><li><span class='xr-has-index'>lat</span>: 192</li><li><span class='xr-has-index'>lon</span>: 288</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-57778533-3c04-48b2-b296-27b9b8d0ccb1' class='xr-array-in' type='checkbox' checked><label for='section-57778533-3c04-48b2-b296-27b9b8d0ccb1' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>...</span></div><div class='xr-array-data'><pre>[55296 values with dtype=float32]</pre></div></div></li><li class='xr-section-item'><input id='section-c84612bb-e0cc-4092-afa4-72bd7cc84f10' class='xr-section-summary-in' type='checkbox'  checked><label for='section-c84612bb-e0cc-4092-afa4-72bd7cc84f10' class='xr-section-summary' >Coordinates: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lon</span></div><div class='xr-var-dims'>(lon)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>0.0 1.25 2.5 ... 356.2 357.5 358.8</div><input id='attrs-bf0f8f1d-b80b-4544-a9ce-5ee18a0cf9fe' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-bf0f8f1d-b80b-4544-a9ce-5ee18a0cf9fe' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-1bd33ade-9a16-4f81-86f6-acdbfa1e691e' class='xr-var-data-in' type='checkbox'><label for='data-1bd33ade-9a16-4f81-86f6-acdbfa1e691e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>longitude</dd><dt><span>long_name :</span></dt><dd>longitude</dd><dt><span>units :</span></dt><dd>degrees_east</dd><dt><span>axis :</span></dt><dd>X</dd></dl></div><div class='xr-var-data'><pre>array([  0.  ,   1.25,   2.5 , ..., 356.25, 357.5 , 358.75])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lat</span></div><div class='xr-var-dims'>(lat)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-90.0 -89.06 -88.12 ... 89.06 90.0</div><input id='attrs-c9d5b3ac-b488-4d3f-873b-2dcfb25f0ac7' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-c9d5b3ac-b488-4d3f-873b-2dcfb25f0ac7' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-8f697529-e4a9-42b4-ab5c-00943f1eadc9' class='xr-var-data-in' type='checkbox'><label for='data-8f697529-e4a9-42b4-ab5c-00943f1eadc9' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>latitude</dd><dt><span>long_name :</span></dt><dd>latitude</dd><dt><span>units :</span></dt><dd>degrees_north</dd><dt><span>axis :</span></dt><dd>Y</dd></dl></div><div class='xr-var-data'><pre>array([-90.      , -89.057592, -88.115183, -87.172775, -86.230366, -85.287958,\n",
       "       -84.34555 , -83.403141, -82.460733, -81.518325, -80.575916, -79.633508,\n",
       "       -78.691099, -77.748691, -76.806283, -75.863874, -74.921466, -73.979058,\n",
       "       -73.036649, -72.094241, -71.151832, -70.209424, -69.267016, -68.324607,\n",
       "       -67.382199, -66.439791, -65.497382, -64.554974, -63.612565, -62.670157,\n",
       "       -61.727749, -60.78534 , -59.842932, -58.900524, -57.958115, -57.015707,\n",
       "       -56.073298, -55.13089 , -54.188482, -53.246073, -52.303665, -51.361257,\n",
       "       -50.418848, -49.47644 , -48.534031, -47.591623, -46.649215, -45.706806,\n",
       "       -44.764398, -43.82199 , -42.879581, -41.937173, -40.994764, -40.052356,\n",
       "       -39.109948, -38.167539, -37.225131, -36.282723, -35.340314, -34.397906,\n",
       "       -33.455497, -32.513089, -31.570681, -30.628272, -29.685864, -28.743455,\n",
       "       -27.801047, -26.858639, -25.91623 , -24.973822, -24.031414, -23.089005,\n",
       "       -22.146597, -21.204188, -20.26178 , -19.319372, -18.376963, -17.434555,\n",
       "       -16.492147, -15.549738, -14.60733 , -13.664921, -12.722513, -11.780105,\n",
       "       -10.837696,  -9.895288,  -8.95288 ,  -8.010471,  -7.068063,  -6.125654,\n",
       "        -5.183246,  -4.240838,  -3.298429,  -2.356021,  -1.413613,  -0.471204,\n",
       "         0.471204,   1.413613,   2.356021,   3.298429,   4.240838,   5.183246,\n",
       "         6.125654,   7.068063,   8.010471,   8.95288 ,   9.895288,  10.837696,\n",
       "        11.780105,  12.722513,  13.664921,  14.60733 ,  15.549738,  16.492147,\n",
       "        17.434555,  18.376963,  19.319372,  20.26178 ,  21.204188,  22.146597,\n",
       "        23.089005,  24.031414,  24.973822,  25.91623 ,  26.858639,  27.801047,\n",
       "        28.743455,  29.685864,  30.628272,  31.570681,  32.513089,  33.455497,\n",
       "        34.397906,  35.340314,  36.282723,  37.225131,  38.167539,  39.109948,\n",
       "        40.052356,  40.994764,  41.937173,  42.879581,  43.82199 ,  44.764398,\n",
       "        45.706806,  46.649215,  47.591623,  48.534031,  49.47644 ,  50.418848,\n",
       "        51.361257,  52.303665,  53.246073,  54.188482,  55.13089 ,  56.073298,\n",
       "        57.015707,  57.958115,  58.900524,  59.842932,  60.78534 ,  61.727749,\n",
       "        62.670157,  63.612565,  64.554974,  65.497382,  66.439791,  67.382199,\n",
       "        68.324607,  69.267016,  70.209424,  71.151832,  72.094241,  73.036649,\n",
       "        73.979058,  74.921466,  75.863874,  76.806283,  77.748691,  78.691099,\n",
       "        79.633508,  80.575916,  81.518325,  82.460733,  83.403141,  84.34555 ,\n",
       "        85.287958,  86.230366,  87.172775,  88.115183,  89.057592,  90.      ])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>time</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>1920</div><input id='attrs-77d1315c-300b-47ea-85bd-22dd1ebfcacf' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-77d1315c-300b-47ea-85bd-22dd1ebfcacf' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-ac104875-d04f-4aaa-8ccf-07d1821ec8a6' class='xr-var-data-in' type='checkbox'><label for='data-ac104875-d04f-4aaa-8ccf-07d1821ec8a6' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array(1920)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-56bb1fef-1ad5-42af-9eb4-a8c410b0ce52' class='xr-section-summary-in' type='checkbox'  ><label for='section-56bb1fef-1ad5-42af-9eb4-a8c410b0ce52' class='xr-section-summary' >Indexes: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>lon</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-7103ded9-c3b3-4d37-b86b-a484380acbe7' class='xr-index-data-in' type='checkbox'/><label for='index-7103ded9-c3b3-4d37-b86b-a484380acbe7' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Float64Index([   0.0,   1.25,    2.5,   3.75,    5.0,   6.25,    7.5,   8.75,\n",
       "                10.0,  11.25,\n",
       "              ...\n",
       "               347.5, 348.75,  350.0, 351.25,  352.5, 353.75,  355.0, 356.25,\n",
       "               357.5, 358.75],\n",
       "             dtype=&#x27;float64&#x27;, name=&#x27;lon&#x27;, length=288))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>lat</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-6e306e80-92c6-4d51-ad62-b912491588e7' class='xr-index-data-in' type='checkbox'/><label for='index-6e306e80-92c6-4d51-ad62-b912491588e7' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Float64Index([             -90.0, -89.05759162303664,  -88.1151832460733,\n",
       "              -87.17277486910994,  -86.2303664921466, -85.28795811518324,\n",
       "               -84.3455497382199, -83.40314136125654, -82.46073298429319,\n",
       "              -81.51832460732984,\n",
       "              ...\n",
       "               81.51832460732984,   82.4607329842932,  83.40314136125653,\n",
       "               84.34554973821989,  85.28795811518324,   86.2303664921466,\n",
       "               87.17277486910996,  88.11518324607329,  89.05759162303664,\n",
       "                            90.0],\n",
       "             dtype=&#x27;float64&#x27;, name=&#x27;lat&#x27;, length=192))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-4d79413d-1bc0-4730-a34a-70805e43cb10' class='xr-section-summary-in' type='checkbox'  checked><label for='section-4d79413d-1bc0-4730-a34a-70805e43cb10' class='xr-section-summary' >Attributes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>K</dd><dt><span>note :</span></dt><dd>TS_anom = TS_his - TS_ref</dd><dt><span>ref :</span></dt><dd>B1850C5CN mean state</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.DataArray 'TS_anom' (lat: 192, lon: 288)>\n",
       "[55296 values with dtype=float32]\n",
       "Coordinates:\n",
       "  * lon      (lon) float64 0.0 1.25 2.5 3.75 5.0 ... 355.0 356.2 357.5 358.8\n",
       "  * lat      (lat) float64 -90.0 -89.06 -88.12 -87.17 ... 87.17 88.12 89.06 90.0\n",
       "    time     int64 1920\n",
       "Attributes:\n",
       "    units:    K\n",
       "    note:     TS_anom = TS_his - TS_ref\n",
       "    ref:      B1850C5CN mean state"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_hist[In_name][0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb2c543b-933c-46ed-af03-f5ab9f5c76d4",
   "metadata": {
    "id": "01eeb71e-60e4-465f-91f7-412a164df9a4",
    "outputId": "d21eee23-7b2c-42fb-ded3-36fabbf75258"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.10623824e-04 -3.53298992e-06 -9.22559411e-05 ...  3.87442778e-05\n",
      "  -2.56544208e-05 -1.04447725e-04]\n",
      " [ 1.70041036e-04 -1.25976090e-04  2.52858736e-04 ...  4.71912563e-06\n",
      "   2.21883631e-04  8.57152554e-05]\n",
      " [ 3.45589127e-04 -1.95924207e-04 -4.28872809e-05 ... -1.06506515e-04\n",
      "  -1.65177873e-04  1.70413812e-04]\n",
      " ...\n",
      " [-1.88519334e-04 -5.27491502e-05 -1.12945541e-04 ...  2.52350379e-04\n",
      "  -4.53866378e-05  2.37345768e-04]\n",
      " [ 2.46504438e-04 -1.04337581e-04  1.80885709e-05 ...  2.75547791e-04\n",
      "   1.28102663e-04 -3.28729038e-06]\n",
      " [-1.15351701e-04 -7.93524814e-05 -8.38174383e-05 ... -1.90018953e-04\n",
      "  -2.44818424e-04 -1.57874674e-04]]\n",
      "[[0.53236276 0.5272246  0.52714384 ... 0.52715605 0.5320779  0.5270208 ]\n",
      " [0.5789423  0.5785266  0.5860141  ... 0.5794204  0.58237875 0.583376  ]\n",
      " [0.6280227  0.61705714 0.613267   ... 0.6247527  0.6240835  0.6179534 ]\n",
      " ...\n",
      " [1.0660722  1.0682158  1.0702783  ... 1.060526   1.0623665  1.06411   ]\n",
      " [1.0350558  1.0358475  1.0366569  ... 1.0323555  1.033299   1.0342044 ]\n",
      " [1.0114969  1.0116085  1.0117124  ... 1.0110848  1.0112363  1.0113714 ]]\n"
     ]
    }
   ],
   "source": [
    "### load the mean and STD calculated in the Pre-industrial control run for normalization\n",
    "\n",
    "# Paths to the datasets and trained model\n",
    "\n",
    "\n",
    "########### CESM1 Model ##########\n",
    "#path_PIc = \"/ocean/projects/ees250004p/ezhu3/data/CESM1/trained_model/NeuralNet/CNN_DivideSTD_Neur32x32_BS32_5foldCV_Reg0Drop0.25_relu+elu/TOA_anom\" #original model\n",
    "\n",
    "path_PIc = \"/ocean/projects/ees250004p/ezhu3/data/CESM1/trained_for_FSNT/NeuralNet/CNN_StandardScaler_Neur32x32_BS32_5foldCV_Reg0Drop0.25_gelu+PRelu/FSNT\"\n",
    "#new model(1) -wrong one\n",
    "#path_PIc = \"/ocean/projects/ees250004p/ezhu3/data/CESM2/trained_model_data2/NeuralNet/CNN_DivideSTD_Neur32x32_BS32_5foldCV_Reg0Drop0.25_relu+elu/TOA_anom\" #wrong place but the model is for data 1\n",
    "######### CESM1 #########\n",
    "\n",
    "########## CESM2 Model ######\n",
    "#path_PIc = \"/ocean/projects/ees250004p/ezhu3/data/CESM2/trained_for_FSNT/NeuralNet/CNN_StandardScaler_Neur32x32_BS32_5foldCV_Reg0Drop0.25_gelu+PRelu/FSNT_anom\"\n",
    "######### CESM2 #########\n",
    "\n",
    "normalization= sio.loadmat(os.path.join(path_PIc, 'Normalization.mat'))\n",
    "\n",
    "X_mean = normalization['X_mean']\n",
    "X_std = normalization['X_std']\n",
    "y_mean = normalization['y_mean']\n",
    "y_std = normalization['y_std']\n",
    "\n",
    "\n",
    "TS_hist_norm = (TS_hist-X_mean)/X_std.squeeze()\n",
    "# TS_2xCO2_norm = (TS_2xCO2-X_mean)/X_std.squeeze()\n",
    "TS_4xCO2_norm = (TS_4xCO2-X_mean)/X_std.squeeze()\n",
    "\n",
    "TS_hist_norm = TS_hist_norm.values[..., tf.newaxis]\n",
    "# TS_2xCO2_norm = TS_2xCO2_norm.values[..., tf.newaxis]\n",
    "TS_4xCO2_norm = TS_4xCO2_norm.values[..., tf.newaxis]\n",
    "\n",
    "print(X_mean)\n",
    "print(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f46c5f7e-d97a-4292-b5a2-802a8623e715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 13:31:21.958452: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-13 13:31:22.402091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 195 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b3:00.0, compute capability: 7.0\n",
      "2025-08-13 13:31:22.408691: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2025-08-13 13:31:32.880443: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 216.00MiB (rounded to 226492416)requested by op conv2d/Conv2D\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-08-13 13:31:32.880491: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] BFCAllocator dump for GPU_0_bfc\n",
      "2025-08-13 13:31:32.880508: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (256): \tTotal Chunks: 50, Chunks in use: 50. 12.5KiB allocated for chunks. 12.5KiB in use in bin. 2.4KiB client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880514: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (512): \tTotal Chunks: 5, Chunks in use: 5. 2.5KiB allocated for chunks. 2.5KiB in use in bin. 2.5KiB client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880518: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1024): \tTotal Chunks: 6, Chunks in use: 6. 7.5KiB allocated for chunks. 7.5KiB in use in bin. 6.6KiB client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880523: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880527: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880531: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880535: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880540: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (32768): \tTotal Chunks: 5, Chunks in use: 5. 180.0KiB allocated for chunks. 180.0KiB in use in bin. 180.0KiB client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880544: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880548: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880551: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880555: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880558: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880562: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880566: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4194304): \tTotal Chunks: 6, Chunks in use: 6. 40.50MiB allocated for chunks. 40.50MiB in use in bin. 40.50MiB client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880570: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880573: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880579: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880582: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880586: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 0. 154.74MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880590: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-13 13:31:32.880596: I tensorflow/core/common_runtime/bfc_allocator.cc:1056] Bin for 216.00MiB was 128.00MiB, Chunk State: \n",
      "2025-08-13 13:31:32.880605: I tensorflow/core/common_runtime/bfc_allocator.cc:1062]   Size: 154.74MiB | Requested Size: 0B | in_use: 0 | bin_num: 19, prev:   Size: 256B | Requested Size: 64B | in_use: 1 | bin_num: -1\n",
      "2025-08-13 13:31:32.880608: I tensorflow/core/common_runtime/bfc_allocator.cc:1069] Next region of size 204931072\n",
      "2025-08-13 13:31:32.880615: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6000000 of size 1280 next 1\n",
      "2025-08-13 13:31:32.880639: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6000500 of size 256 next 2\n",
      "2025-08-13 13:31:32.880642: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6000600 of size 1280 next 3\n",
      "2025-08-13 13:31:32.880645: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6000b00 of size 36864 next 4\n",
      "2025-08-13 13:31:32.880648: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6009b00 of size 256 next 5\n",
      "2025-08-13 13:31:32.880651: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6009c00 of size 256 next 6\n",
      "2025-08-13 13:31:32.880654: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6009d00 of size 512 next 7\n",
      "2025-08-13 13:31:32.880657: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6009f00 of size 256 next 8\n",
      "2025-08-13 13:31:32.880660: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae600a000 of size 256 next 9\n",
      "2025-08-13 13:31:32.880663: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae600a100 of size 256 next 10\n",
      "2025-08-13 13:31:32.880666: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae600a200 of size 256 next 11\n",
      "2025-08-13 13:31:32.880669: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae600a300 of size 7077888 next 12\n",
      "2025-08-13 13:31:32.880672: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae66ca300 of size 256 next 13\n",
      "2025-08-13 13:31:32.880675: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae66ca400 of size 256 next 14\n",
      "2025-08-13 13:31:32.880677: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae66ca500 of size 256 next 15\n",
      "2025-08-13 13:31:32.880680: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae66ca600 of size 256 next 16\n",
      "2025-08-13 13:31:32.880683: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae66ca700 of size 512 next 17\n",
      "2025-08-13 13:31:32.880686: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae66ca900 of size 1280 next 18\n",
      "2025-08-13 13:31:32.880689: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae66cae00 of size 256 next 19\n",
      "2025-08-13 13:31:32.880692: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae66caf00 of size 36864 next 20\n",
      "2025-08-13 13:31:32.880694: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae66d3f00 of size 7077888 next 21\n",
      "2025-08-13 13:31:32.880698: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6d93f00 of size 256 next 22\n",
      "2025-08-13 13:31:32.880701: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6d94000 of size 256 next 23\n",
      "2025-08-13 13:31:32.880704: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6d94100 of size 256 next 24\n",
      "2025-08-13 13:31:32.880707: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6d94200 of size 256 next 25\n",
      "2025-08-13 13:31:32.880709: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6d94300 of size 256 next 26\n",
      "2025-08-13 13:31:32.880712: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6d94400 of size 256 next 27\n",
      "2025-08-13 13:31:32.880715: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6d94500 of size 1280 next 28\n",
      "2025-08-13 13:31:32.880718: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6d94a00 of size 256 next 29\n",
      "2025-08-13 13:31:32.880721: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6d94b00 of size 36864 next 30\n",
      "2025-08-13 13:31:32.880723: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6d9db00 of size 256 next 31\n",
      "2025-08-13 13:31:32.880727: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6d9dc00 of size 256 next 32\n",
      "2025-08-13 13:31:32.880729: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae6d9dd00 of size 7077888 next 33\n",
      "2025-08-13 13:31:32.880732: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae745dd00 of size 256 next 34\n",
      "2025-08-13 13:31:32.880735: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae745de00 of size 512 next 35\n",
      "2025-08-13 13:31:32.880738: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae745e000 of size 256 next 36\n",
      "2025-08-13 13:31:32.880741: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae745e100 of size 256 next 37\n",
      "2025-08-13 13:31:32.880743: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae745e200 of size 256 next 38\n",
      "2025-08-13 13:31:32.880746: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae745e300 of size 256 next 39\n",
      "2025-08-13 13:31:32.880749: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae745e400 of size 256 next 40\n",
      "2025-08-13 13:31:32.880752: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae745e500 of size 256 next 41\n",
      "2025-08-13 13:31:32.880755: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae745e600 of size 256 next 42\n",
      "2025-08-13 13:31:32.880758: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae745e700 of size 512 next 43\n",
      "2025-08-13 13:31:32.880761: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae745e900 of size 256 next 44\n",
      "2025-08-13 13:31:32.880763: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae745ea00 of size 256 next 45\n",
      "2025-08-13 13:31:32.880766: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae745eb00 of size 256 next 46\n",
      "2025-08-13 13:31:32.880769: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae745ec00 of size 256 next 47\n",
      "2025-08-13 13:31:32.880772: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae745ed00 of size 512 next 48\n",
      "2025-08-13 13:31:32.880774: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae745ef00 of size 36864 next 49\n",
      "2025-08-13 13:31:32.880777: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae7467f00 of size 7077888 next 50\n",
      "2025-08-13 13:31:32.880780: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae7b27f00 of size 256 next 51\n",
      "2025-08-13 13:31:32.880783: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae7b28000 of size 256 next 52\n",
      "2025-08-13 13:31:32.880786: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae7b28100 of size 256 next 53\n",
      "2025-08-13 13:31:32.880789: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae7b28200 of size 256 next 54\n",
      "2025-08-13 13:31:32.880792: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae7b28300 of size 1280 next 55\n",
      "2025-08-13 13:31:32.880794: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae7b28800 of size 256 next 56\n",
      "2025-08-13 13:31:32.880797: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae7b28900 of size 256 next 57\n",
      "2025-08-13 13:31:32.880800: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae7b28a00 of size 256 next 58\n",
      "2025-08-13 13:31:32.880803: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae7b28b00 of size 1280 next 59\n",
      "2025-08-13 13:31:32.880806: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae7b29000 of size 256 next 60\n",
      "2025-08-13 13:31:32.880808: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae7b29100 of size 256 next 61\n",
      "2025-08-13 13:31:32.880811: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae7b29200 of size 7077888 next 62\n",
      "2025-08-13 13:31:32.880814: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae81e9200 of size 256 next 63\n",
      "2025-08-13 13:31:32.880817: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae81e9300 of size 36864 next 64\n",
      "2025-08-13 13:31:32.880820: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae81f2300 of size 256 next 65\n",
      "2025-08-13 13:31:32.880824: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae81f2400 of size 256 next 66\n",
      "2025-08-13 13:31:32.880826: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae81f2500 of size 256 next 67\n",
      "2025-08-13 13:31:32.880829: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae81f2600 of size 256 next 68\n",
      "2025-08-13 13:31:32.880832: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae81f2700 of size 256 next 69\n",
      "2025-08-13 13:31:32.880835: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae81f2800 of size 7077888 next 70\n",
      "2025-08-13 13:31:32.880837: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae88b2800 of size 256 next 71\n",
      "2025-08-13 13:31:32.880840: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 153ae88b2900 of size 256 next 72\n",
      "2025-08-13 13:31:32.880843: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 153ae88b2a00 of size 162256384 next 18446744073709551615\n",
      "2025-08-13 13:31:32.880846: I tensorflow/core/common_runtime/bfc_allocator.cc:1094]      Summary of in-use Chunks by size: \n",
      "2025-08-13 13:31:32.880851: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 50 Chunks of size 256 totalling 12.5KiB\n",
      "2025-08-13 13:31:32.880855: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 5 Chunks of size 512 totalling 2.5KiB\n",
      "2025-08-13 13:31:32.880858: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 6 Chunks of size 1280 totalling 7.5KiB\n",
      "2025-08-13 13:31:32.880861: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 5 Chunks of size 36864 totalling 180.0KiB\n",
      "2025-08-13 13:31:32.880865: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 6 Chunks of size 7077888 totalling 40.50MiB\n",
      "2025-08-13 13:31:32.880869: I tensorflow/core/common_runtime/bfc_allocator.cc:1101] Sum Total of in-use chunks: 40.70MiB\n",
      "2025-08-13 13:31:32.880872: I tensorflow/core/common_runtime/bfc_allocator.cc:1103] total_region_allocated_bytes_: 204931072 memory_limit_: 204931072 available bytes: 0 curr_region_allocation_bytes_: 409862144\n",
      "2025-08-13 13:31:32.880879: I tensorflow/core/common_runtime/bfc_allocator.cc:1109] Stats: \n",
      "Limit:                       204931072\n",
      "InUse:                        42674688\n",
      "MaxInUse:                     42674688\n",
      "NumAllocs:                          72\n",
      "MaxAllocSize:                  7077888\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-08-13 13:31:32.880888: W tensorflow/core/common_runtime/bfc_allocator.cc:491] *********************_______________________________________________________________________________\n",
      "2025-08-13 13:31:32.880920: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at conv_ops.cc:686 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,32,192,288] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,32,192,288] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d/Conv2D}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[dense_2/BiasAdd/_283]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,32,192,288] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d/Conv2D}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()  \n\u001b[1;32m     20\u001b[0m Model \u001b[38;5;241m=\u001b[39m load_model(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_PIc,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_fold\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(fold_no)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_ens\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(ens_no)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 21\u001b[0m pred_hist \u001b[38;5;241m=\u001b[39m pred_hist\u001b[38;5;241m+\u001b[39m \u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTS_hist_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# pred_2xCO2 = pred_2xCO2+ Model.predict(TS_2xCO2_norm)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m pred_4xCO2 \u001b[38;5;241m=\u001b[39m pred_4xCO2\u001b[38;5;241m+\u001b[39m Model\u001b[38;5;241m.\u001b[39mpredict(TS_4xCO2_norm)\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training_v1.py:1058\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1057\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m-> 1058\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py:801\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.predict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    797\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_validate_or_infer_batch_size(batch_size, steps, x)\n\u001b[1;32m    798\u001b[0m x, _, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_standardize_user_data(\n\u001b[1;32m    799\u001b[0m     x, check_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, steps_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, steps\u001b[38;5;241m=\u001b[39msteps\n\u001b[1;32m    800\u001b[0m )\n\u001b[0;32m--> 801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredict_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py:419\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m callbacks\u001b[38;5;241m.\u001b[39m_call_batch_hook(\n\u001b[1;32m    415\u001b[0m     mode, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_index, batch_logs\n\u001b[1;32m    416\u001b[0m )\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Get outputs.\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m batch_outs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_outs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    421\u001b[0m     batch_outs \u001b[38;5;241m=\u001b[39m [batch_outs]\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/keras/backend.py:4577\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4568\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4569\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m feed_arrays \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_arrays\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4573\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m session \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\n\u001b[1;32m   4574\u001b[0m ):\n\u001b[1;32m   4575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[0;32m-> 4577\u001b[0m fetched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marray_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches) :])\n\u001b[1;32m   4579\u001b[0m output_structure \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[1;32m   4580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_structure,\n\u001b[1;32m   4581\u001b[0m     fetched[: \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs)],\n\u001b[1;32m   4582\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   4583\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/tensorflow/python/client/session.py:1481\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1480\u001b[0m   run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1481\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRunCallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1484\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m   1485\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,32,192,288] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d/Conv2D}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[dense_2/BiasAdd/_283]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,32,192,288] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d/Conv2D}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "# load the neural network trained from Pre-industrial control simulation\n",
    "\n",
    "n_folds = 5\n",
    "n_ensembles = 1\n",
    "\n",
    "pred_hist_ALL = np.empty((TOA_hist.shape[0],0))\n",
    "# pred_2xCO2_ALL = np.empty((TOA_2xCO2.shape[0],0))\n",
    "pred_4xCO2_ALL = np.empty((TOA_4xCO2.shape[0],0))\n",
    "\n",
    "\n",
    "for fold_no in range(1,n_folds+1):\n",
    "\n",
    "    pred_hist = np.zeros((TOA_hist.shape[0],1));\n",
    "    # pred_2xCO2 = np.zeros((TOA_2xCO2.shape[0],1));\n",
    "    pred_4xCO2 = np.zeros((TOA_4xCO2.shape[0],1));\n",
    "    for ens_no in range(1,n_ensembles+1):\n",
    "        K.clear_session() # Clears the Keras session and TensorFlow graph\n",
    "        gc.collect()  \n",
    "        Model = load_model(os.path.join(path_PIc,'model_fold'+str(fold_no)+'_ens'+str(ens_no)+'.h5'))\n",
    "        pred_hist = pred_hist+ Model.predict(TS_hist_norm)\n",
    "        # pred_2xCO2 = pred_2xCO2+ Model.predict(TS_2xCO2_norm)\n",
    "        pred_4xCO2 = pred_4xCO2+ Model.predict(TS_4xCO2_norm)\n",
    "        \n",
    "    pred_hist = pred_hist/n_ensembles\n",
    "    # pred_2xCO2 = pred_2xCO2/n_ensembles\n",
    "    pred_4xCO2 = pred_4xCO2/n_ensembles\n",
    "\n",
    "\n",
    "# re-scale the prediction from the neural network\n",
    "    pred_hist = pred_hist*y_std + y_mean\n",
    "    # pred_2xCO2 = pred_2xCO2*y_std + y_mean\n",
    "    pred_4xCO2 = pred_4xCO2*y_std + y_mean\n",
    "\n",
    "\n",
    "\n",
    "    pred_hist_ALL = np.concatenate(( pred_hist_ALL,pred_hist),axis = 1)\n",
    "    # pred_2xCO2_ALL = np.concatenate(( pred_2xCO2_ALL,pred_2xCO2),axis = 1)\n",
    "    pred_4xCO2_ALL = np.concatenate(( pred_4xCO2_ALL,pred_4xCO2),axis = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad86458-8695-4a62-bf88-0a575e084d94",
   "metadata": {
    "id": "5ad86458-8695-4a62-bf88-0a575e084d94"
   },
   "outputs": [],
   "source": [
    "def plot_predictions(time, truth, preds, title):\n",
    "    \"\"\"\n",
    "    Plots the ground truth, mean prediction, and individual model predictions with transparency.\n",
    "\n",
    "    Parameters:\n",
    "        time (array-like): Time axis values.\n",
    "        truth (array-like): Ground truth values.\n",
    "        preds (2D array): Predictions from ensemble or multiple models (shape: time x ensemble).\n",
    "        title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot truth\n",
    "    ax.plot(time, truth, label=\"Truth\", color=\"C0\", linewidth=2)\n",
    "\n",
    "    # Plot mean prediction\n",
    "    mean_pred = np.mean(preds, axis=1)\n",
    "    ax.plot(time, mean_pred, label=\"Prediction (Mean)\", color=\"C1\", linewidth=1.8)\n",
    "\n",
    "    # Plot individual models\n",
    "    for i in range(preds.shape[1]):\n",
    "        ax.plot(time, preds[:, i], linewidth=1, alpha=0.5,\n",
    "                label=\"Prediction (CV\" + str(i+1) + \")\")\n",
    "\n",
    "    # Style\n",
    "    ax.set_xlabel(\"Time (Year)\", fontsize=16)\n",
    "    ax.set_ylabel(Out_name, fontsize=16)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    ax.tick_params(axis='both', labelsize=14)\n",
    "    ax.set_title(title, fontsize=18, pad=15)\n",
    "    ax.legend(fontsize=14, loc=\"best\")\n",
    "\n",
    "    # R² annotation\n",
    "    r2 = r2_score(truth, mean_pred)\n",
    "    ax.text(0.02, 0.95, f\"$R^2$ = {r2:.3f}\", transform=ax.transAxes,\n",
    "            fontsize=16, bbox=dict(facecolor=\"white\", edgecolor=\"black\", alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3229ce33-21a2-44d6-b874-9d4e33ef0400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def plot_predictions_twin_axes(time, truth, preds, title):\n",
    "    \"\"\"\n",
    "    Plots the ground truth and predictions on separate y-axes with a combined legend.\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(16, 8)) # Increased size for better legend visibility\n",
    "\n",
    "    # --- Left Y-Axis (for Truth) ---\n",
    "    color = 'C0'\n",
    "    ax1.set_xlabel(\"Time (Year)\", fontsize=16)\n",
    "    ax1.set_ylabel(\"Truth (FSNT_anom)\", color=color, fontsize=16)\n",
    "    # Plot truth and store its handle for the legend\n",
    "    line1 = ax1.plot(time, truth, label=\"Truth\", color=color, linewidth=2.5)\n",
    "    ax1.tick_params(axis='y', labelcolor=color, labelsize=14)\n",
    "    ax1.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    # --- Right Y-Axis (for Predictions) ---\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'C1'\n",
    "    ax2.set_ylabel(\"Prediction\", color=color, fontsize=16)\n",
    "    mean_pred = np.mean(preds, axis=1)\n",
    "    # Plot mean prediction and store its handle\n",
    "    line2 = ax2.plot(time, mean_pred, label=\"Prediction (Mean)\", color=color, linewidth=2)\n",
    "    ax2.tick_params(axis='y', labelcolor=color, labelsize=14)\n",
    "\n",
    "    # Plot individual models and store their handles\n",
    "    other_lines = []\n",
    "    for i in range(preds.shape[1]):\n",
    "        line = ax2.plot(time, preds[:, i], linewidth=1, alpha=0.3,\n",
    "                        label=f\"Prediction (CV{i+1})\")\n",
    "        other_lines.extend(line)\n",
    "\n",
    "    # --- Combined Legend ---\n",
    "    # Combine all the line handles and labels into one legend\n",
    "    all_lines = line1 + line2 + other_lines\n",
    "    labels = [l.get_label() for l in all_lines]\n",
    "    ax1.legend(all_lines, labels, loc='lower right', fontsize=12)\n",
    "\n",
    "    # --- Title and R² Annotation ---\n",
    "    truth_flat = truth.flatten()\n",
    "    pred_flat = mean_pred.flatten()\n",
    "    correlation_score, _ = stats.pearsonr(truth, mean_pred)\n",
    "\n",
    "    ax1.text(0.02, 0.95, f\"Correlation = {correlation_score:.3f}\", transform=ax1.transAxes,\n",
    "            fontsize=16, bbox=dict(facecolor=\"white\", edgecolor=\"black\", alpha=0.5))\n",
    "\n",
    "    \n",
    "        \n",
    "    ax1.set_title(title, fontsize=20, pad=15)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9856d00-64d8-4981-b647-a8d3d49a2d2d",
   "metadata": {
    "id": "a20ad84f-2a7e-4711-abf9-bb2634c91652",
    "outputId": "26709b22-34b3-41e6-f587-5a724d1f62e4"
   },
   "outputs": [],
   "source": [
    "# plot_predictions(time_hist, TOA_hist, pred_hist_ALL,\n",
    "#                  \"Testing the Neural Network in the Historical Run\")\n",
    "\n",
    "\n",
    "plot_predictions(time_4xCO2, TOA_4xCO2, pred_4xCO2_ALL,\n",
    "                 \"Testing the Neural Network in the 4xCO2 Run\")\n",
    "\n",
    "\n",
    "plot_predictions_twin_axes(time_4xCO2, TOA_4xCO2, pred_4xCO2_ALL,\n",
    "                \"Testing the Neural Network in the 4xCO2 Run (FSNT)\")\n",
    "\n",
    "# print(f\"true data: {TOA_4xCO2.shape[0]}\")\n",
    "# print(f\"prediction: {pred_4xCO2_ALL.shape[0]}\")\n",
    "\n",
    "# # # Create the plot\n",
    "# # plt.figure(figsize=(12, 6))\n",
    "# # plt.plot(time_4xCO2, TOA_4xCO2, label='true data')\n",
    "\n",
    "# # plt.figure(figsize=(12,6))\n",
    "# # plt.plot(time_4xCO2, pred_4xCO2_ALL, label='predic data')\n",
    "\n",
    "# fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# # Plot the \"Truth\" data on the left y-axis\n",
    "# color = 'tab:blue'\n",
    "# ax1.set_xlabel('Time (Year)')\n",
    "# ax1.set_ylabel('True Data (FSNT_anom)', color=color)\n",
    "# ax1.plot(time_4xCO2, TOA_4xCO2, label='True Data', color=color)\n",
    "# ax1.tick_params(axis='y', labelcolor=color)\n",
    "# ax1.grid(True, linestyle='--')\n",
    "\n",
    "# # Create the second y-axis (ax2) that shares the x-axis\n",
    "# ax2 = ax1.twinx()\n",
    "\n",
    "# # Plot the \"Prediction\" data on the right y-axis\n",
    "# color = 'tab:orange'\n",
    "# ax2.set_ylabel('Prediction', color=color)\n",
    "# ax2.plot(time_4xCO2, pred_4xCO2_ALL, label='Prediction', color=color)\n",
    "# ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# # Add a title and show the plot\n",
    "# plt.title('True Data vs. Prediction')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e187b84-4d59-4c61-a011-014670ae34c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ==========================================================\n",
    "# ANALYSIS: Sum of Predicted FLNT + FSNT vs. True TOA (Corrected)\n",
    "# ==========================================================\n",
    "print(\"\\n--- Starting new task: Sum of predictions vs. TOA ---\")\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.models import load_model\n",
    "import xarray as xr\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# --- 1. Configuration: Corrected Paths ---\n",
    "\n",
    "# === Paths for the CESM2 4xCO2 Scenario ===\n",
    "\n",
    "# This is the test file containing the input data for the FLNT prediction\n",
    "path_test_input_FLNT = \"/ocean/projects/ees250004p/ezhu3/data/CESM2/test/test.CESM2-4xCO2.FLNT.ANN.nc\"\n",
    "\n",
    "# This is the separate test file containing the input data for the FSNT prediction\n",
    "path_test_input_FSNT = \"/ocean/projects/ees250004p/ezhu3/data/CESM2/test/test.CESM2-4xCO2.FSNT.ANN.nc\"\n",
    "\n",
    "# This is the reference file containing the GROUND TRUTH TOA_anom to compare against.\n",
    "# This should be your general 4xCO2 test file.\n",
    "path_test_truth_TOA = \"/ocean/projects/ees250004p/ezhu3/data/CESM2/test/test.CESM2-4xCO2.ANN.nc\"\n",
    "\n",
    "# Path to the directory where your TRAINED FLNT MODEL is saved\n",
    "path_model_FLNT = \"/ocean/projects/ees250004p/ezhu3/data/CESM2/trained_for_FLNT/NeuralNet/CNN_StandardScaler_Neur32x32_BS32_5foldCV_Reg0Drop0.25_gelu+PRelu/FLNT\"\n",
    "\n",
    "# Path to the directory where your TRAINED FSNT MODEL is saved\n",
    "path_model_FSNT = \"/ocean/projects/ees250004p/ezhu3/data/CESM2/trained_for_FSNT/NeuralNet/CNN_StandardScaler_Neur32x32_BS32_5foldCV_Reg0Drop0.25_gelu+PRelu/FSNT\"\n",
    "\n",
    "\n",
    "# --- 2. Helper Function to Get Predictions ---\n",
    "def get_prediction(model_path, input_data_path, input_var_name=\"TS_anom\"):\n",
    "    \"\"\"\n",
    "    Loads a model and its normalization data to make a prediction.\n",
    "    \"\"\"\n",
    "    # Load normalization data\n",
    "    norm_path = os.path.join(model_path, 'Normalization.mat')\n",
    "    normalization = sio.loadmat(norm_path)\n",
    "    X_mean = normalization['X_mean']\n",
    "    X_std = normalization['X_std']\n",
    "    y_mean = normalization['y_mean']\n",
    "    y_std = normalization['y_std']\n",
    "    \n",
    "    # --- THIS IS THE FIX ---\n",
    "    # Ensure X_mean and X_std have a channel dimension to match the input data\n",
    "    if X_mean.ndim == 2:\n",
    "        X_mean = X_mean[..., np.newaxis] # Shape becomes (192, 288, 1)\n",
    "        X_std = X_std[..., np.newaxis]   # Shape becomes (192, 288, 1)\n",
    "    # ----------------------\n",
    "    \n",
    "    # Load and normalize input data\n",
    "    ds_X = xr.open_dataset(input_data_path)\n",
    "    input_raw = ds_X[input_var_name]\n",
    "    input_norm = (input_raw.values[..., np.newaxis] - X_mean) / X_std\n",
    "    \n",
    "    # Prediction loop\n",
    "    predictions_from_folds = []\n",
    "    for fold_no in range(1, 6): # Assuming 5 folds\n",
    "        K.clear_session(); gc.collect()\n",
    "        model = load_model(os.path.join(model_path, f'model_fold{fold_no}_ens1.h5'))\n",
    "        pred_norm = model.predict(input_norm)\n",
    "        pred_unnorm = pred_norm * y_std + y_mean\n",
    "        predictions_from_folds.append(pred_unnorm)\n",
    "        \n",
    "    # Return the mean prediction across folds\n",
    "    return np.mean(np.hstack(predictions_from_folds), axis=1)\n",
    "\n",
    "# --- 3. Run Analysis for 4xCO2 Scenario ---\n",
    "print(\"\\n--- Processing 4xCO2 Scenario ---\")\n",
    "\n",
    "# Get prediction for FLNT using the FLNT test file\n",
    "print(\"    Getting FLNT prediction...\")\n",
    "pred_FLNT = get_prediction(path_model_FLNT, path_test_input_FLNT)\n",
    "\n",
    "# Get prediction for FSNT using the FSNT test file\n",
    "print(\"    Getting FSNT prediction...\")\n",
    "pred_FSNT = get_prediction(path_model_FSNT, path_test_input_FSNT)\n",
    "\n",
    "# Sum the predictions\n",
    "sum_of_preds = pred_FLNT + pred_FSNT\n",
    "\n",
    "# Load the ground truth TOA_anom from the reference file\n",
    "print(\"    Loading ground truth TOA_anom...\")\n",
    "ds_toa = xr.open_dataset(path_test_truth_TOA)\n",
    "truth_TOA = ds_toa[\"TOA_anom\"].values\n",
    "time_axis = ds_toa[\"year\"].values if \"year\" in ds_toa else ds_toa[\"time\"].values\n",
    "\n",
    "# --- 4. Plot the Comparison (Corrected) ---\n",
    "print(\"\\n--- Plotting Comparison ---\")\n",
    "\n",
    "# Calculate R-squared score\n",
    "r2 = r2_score(truth_TOA, sum_of_preds)\n",
    "\n",
    "# === THIS IS THE FIX ===\n",
    "# Instead of using the complex time objects, we create a simple numerical axis.\n",
    "# This will be an array like [0, 1, 2, ...] that has the correct length.\n",
    "time_axis_for_plot = np.arange(len(truth_TOA))\n",
    "# ======================\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Use the new, simple time axis for plotting\n",
    "ax.plot(time_axis_for_plot, truth_TOA, label=\"Ground Truth TOA_anom\", color=\"k\", linewidth=2.5)\n",
    "ax.plot(time_axis_for_plot, sum_of_preds, label=\"Sum of Predictions (FSNT + FLNT)\", color=\"C3\", linestyle='--')\n",
    "\n",
    "ax.set_xlabel(\"Time (Model Years)\", fontsize=16) # Updated label for clarity\n",
    "ax.set_ylabel(\"TOA Anomaly (W/m²)\", fontsize=16)\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "ax.tick_params(axis='both', labelsize=14)\n",
    "ax.set_title(\"Physical Consistency Check: Predicted Components vs. Total\", fontsize=18, pad=15)\n",
    "\n",
    "ax.text(0.02, 0.95, f\"$R^2$ = {r2:.3f}\", transform=ax.transAxes,\n",
    "        fontsize=16, bbox=dict(facecolor=\"white\", edgecolor=\"black\", alpha=0.7))\n",
    "\n",
    "ax.legend(fontsize=14, loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5154bfd-1bcb-45b9-afe5-6065373821c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pacific_centered_map(lon, lat, data2d, title, cbar_label=\"Relevance\"):\n",
    "    \"\"\"\n",
    "    Plots a 2D global map using Cartopy, centered on the Pacific Ocean,\n",
    "    with a tight, percentile-based color scale.\n",
    "    \"\"\"\n",
    "    # Set color scale limit based on the 98th percentile to see patterns clearly\n",
    "    vmax = np.nanpercentile(np.abs(data2d), 95)\n",
    "    vmin = -vmax\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6),\n",
    "                           subplot_kw=dict(projection=ccrs.PlateCarree(central_longitude=180)))\n",
    "    \n",
    "    pcm = ax.pcolormesh(lon, lat, data2d, cmap=\"RdBu_r\", transform=ccrs.PlateCarree(),\n",
    "                        vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    ax.coastlines(zorder=2)\n",
    "    gl = ax.gridlines(draw_labels=True, linestyle=\"--\", alpha=0.7)\n",
    "    gl.top_labels = False; gl.right_labels = False\n",
    "    \n",
    "    cbar = plt.colorbar(pcm, ax=ax, orientation=\"vertical\", pad=0.03, shrink=0.85)\n",
    "    cbar.set_label(cbar_label, fontsize=14)\n",
    "    ax.set_title(title, fontsize=16, pad=15)\n",
    "    plt.show()\n",
    "\n",
    "def ensemble_lrp_analyze(model_dir, num_folds, data_to_analyze):\n",
    "    \"\"\"\n",
    "    Loads each model in an ensemble, generates its LRP map, and returns\n",
    "    the averaged map from all models.\n",
    "    \"\"\"\n",
    "    all_relevance_maps = []\n",
    "    for i in range(1, num_folds + 1):\n",
    "        K.clear_session(); gc.collect()\n",
    "        model_path = os.path.join(model_dir, f'model_fold{i}_ens1.h5')\n",
    "        print(f\"    Analyzing model: {os.path.basename(model_path)}\")\n",
    "        try:\n",
    "            model = load_model(model_path)\n",
    "            # The analyzer needs the model without the final activation if it's not linear\n",
    "            analyzer = innvestigate.create_analyzer(\"lrp.epsilon\", model)\n",
    "            relevance = analyzer.analyze(data_to_analyze)\n",
    "            all_relevance_maps.append(relevance)\n",
    "        except Exception as e:\n",
    "            print(f\"      ❌ ERROR analyzing model {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_relevance_maps:\n",
    "        return None\n",
    "        \n",
    "    # Stack along a new axis and compute the mean across the folds\n",
    "    ensemble_relevance = np.mean(np.stack(all_relevance_maps), axis=0)\n",
    "    return ensemble_relevance\n",
    "\n",
    "# --- 4. Main LRP Analysis and Plotting ---\n",
    "TEST_SCENARIOS = {\n",
    "    \"historical\": TS_hist_norm,\n",
    "    \"4xCO2\": TS_4xCO2_norm\n",
    "}\n",
    "\n",
    "for scenario, data in TEST_SCENARIOS.items():\n",
    "    print(f\"\\n======================================================\")\n",
    "    print(f\"▶️ PROCESSING LRP FOR SCENARIO: {scenario.upper()}\")\n",
    "    print(f\"======================================================\")\n",
    "    \n",
    "    # Generate the full [time, lat, lon] attribution map\n",
    "    attribution_map = ensemble_lrp_analyze(\n",
    "        model_dir=path_PIc,\n",
    "        num_folds=n_folds,\n",
    "        data_to_analyze=data\n",
    "    )\n",
    "    \n",
    "    if attribution_map is not None:\n",
    "        # Squeeze out the channel dimension for saving and plotting\n",
    "        attribution_map = attribution_map.squeeze()\n",
    "        \n",
    "        # Save the full attribution map for future analyses\n",
    "        output_filename = f\"attribution_map_{scenario}_FSNT_C1.npy\"\n",
    "        np.save(output_filename, attribution_map)\n",
    "        print(f\"✅ Full attribution map saved to: {output_filename}\")\n",
    "        \n",
    "        # Calculate the time-averaged mean relevance\n",
    "        mean_relevance_map = np.mean(attribution_map, axis=0)\n",
    "        \n",
    "        # Plot the mean relevance map\n",
    "        plot_pacific_centered_map(\n",
    "            lon, lat, mean_relevance_map,\n",
    "            title=f\"Time-Averaged Mean Relevance - {scenario.capitalize()} Scenario\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d451b-056c-4606-af7a-19f0825be8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcdcb0c-3696-4489-b482-e4f6257bebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Physically Scaled Local Lambda Contribution Map Calculation\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# --- 1. Configuration: Please fill in these paths ---\n",
    "\n",
    "# Path to the out-of-sample test file containing the INPUT data (e.g., TS_anom)\n",
    "path_test_input = \"/ocean/projects/ees250004p/ezhu3/data/CESM1/test/test.4xCO2.ANN.new.nc\"\n",
    "#\n",
    "input_var_name = \"TS_anom\"\n",
    "\n",
    "# Path to the saved LRP relevance map (.npy file) for the same scenario\n",
    "path_lrp_map = \"attribution_map_historical_FSNT_C2.npy\"\n",
    "#path_lrp_map = \"attribution_map_historical_FSNT_C1.npy\"\n",
    "\n",
    "# Path to the directory of the trained model that generated the LRP map\n",
    "# This is needed to get the normalization constant (y_std).\n",
    "path_to_model_dir = path_PIc\n",
    "\n",
    "\n",
    "# --- 2. Helper Functions ---\n",
    "\n",
    "def calculate_area_weighted_global_mean(data_3d, lat_coords):\n",
    "    \"\"\"\n",
    "    Calculates the area-weighted global mean from a [time, lat, lon] array.\n",
    "    \"\"\"\n",
    "    weights_lat = np.cos(np.deg2rad(lat_coords))\n",
    "    mean_over_lon = np.mean(data_3d, axis=2)\n",
    "    global_mean_timeseries = np.average(mean_over_lon, axis=1, weights=weights_lat)\n",
    "    return global_mean_timeseries\n",
    "\n",
    "def calculate_local_lambda_map(local_relevance_scaled, global_mean_tsa):\n",
    "    \"\"\"\n",
    "    Calculates the physically-scaled local lambda contribution map.\n",
    "    \"\"\"\n",
    "    sum_relevance_map = np.sum(local_relevance_scaled, axis=0)\n",
    "    sum_centered_tsa = np.sum(global_mean_tsa - np.mean(global_mean_tsa))\n",
    "    local_lambda_contribution_map = sum_relevance_map / sum_centered_tsa\n",
    "    return local_lambda_contribution_map\n",
    "\n",
    "def plot_pacific_centered_map(lon, lat, data2d, title, cbar_label=\"\"):\n",
    "    \"\"\"\n",
    "    Plots a 2D global map with a sensitive, percentile-based color scale.\n",
    "    \"\"\"\n",
    "    # --- THIS IS THE KEY CHANGE FOR A SMALLER COLOR SCALE ---\n",
    "    # By changing 98 to a smaller number like 95 or 90, you make the\n",
    "    # color scale more sensitive to smaller variations, making them more obvious.\n",
    "    vmax = np.nanpercentile(np.abs(data2d), 95) # Changed from 98 to 95\n",
    "    vmin = -vmax\n",
    "    # ---------------------------------------------------------\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6),\n",
    "                           subplot_kw=dict(projection=ccrs.PlateCarree(central_longitude=180)))\n",
    "    pcm = ax.pcolormesh(lon, lat, data2d, cmap=\"RdBu_r\", transform=ccrs.PlateCarree(), vmin=vmin, vmax=vmax)\n",
    "    ax.coastlines(zorder=2)\n",
    "    gl = ax.gridlines(draw_labels=True, linestyle=\"--\", alpha=0.7)\n",
    "    gl.top_labels = False; gl.right_labels = False\n",
    "    cbar = plt.colorbar(pcm, ax=ax, orientation=\"vertical\", pad=0.03, shrink=0.85)\n",
    "    cbar.set_label(cbar_label, fontsize=14)\n",
    "    ax.set_title(title, fontsize=16, pad=15)\n",
    "    \n",
    "    total_sum = np.nansum(data2d)\n",
    "    # Add the sum as text to the top-left corner of the plot\n",
    "    #ax.text(0.02, 0.95, f\"Sum = {total_sum:.2f}\", transform=ax.transAxes,\n",
    "    #        fontsize=14, bbox=dict(facecolor=\"white\", edgecolor=\"black\", alpha=0.7))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- 3. Main Analysis ---\n",
    "print(\"--- Starting Lambda Map Calculation ---\")\n",
    "\n",
    "try:\n",
    "    # Load the necessary data\n",
    "    print(\"    Loading data...\")\n",
    "    ds_input = xr.open_dataset(path_test_input)\n",
    "    ts_anomaly_data = ds_input[input_var_name].values\n",
    "    lat = ds_input['lat'].values\n",
    "    lon = ds_input['lon'].values\n",
    "    \n",
    "    lrp_map = np.load(path_lrp_map)\n",
    "    \n",
    "    normalization = sio.loadmat(os.path.join(path_to_model_dir, 'Normalization.mat'))\n",
    "    y_std_scalar = float(normalization['y_std'][0, 0])\n",
    "    print(\"✅ Data loaded successfully.\")\n",
    "\n",
    "    # Step 1: Calculate the global mean surface temperature anomaly time series\n",
    "    print(\"\\n    Step 1: Calculating area-weighted global mean...\")\n",
    "    ts_mean_timeseries = calculate_area_weighted_global_mean(ts_anomaly_data, lat)\n",
    "    print(\"✅ Global mean calculated.\")\n",
    "\n",
    "    # Step 2: Scale the LRP relevance map\n",
    "    print(\"\\n    Step 2: Scaling LRP relevance map...\")\n",
    "    lrp_map_scaled = lrp_map * y_std_scalar\n",
    "    print(\"✅ LRP map scaled.\")\n",
    "\n",
    "    # Step 3: Calculate the local lambda contribution map\n",
    "    print(\"\\n    Step 3: Calculating local lambda contribution map...\")\n",
    "    lambda_map = calculate_local_lambda_map(lrp_map_scaled, ts_mean_timeseries)\n",
    "    print(\"✅ Lambda map calculated.\")\n",
    "    \n",
    "    # Step 4: Plot the final result\n",
    "    print(\"\\n    Step 4: Plotting the final map...\")\n",
    "    plot_pacific_centered_map(\n",
    "        lon, lat, lambda_map,\n",
    "        title=\"Physically Scaled Local λ Contribution Map\",\n",
    "        cbar_label=\"Local λ Contribution (W/m²/K per global K)\"\n",
    "    )\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: A file was not found. Please double-check your paths:\\n\"\n",
    "          f\"  Input Path: {path_test_input}\\n\"\n",
    "          f\"  LRP Path: {path_lrp_map}\\n\"\n",
    "          f\"  Model Path: {path_to_model_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (tf210)",
   "language": "python",
   "name": "tf210"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
