{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d14b88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense,LeakyReLU, Dropout, Add, Activation, Conv2D, Flatten, MaxPooling2D, Dense, PReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from matplotlib.colors import Normalize\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "tf.keras.utils.set_random_seed(0)\n",
    "tf.config.list_physical_devices('GPU')  \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras.engine.training_v1\")\n",
    "import xarray as xr\n",
    "import innvestigate\n",
    "import scipy.io as sio\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c3435-9030-45fd-9f63-4821b48f1294",
   "metadata": {},
   "source": [
    "##IMPORTANT!!\n",
    "NEED TO CHANGE INPUT/OUTPUT path for CESM1/CESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "560eda33",
   "metadata": {
    "title": "load data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (lon: 288, lat: 192, time: 2000)\n",
      "Coordinates:\n",
      "  * lon      (lon) float64 0.0 1.25 2.5 3.75 5.0 ... 355.0 356.2 357.5 358.8\n",
      "  * lat      (lat) float64 -90.0 -89.06 -88.12 -87.17 ... 87.17 88.12 89.06 90.0\n",
      "  * time     (time) int64 1 2 3 4 5 6 7 8 ... 1994 1995 1996 1997 1998 1999 2000\n",
      "Data variables:\n",
      "    TS       (time, lat, lon) float32 ...\n",
      "    TS_anom  (time, lat, lon) float32 ...\n",
      "Attributes:\n",
      "    script:   /glade/work/dongy24/Python/create_input_for_huaiyu_CESM2.ipynb\n",
      "    author:   Y. Dong, 04/01/2025\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'/ocean/projects/ees250004p/ezhu3/data/CESM2/control/output.B1850C5CN.TOA.gmean.ANN.0400-2200.new.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/xarray/backends/file_manager.py:209\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/xarray/backends/lru_cache.py:55\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 55\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/ocean/projects/ees250004p/ezhu3/data/CESM2/control/output.B1850C5CN.TOA.gmean.ANN.0400-2200.new.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '3be829d0-a7fc-4e79-bb7d-8022685fc333']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var, filename \u001b[38;5;129;01min\u001b[39;00m files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     36\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, filename)\n\u001b[0;32m---> 37\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     datasets[var] \u001b[38;5;241m=\u001b[39m ds[var]  \u001b[38;5;66;03m# Extract only the variable\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Now the variables can be accessed as datasets[\"CRE\"], datasets[\"TOA\"], etc.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# path for storing the trained neural networks\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM2/trained_model_data2'#CESM2\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/xarray/backends/api.py:541\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    530\u001b[0m     decode_cf,\n\u001b[1;32m    531\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    537\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    538\u001b[0m )\n\u001b[1;32m    540\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 541\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    548\u001b[0m     backend_ds,\n\u001b[1;32m    549\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    558\u001b[0m )\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/xarray/backends/netCDF4_.py:578\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    559\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    575\u001b[0m ):\n\u001b[1;32m    577\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 578\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    590\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/xarray/backends/netCDF4_.py:382\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    376\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    377\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    379\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    380\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    381\u001b[0m )\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/xarray/backends/netCDF4_.py:329\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/xarray/backends/netCDF4_.py:391\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/xarray/backends/netCDF4_.py:385\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    386\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/contextlib.py:113\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/xarray/backends/file_manager.py:197\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/xarray/backends/file_manager.py:215\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    213\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    214\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 215\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2307\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:1925\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'/ocean/projects/ees250004p/ezhu3/data/CESM2/control/output.B1850C5CN.TOA.gmean.ANN.0400-2200.new.nc'"
     ]
    }
   ],
   "source": [
    "# load the input variable -- global Surface temperature\n",
    "\n",
    "#CESM1 control\n",
    "ds = xr.open_dataset(\"/ocean/projects/ees250004p/ezhu3/data/CESM1/control/input.B1850C5CN.TS.ANN.0400-2200.new.nc\")\n",
    "\n",
    "#CESM2 data 1: \n",
    "#ds = xr.open_dataset(\"/ocean/projects/ees250004p/ezhu3/data/CESM2/control/input.CESM2-B1850.TS_detrend.ANN.nc\")\n",
    "#CESM2 data2:\n",
    "#ds = xr.open_dataset(\"/ocean/projects/ees250004p/ezhu3/data/CESM2/control/input.CESM2-B1850.TS.ANN.nc\")\n",
    "\n",
    "#ds = xr.open_dataset(\"E:\\\\Yue\\\\CESM2\\\\control\\\\input.CESM2-B1850.TS_detrend.ANN.nc\")# Display dataset info\n",
    "print(ds)\n",
    "# Access a specific variable\n",
    "TS = ds[\"TS_anom\"] # Surface temperature (radiative)     units = 'K'\n",
    "lat = ds[\"lat\"] #latitude\n",
    "lon = ds[\"lon\"] #longitude\n",
    "time = ds[\"time\"]\n",
    "\n",
    "\n",
    "# Define the directory where the output variables are stored\n",
    "data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM2/control' #this is for CESM1\n",
    "# Define the filenames and corresponding variable names\n",
    "files = {\n",
    "    #CESM1:\n",
    "    \"TOA_anom\": \"output.B1850C5CN.TOA.gmean.ANN.0400-2200.new.nc\"\n",
    "    \n",
    "    #CESM2 data 1:\n",
    "    #\"TOA_anom\": \"output.CESM2-B1850.TOA_detrend.ANN.nc\"\n",
    "    #CESM2 data 2:\n",
    "    #\"TOA_anom\": \"output.CESM2-B1850.TOA.ANN.nc\" #it is possible that the output i used is data 2\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "# Load variables\n",
    "for var, filename in files.items():\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    datasets[var] = ds[var]  # Extract only the variable\n",
    "\n",
    "\n",
    "# Now the variables can be accessed as datasets[\"CRE\"], datasets[\"TOA\"], etc.\n",
    "\n",
    "\n",
    "# path for storing the trained neural networks\n",
    "#data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM2/trained_model_data2'#CESM2 ->this is where the model went\n",
    "data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM1/trained_model'#CESM1\n",
    "#it was being output the the wrong place with CESM2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd24db",
   "metadata": {
    "title": "Neural Network Hyperparameters"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "            \n",
    "# Neural network architecture:\n",
    "# Example: [64, 64, 64] means three hidden layers, each containing 64 neurons.\n",
    "kernels = [32, 32]\n",
    "kernel_acts =  [\"relu\", \"relu\"]\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "rng_seed = 42\n",
    "hiddens = [16, 8]\n",
    "activation_function_dense = [\"elu\", \"elu\"]\n",
    "pool_size = 2\n",
    "\n",
    "# Loss function for regression tasks:\n",
    "# Options: 'mse' (mean squared error), 'mae' (mean absolute error), 'mape' (mean absolute percentage error)\n",
    "# Full list of regression losses: https://keras.io/api/losses/\n",
    "loss_function = 'mse'\n",
    "\n",
    "\n",
    "reg_strength = 0         # L2 regularization strength; 1e-1~1e-5\n",
    "dropout_rate = 0.25         # Dropout rate (0.0 to disable dropout)\n",
    "\n",
    "\n",
    "# low-pass filter time scale; 0 means no low-pass filter\n",
    "LPF_year = 0\n",
    "\n",
    "\n",
    "#### normalization\n",
    "remove_mean = 0\n",
    "divide_std = 1\n",
    "\n",
    "\n",
    "#### usually we do not change the parameters below \n",
    "\n",
    "# Training configuration\n",
    "epoch_max = 25000            # Maximum number of training epochs\n",
    "batch_size = 32            # Batch size used during training\n",
    "learning_rate = 0.000005       # Default learning rate for Adam optimizer 0.001\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "num_folds = 5 # Number of fold during cross-validation\n",
    "NNrepeats = 1 # Repeat the training for NNrepeats times\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c050e93",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "pre-process data"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TOA_anom'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m names_strALL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOA_anom\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m names_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOA_anom\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m output_raw \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnames_str\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TOA_anom'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# input_raw = TS.values.reshape(TS.shape[0],TS.shape[1]*TS.shape[2])\n",
    "# For CNN, we don't need to flatten the data\n",
    "input_raw = TS.values\n",
    "\n",
    "\n",
    "\n",
    "# Define the variable names as a comma-separated string\n",
    "# names_strALL = \"CRE,FLNT,FSNT,LCC,LWCF,SWCF,TCC,TOA\"\n",
    "# If we only want to reconstruct TOA\n",
    "names_strALL = \"TOA_anom\"\n",
    "names_str = \"TOA_anom\"\n",
    "\n",
    "output_raw = datasets[names_str].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49bea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_NN_name():\n",
    "\n",
    "    NN_name = 'CNN'\n",
    "        \n",
    "    activation_function_str = '_'+kernel_acts[0]+ '+'+activation_function_dense[0] \n",
    "    loss_function_str = '_'+str(loss_function)+'loss' if loss_function!='mse' else ''\n",
    "    \n",
    "    if remove_mean + divide_std == 0:\n",
    "        scaler_str = 'NoScaler_'\n",
    "    elif remove_mean + divide_std == 2:\n",
    "        scaler_str = 'StandardScaler_'\n",
    "    elif remove_mean:\n",
    "        scaler_str = 'RemoveMean_'\n",
    "    elif divide_std:\n",
    "        scaler_str = 'DivideSTD_'\n",
    "    LPF_str = f'_LPF{int(LPF_year)}Year' if LPF_year else ''\n",
    "    NN_structure_str = 'x'.join(map(str, kernels)) \n",
    "    reg_str = f'Reg{reg_strength}' + (f'Drop{dropout_rate}' if dropout_rate != 0 else '')\n",
    "    batch_size_str =  f'BS{batch_size}_' if batch_size !=600 else ''\n",
    "    return f\"{NN_name}_{scaler_str}Neur{NN_structure_str}_{batch_size_str}{num_folds}foldCV_{reg_str}{loss_function_str}{activation_function_str}{LPF_str}\"\n",
    "NN_name = create_NN_name()\n",
    "\n",
    "    \n",
    "def construct_output_directory(data_dir, NN_name):\n",
    "    output_dir = os.path.join(data_dir, 'NeuralNet', NN_name, names_str)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "output_dir = construct_output_directory(data_dir, NN_name)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c1afa",
   "metadata": {
    "title": "Define logger that can print useful information into a logfile"
   },
   "outputs": [],
   "source": [
    "logger_name = 'logfile.log'  \n",
    "# Configure logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create handlers\n",
    "file_handler = logging.FileHandler(os.path.join(output_dir, logger_name))\n",
    "console_handler = logging.StreamHandler()\n",
    "\n",
    "# Set level and format for handlers\n",
    "file_handler.setLevel(logging.INFO)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Test the setup\n",
    "logger.info(\"logfile\")\n",
    "\n",
    " \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    logger.info(\"Using GPU for training.\")\n",
    "    logger.info(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "else:\n",
    "    logger.info(\"Using CPU for training.\") \n",
    "\n",
    "logger.info(os.getenv('TF_GPU_ALLOCATOR'))\n",
    "    \n",
    "logger.info(f\"Output path: {'created successfully' if os.path.exists(output_dir) else 'already exists'}\")\n",
    "logger.info(f\"The output path is {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9886a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info(f'Applying {LPF_year}-year low pass filter ...')\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "\n",
    "from scipy.signal import butter, sosfilt\n",
    "\n",
    "def apply_low_pass_filter(data, cutoff_freq, order=5, sampling_rate=1, padding_length=None):\n",
    "    \"\"\"Applies a Butterworth low-pass filter to the given data.\"\"\"\n",
    "    sos = butter(order, cutoff_freq, btype='low', output='sos', analog=False, fs=sampling_rate)\n",
    "    \n",
    "    # Apply Boundary Padding to the data before filtering\n",
    "    padded_data = np.pad(data, [(padding_length, padding_length), (0, 0)], mode='reflect')\n",
    "    \n",
    "    # Apply the filter across each column without explicit looping\n",
    "    filtered_data = sosfilt(sos, padded_data, axis=0)\n",
    "    \n",
    "    # Remove padding\n",
    "    return filtered_data[padding_length:-padding_length]\n",
    "\n",
    "if LPF_year:\n",
    "    # Calculate the sampling rate (monthly data)\n",
    "    sampling_rate = 1  # Data is sampled monthly    \n",
    "    cutoff_freq = 1 / LPF_year\n",
    "    order = 5\n",
    "    padding_length =  3* LPF_year\n",
    "\n",
    "if LPF_year:\n",
    "    input = apply_low_pass_filter(input_raw, cutoff_freq, order, sampling_rate, padding_length)     \n",
    "    output= apply_low_pass_filter(output_raw, cutoff_freq, order, sampling_rate, padding_length)     \n",
    "    \n",
    "    plt.figure(figsize=(30, 6))  \n",
    "    plt.plot(time, output_raw, label=\"Original\", color=\"C0\", linewidth=2)  # Use color and linewidth\n",
    "    plt.plot(time, output, label=\"Low-pass filtered\", color=\"C1\", linestyle=\"-\", linewidth=1.5)  # Dashed line for prediction\n",
    "    plt.xlabel(\"Time (year)\", fontsize=14)\n",
    "    plt.ylabel(names_str, fontsize=14)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.legend(fontsize=12, loc=\"best\")\n",
    "    plt.savefig(os.path.join(output_dir, names_str+\"_LPF.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    input  =  input_raw\n",
    "    output = output_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#                       define the neural network                     #\n",
    "#######################################################################\n",
    "\n",
    "log_path =os.path.join(output_dir, 'training_logs.txt')\n",
    "os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "class PrintTrainingOnTextEvery10EpochsCallback(Callback):\n",
    "    def __init__(self, log_path):\n",
    "        super().__init__()\n",
    "        self.log_path = log_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 10 == 0:  # Log every 10 epochs\n",
    "            with open(self.log_path, \"a\") as log_file:\n",
    "                log_file.write(\n",
    "                    f\"Epoch: {epoch:>3} | \"\n",
    "                    f\"Loss: {logs.get('loss', 0):.2e} | \"\n",
    "#                        f\"Accuracy: {logs.get('accuracy', 0):.2e} | \"\n",
    "                    f\"Validation loss: {logs.get('val_loss', 0):.2e} |\\n \"\n",
    "#                        f\"Validation accuracy: {logs.get('val_accuracy', 0):.2e}\\n\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"Epoch {epoch:>3} - \"\n",
    "                    f\"Loss: {logs.get('loss', 0):.2e}, \"\n",
    "#                        f\"Accuracy: {logs.get('accuracy', 0):.2e}, \"\n",
    "                    f\"Validation loss: {logs.get('val_loss', 0):.2e}, \"\n",
    "#                        f\"Validation accuracy: {logs.get('val_accuracy', 0):.2e}\"\n",
    "                )\n",
    "\n",
    "my_callbacks = [\n",
    "    PrintTrainingOnTextEvery10EpochsCallback(log_path=log_path),\n",
    "]   \n",
    "\n",
    "        \n",
    "#########ORIGINAL#########        \n",
    "# def train_model(X_train, y_train, X_test, y_test,y_mean,y_std):\n",
    "                \n",
    "#     # Create a new model with random initial weights and biases\n",
    "#     inputs = tf.keras.Input(shape=(X_train.shape[1:]),)\n",
    "#     layers = inputs\n",
    "\n",
    "#     for kernel, kernel_act in zip(kernels, kernel_acts):\n",
    "#         layers = Conv2D(\n",
    "#             kernel,\n",
    "#             (kernel_size, kernel_size),\n",
    "#             strides=(stride, stride),\n",
    "#             use_bias=True,\n",
    "#             activation=kernel_act,\n",
    "#             padding=\"same\",\n",
    "#             bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "#             kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "#         )(layers)\n",
    "#         layers = MaxPooling2D((pool_size, pool_size))(layers)\n",
    "    \n",
    "    \n",
    "#     # make final dense layers\n",
    "#     layers = Flatten()(layers)\n",
    "    # for hidden, activation in zip(hiddens, activation_function_dense):\n",
    "    #     layers = Dense(\n",
    "    #         hidden,\n",
    "    #         activation=activation,\n",
    "    #         use_bias=True,\n",
    "    #         bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    #         kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    #     )(layers)\n",
    "    \n",
    "    \n",
    "    # output_layer = Dense(\n",
    "    #     y_train.shape[-1],\n",
    "    #     activation=\"linear\",\n",
    "    #     use_bias=True,\n",
    "    #     bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    #     kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    # )(layers)\n",
    "    \n",
    "    # # CONSTRUCT THE MODEL\n",
    "    # model = Model(inputs, output_layer)\n",
    "\n",
    "    \n",
    "    # if fold_no + ens_no == 2:\n",
    "    #     # show the model summary\n",
    "    #     model.summary()\n",
    "    #     with open(os.path.join(output_dir, 'model_' + names_str  + '.txt'), 'w') as f:\n",
    "    #         model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "    #     dot_img_file = os.path.join(output_dir, 'model_' + names_str  + '.png')\n",
    "    #     plot_model(model, to_file=dot_img_file,\n",
    "    #                show_shapes=True,\n",
    "    #                show_dtype=False,\n",
    "    #                show_layer_names=True,\n",
    "    #                rankdir='LR',\n",
    "    #                expand_nested=False,\n",
    "    #                dpi=300,\n",
    "    #                show_layer_activations=True)\n",
    "        \n",
    "    # # Compile the model with mean squared error loss and Adam optimizer\n",
    "    # model.compile(loss=loss_function, optimizer=Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    # # Define early stopping callback\n",
    "    # early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='auto', restore_best_weights=1, verbose=1)\n",
    "\n",
    "    # # Train the model with early stopping callback\n",
    "    # history = model.fit(X_train, y_train, epochs=epoch_max, batch_size=batch_size,\n",
    "    #                     validation_data=(X_test, y_test), callbacks=[early_stopping,my_callbacks],verbose=0)\n",
    "\n",
    "    # # Evaluate the performance on the testing set (less useful)\n",
    "    # skill = model.evaluate(X_test, y_test, verbose=0) \n",
    "\n",
    "\n",
    "    # # Make the prediction with the actual scale in the testing set\n",
    "    # pred = (model.predict(X_test)*y_std)+y_mean\n",
    "    \n",
    "    # # Calculate the R2 value in the testing set at each latitude \n",
    "    # R2_val = []\n",
    "    # truth = (y_test*y_std)+y_mean \n",
    "    \n",
    "    # for latind in range(y_test.shape[1]):\n",
    "    #     y_lat = truth[:, latind];\n",
    "    #     y_pred_lat = pred[:, latind];\n",
    "    #     R2_val.append(r2_score(y_lat, y_pred_lat))\n",
    "        \n",
    "    # model.save(os.path.join(output_dir,'model_fold'+str(fold_no)+'_ens'+str(ens_no)+'.h5'))\n",
    "\n",
    "\n",
    "    # return skill, history, pred, R2_val\n",
    "        \n",
    "def train_model(X_train, y_train, X_test, y_test, y_mean, y_std):\n",
    "    inputs = tf.keras.Input(shape=(X_train.shape[1:]),)\n",
    "    layers = inputs\n",
    "\n",
    "    # Convolutional Layers\n",
    "    for i, kernel_filters in enumerate(kernels): \n",
    "        layers = Conv2D(\n",
    "            kernel_filters,\n",
    "            (kernel_size, kernel_size), \n",
    "            strides=(stride, stride),     \n",
    "            use_bias=True,\n",
    "            padding=\"same\",\n",
    "            # NO 'activation' argument here if PReLU or another layer follows\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "        )(layers)\n",
    "\n",
    "        # Apply PReLU (or other chosen activation)\n",
    "        if kernel_acts[i].lower() == 'prelu':\n",
    "            layers = PReLU(shared_axes=[1, 2],\n",
    "                           alpha_initializer=tf.keras.initializers.Constant(0.25) # Common starting point\n",
    "                          )(layers) # shared_axes for Conv2D with channels_last\n",
    "        elif kernel_acts[i].lower() == 'gelu':\n",
    "            layers = tf.keras.layers.Activation('gelu')(layers) # Using Activation layer for GeLU\n",
    "        elif kernel_acts[i]: # For 'relu', 'elu', etc.\n",
    "            layers = tf.keras.layers.Activation(kernel_acts[i])(layers)\n",
    "        # If kernel_acts[i] is None or an empty string, no explicit activation layer added here.\n",
    "\n",
    "        layers = MaxPooling2D((pool_size, pool_size))(layers) \n",
    "    \n",
    "    layers = Flatten()(layers)\n",
    "\n",
    "    # Dense Layers\n",
    "    for i, hidden_units in enumerate(hiddens): \n",
    "        layers = Dense(\n",
    "            hidden_units,\n",
    "            use_bias=True,\n",
    "            # NO 'activation' argument here\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "        )(layers)\n",
    "\n",
    "        # Apply PReLU (or other chosen activation)\n",
    "        if activation_function_dense[i].lower() == 'prelu':\n",
    "            layers = PReLU(alpha_initializer=tf.keras.initializers.Constant(0.25))(layers) # No shared_axes for Dense\n",
    "        elif activation_function_dense[i].lower() == 'gelu':\n",
    "            layers = tf.keras.layers.Activation('gelu')(layers)\n",
    "        elif activation_function_dense[i]: # For 'elu', 'relu', etc.\n",
    "            layers = tf.keras.layers.Activation(activation_function_dense[i])(layers)\n",
    "        # If activation_function_dense[i] is None or an empty string, no explicit activation layer.\n",
    "\n",
    "    output_layer = Dense(\n",
    "        y_train.shape[-1], \n",
    "        activation=\"linear\",\n",
    "        use_bias=True,\n",
    "        bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "        kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    )(layers)\n",
    "    \n",
    "    model = Model(inputs, output_layer)\n",
    "\n",
    "    if fold_no + ens_no == 2: \n",
    "        model.summary()\n",
    "\n",
    "    model.compile(loss=loss_function, optimizer=Adam(learning_rate=learning_rate))\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='auto', restore_best_weights=True, verbose=1) # restore_best_weights=1 is True\n",
    "\n",
    "    # Make sure my_callbacks is defined in this scope or passed as an argument\n",
    "    history = model.fit(X_train, y_train, epochs=epoch_max, batch_size=batch_size,\n",
    "                        validation_data=(X_test, y_test), callbacks=[early_stopping] + my_callbacks, verbose=0) # Added '+' for list concatenation\n",
    "\n",
    "    skill = model.evaluate(X_test, y_test, verbose=0)\n",
    "    pred = (model.predict(X_test) * y_std) + y_mean # Make sure y_std and y_mean are defined\n",
    "\n",
    "    R2_val = []\n",
    "    truth = (y_test * y_std) + y_mean\n",
    "    for latind in range(truth.shape[1] if truth.ndim > 1 else 1): # Handle if y_test is 1D output\n",
    "        y_lat = truth[:, latind] if truth.ndim > 1 else truth\n",
    "        y_pred_lat = pred[:, latind] if pred.ndim > 1 else pred\n",
    "        R2_val.append(r2_score(y_lat, y_pred_lat))\n",
    "    \n",
    "    model_save_path = os.path.join(output_dir, f'model_fold{fold_no}_ens{ens_no}.keras') # Use .keras\n",
    "    model.save(model_save_path)\n",
    "    \n",
    "    return skill, history, pred, R2_val\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff17f61",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#                             Train loop                              #\n",
    "#######################################################################\n",
    "X = input\n",
    "y = output\n",
    "\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "\n",
    "y_mean = np.mean(y, axis=0)\n",
    "y_std = np.std(y, axis=0)\n",
    "\n",
    "if remove_mean ==0:\n",
    "    X_mean = 0\n",
    "    y_mean = 0\n",
    "\n",
    "    \n",
    "    \n",
    "if divide_std ==0:\n",
    "    X_std = 1\n",
    "    y_std = 1\n",
    "else:\n",
    "    X_std = X_std[:,:, tf.newaxis]\n",
    "    \n",
    "    \n",
    "    \n",
    "sio.savemat(os.path.join(output_dir,'Normalization.mat'), \n",
    "            {'X_mean': X_mean, 'X_std': X_std,'y_mean':y_mean,'y_std':y_std})\n",
    "\n",
    "# if not os.path.exists(os.path.join(output_dir, 'inputs_info.mat')):\n",
    "\n",
    "        \n",
    "trained_models = []\n",
    "y_pred_reconstructed_allfolds = []\n",
    "\n",
    "\n",
    "# Train the neural network multiple times using k-fold cross validation\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "fold_no = 1\n",
    "for trainind, testind in kfold.split(X, y):   \n",
    "    # break\n",
    "    X_train = X[trainind,:,:, tf.newaxis]\n",
    "    y_train = y[trainind,:]\n",
    "    X_test = X[testind,:, :,tf.newaxis]\n",
    "    y_test = y[testind,:]\n",
    " \n",
    "    # Normalize the input and out data based on the information from the entire PI control run\n",
    "    \n",
    "    \n",
    "    X_train = (X_train - X_mean)/X_std\n",
    "    X_test = (X_test - X_mean)/X_std\n",
    "    \n",
    "    y_train = (y_train - y_mean)/y_std\n",
    "    y_test = (y_test - y_mean)/y_std\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Generate a print\n",
    "    logger.info('------------------------------------------------------------------------')\n",
    "    # We use ensemble training for each fold of the cross-validation\n",
    "    for ens_no  in np.arange(1,NNrepeats+1):\n",
    "        logger.info(f'Training for fold {fold_no} ensemble {ens_no}...')\n",
    "        skill, history, pred, R2_val = train_model(X_train, y_train, X_test, y_test,y_mean,y_std)\n",
    "        trained_models.append((skill, skill, history.history, pred, R2_val))\n",
    "\n",
    "        \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "        \n",
    "        \n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info(f'Training with {num_folds}-fold cross-validation finished!') \n",
    "logger.info('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be32e148",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "have a quick look at the performance of the trainined neural network"
   },
   "outputs": [],
   "source": [
    "\n",
    "skills = [trained_model[1] for trained_model in trained_models[0:num_folds*NNrepeats]]\n",
    "median_skill = np.median(skills)\n",
    "median_index = skills.index(median_skill)\n",
    "minimum_skill = np.min(skills)\n",
    "min_index = skills.index(minimum_skill)\n",
    "std_dev_skill = np.std(skills)\n",
    "\n",
    "formatted_skills = [f\"{skill:.4f}\" for skill in skills]\n",
    "logger.info(f\"Losses in testing set: {formatted_skills}\")\n",
    "fold_median = median_index // NNrepeats + 1  # Compute fold number\n",
    "ensemble_median = median_index % NNrepeats + 1  # Compute ensemble number\n",
    "logger.info(f\"Median Loss is: {median_skill:.4f}, which occurs at Fold {fold_median} Ensemble {ensemble_median}\")\n",
    "fold_min = min_index // NNrepeats + 1  # Compute fold number\n",
    "ensemble_min = min_index % NNrepeats + 1  # Compute ensemble number\n",
    "logger.info(f\"Minimum Loss is: {minimum_skill:.4f}, which occurs at Fold {fold_min} Ensemble {ensemble_min}\")\n",
    "logger.info(f\"Standard Deviation of the Loss is: {std_dev_skill:.4f}\")\n",
    "ratio = std_dev_skill/median_skill * 100\n",
    "logger.info(f\"Standard Deviation/Median: {ratio:.2f}%\")\n",
    "\n",
    "# Check if the standard deviation to median skill ratio is higher than 30%\n",
    "if ratio > 30:\n",
    "    warning_message = (\"Warning: The standard deviation of model skill across \"\n",
    "                       \"folds and ensembles is large!\\nStandard Deviation/Median Skill: {ratio:.2f}%\").format(ratio=ratio)\n",
    "    with open(os.path.join(output_dir,'Warning_'+names_str+'.txt'), \"w\") as file:\n",
    "        file.write(warning_message)\n",
    "    with open(os.path.join(output_dir,'..','Warning_'+names_str+'.txt'), \"w\") as file:\n",
    "        file.write(warning_message)\n",
    "\n",
    "\n",
    "# Plot the loss function during training for all folds\n",
    "# First, find the global minimum and maximum loss values across all folds\n",
    "min_loss = min(min(trained_models[i][2]['loss']+trained_models[i][2]['val_loss']) for i in range(num_folds))\n",
    "max_loss = max(max(trained_models[i][2]['loss']+trained_models[i][2]['val_loss']) for i in range(num_folds))\n",
    "\n",
    "plt.figure(figsize=(9, 12))\n",
    "\n",
    "# Plot the first fold outside the loop to avoid repeating the legend setting\n",
    "plt.subplot(num_folds, 1, 1)\n",
    "for nn in range(NNrepeats):\n",
    "    plt.plot(trained_models[nn][2]['loss'],'-k')\n",
    "    plt.plot(trained_models[nn][2]['val_loss'],'-r')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')  # Set logarithmic scale\n",
    "plt.ylim(min_loss, max_loss)  # Set the same y-limits for all subplots\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Now plot the remaining folds\n",
    "for n in range(1, num_folds):\n",
    "    plt.subplot(num_folds, 1, n+1)\n",
    "    for nn in range(NNrepeats):\n",
    "        plt.plot(trained_models[n*NNrepeats+nn][2]['loss'],'-k')\n",
    "        plt.plot(trained_models[n*NNrepeats+nn][2]['val_loss'],'-r')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')  # Set logarithmic scale\n",
    "    plt.ylim(min_loss, max_loss)  # Set the same y-limits for all subplots\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.subplots_adjust(hspace=0.5)  # Adjust space between plots if needed\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'TrainingLoss_' + names_str  + '.png'),dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "R2_each_fold_ens = [trained_models[i][4] for i in range(0,num_folds*NNrepeats)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22894312",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NNrepeats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Model_preds \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mconcatenate([trained_models[n\u001b[38;5;241m*\u001b[39mNNrepeats \u001b[38;5;241m+\u001b[39m i][\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_folds)]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mNNrepeats\u001b[49m)]\n\u001b[1;32m      2\u001b[0m Model_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(Model_preds,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m Model_error \u001b[38;5;241m=\u001b[39m Model_pred \u001b[38;5;241m-\u001b[39m y\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NNrepeats' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "Model_preds = [np.concatenate([trained_models[n*NNrepeats + i][3] for n in range(num_folds)]) for i in range(NNrepeats)]\n",
    "Model_pred = np.mean(Model_preds,axis=0)\n",
    "Model_error = Model_pred - y\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30, 6))  \n",
    "plt.plot(time, y, label=\"Truth\", color=\"C0\", linewidth=2)  # Use color and linewidth\n",
    "plt.plot(time, Model_pred, label=\"Prediction\", color=\"C1\", linestyle=\"-\", linewidth=1.5)  # Dashed line for prediction\n",
    "plt.xlabel(\"Time (year)\", fontsize=14)\n",
    "plt.ylabel(names_str, fontsize=14)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend(fontsize=12, loc=\"best\")\n",
    "r2 = r2_score(y, Model_pred)\n",
    "plt.text(0.05, 0.9, f\"$R^2$: {r2:.3f}\", transform=plt.gca().transAxes,\n",
    "         fontsize=14, bbox=dict(facecolor=\"white\", alpha=0.7, edgecolor=\"black\"))\n",
    "plt.savefig(os.path.join(output_dir, names_str+\"_TruthvsPred.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "######### scatter plot\n",
    "\n",
    "plt.figure(figsize=(8, 8))  # Keep square aspect ratio\n",
    "\n",
    "# Scatter plot with adjusted marker size and transparency\n",
    "plt.plot(Model_pred, y, 'o', markersize=5, alpha=0.6, color=\"C0\", label=\"Data Points\")\n",
    "\n",
    "# Compute common limits based on both Model_pred and y\n",
    "min_val = min(np.min(Model_pred), np.min(y))\n",
    "max_val = max(np.max(Model_pred), np.max(y))\n",
    "\n",
    "# Set same limits for x and y\n",
    "plt.xlim(min_val, max_val)\n",
    "plt.ylim(min_val, max_val)\n",
    "\n",
    "# Define consistent tick locations\n",
    "num_ticks = 6  # Adjust this for more or fewer ticks\n",
    "ticks = np.linspace(min_val, max_val, num_ticks)\n",
    "\n",
    "plt.xticks(ticks)\n",
    "plt.yticks(ticks)\n",
    "\n",
    "# 1:1 Reference Line\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'k--', linewidth=2, label=\"1:1 Reference\")\n",
    "\n",
    "# Labels and formatting\n",
    "plt.xlabel(\"Prediction\", fontsize=16, fontweight='bold')\n",
    "plt.ylabel(\"Truth\", fontsize=16, fontweight='bold')\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Compute and display R² score\n",
    "r2 = r2_score(y, Model_pred)\n",
    "plt.text(0.05, 0.9, f\"$R^2$: {r2:.3f}\", transform=plt.gca().transAxes,\n",
    "         fontsize=14, bbox=dict(facecolor=\"white\", alpha=0.7, edgecolor=\"black\"))\n",
    "\n",
    "# Equal aspect ratio for fair comparison\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "# Add legend (only for reference line)\n",
    "plt.legend(fontsize=12, loc=\"center left\")\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(os.path.join(output_dir, \"TOA_TruthvsPred_scatter.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a8fc216",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "copy this script to the directory that stores the trained NN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 20:29:56,702 - Script copied to /ocean/projects/ees250004p/ezhu3/data/CESM2/trained_model_data2/NeuralNet/CNN_DivideSTD_Neur32x32_BS32_5foldCV_Reg0Drop0.25_relu+elu/TOA_anom/TS_NN_training_CNN_V2.ipynb\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "def copy_script(target_directory):\n",
    "    # Get the current script file path\n",
    "    current_script = \"TS_NN_training_CNN_V2.ipynb\"\n",
    "    # current_script = script_path_and_name\n",
    "    \n",
    "    # Ensure the target directory exists, create if it does not\n",
    "    os.makedirs(target_directory, exist_ok=True)\n",
    "    \n",
    "    # Define the target path for the script\n",
    "    target_path = os.path.join(target_directory, os.path.basename(current_script))\n",
    "    \n",
    "    # Copy the script\n",
    "    shutil.copy(current_script, target_path)\n",
    "    logger.info(f\"Script copied to {target_path}\")\n",
    "\n",
    "# Example usage\n",
    "copy_script(output_dir)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python (tf210)",
   "language": "python",
   "name": "tf210"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
