{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14b88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Feb 24 10:04:04 2025\n",
    "\n",
    "@author: hwei\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# V3 25-03-2025 updated normalization\n",
    "\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense,LeakyReLU, Dropout, Add, Activation, Conv2D, Flatten, MaxPooling2D, Dense, PReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from matplotlib.colors import Normalize\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "tf.keras.utils.set_random_seed(0)\n",
    "tf.config.list_physical_devices('GPU')  \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras.engine.training_v1\")\n",
    "import xarray as xr\n",
    "import innvestigate\n",
    "import scipy.io as sio\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c3435-9030-45fd-9f63-4821b48f1294",
   "metadata": {},
   "source": [
    "##IMPORTANT!!\n",
    "NEED TO CHANGE INPUT/OUTPUT path for CESM1/CESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560eda33",
   "metadata": {
    "title": "load data"
   },
   "outputs": [],
   "source": [
    "# load the input variable -- global Surface temperature\n",
    "\n",
    "#CESM1 control\n",
    "ds = xr.open_dataset(\"/ocean/projects/ees250004p/ezhu3/data/CESM1/control/input.B1850C5CN.TS.ANN.0400-2200.new.nc\")\n",
    "\n",
    "#CESM2 data 1: \n",
    "#ds = xr.open_dataset(\"/ocean/projects/ees250004p/ezhu3/data/CESM2/control/input.CESM2-B1850.TS_detrend.ANN.nc\")\n",
    "#CESM2 data2:\n",
    "#ds = xr.open_dataset(\"/ocean/projects/ees250004p/ezhu3/data/CESM2/control/input.CESM2-B1850.TS.ANN.nc\")\n",
    "\n",
    "#ds = xr.open_dataset(\"E:\\\\Yue\\\\CESM2\\\\control\\\\input.CESM2-B1850.TS_detrend.ANN.nc\")# Display dataset info\n",
    "print(ds)\n",
    "# Access a specific variable\n",
    "TS = ds[\"TS_anom\"] # Surface temperature (radiative)     units = 'K'\n",
    "lat = ds[\"lat\"] #latitude\n",
    "lon = ds[\"lon\"] #longitude\n",
    "time = ds[\"time\"]\n",
    "\n",
    "\n",
    "# Define the directory where the output variables are stored\n",
    "data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM1/control' #this is for CESM1\n",
    "\n",
    "# Define the filenames and corresponding variable names\n",
    "files = {\n",
    "    #CESM1:\n",
    "    \"TOA_anom\": \"output.B1850C5CN.TOA.gmean.ANN.0400-2200.new.nc\"\n",
    "    \n",
    "    #CESM2 data 1:\n",
    "    #\"TOA_anom\": \"output.CESM2-B1850.TOA_detrend.ANN.nc\"\n",
    "    #CESM2 data 2:\n",
    "    #\"TOA_anom\": \"output.CESM2-B1850.TOA.ANN.nc\"\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "# Load variables\n",
    "for var, filename in files.items():\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    datasets[var] = ds[var]  # Extract only the variable\n",
    "\n",
    "\n",
    "# Now the variables can be accessed as datasets[\"CRE\"], datasets[\"TOA\"], etc.\n",
    "\n",
    "\n",
    "# path for storing the trained neural networks\n",
    "#data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM2/trained_model_data1(1)'#CESM2 model(1)\n",
    "#data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM2/trained_model_data2'#CESM2\n",
    "#data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM1/trained_model'#CESM1\n",
    "data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM1/trained_model_correct'#CESM1 (correct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd24db",
   "metadata": {
    "title": "Neural Network Hyperparameters"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "            \n",
    "# Neural network architecture:\n",
    "# Example: [64, 64, 64] means three hidden layers, each containing 64 neurons.\n",
    "kernels = [32, 32]\n",
    "kernel_acts =  [\"relu\", \"relu\"]\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "rng_seed = 42\n",
    "hiddens = [16, 8]\n",
    "activation_function_dense = [\"elu\", \"elu\"]\n",
    "pool_size = 2\n",
    "\n",
    "# Loss function for regression tasks:\n",
    "# Options: 'mse' (mean squared error), 'mae' (mean absolute error), 'mape' (mean absolute percentage error)\n",
    "# Full list of regression losses: https://keras.io/api/losses/\n",
    "loss_function = 'mse'\n",
    "\n",
    "\n",
    "reg_strength = 0         # L2 regularization strength; 1e-1~1e-5\n",
    "dropout_rate = 0.25         # Dropout rate (0.0 to disable dropout)\n",
    "\n",
    "\n",
    "# low-pass filter time scale; 0 means no low-pass filter\n",
    "LPF_year = 0\n",
    "\n",
    "\n",
    "#### normalization\n",
    "remove_mean = 0\n",
    "divide_std = 1\n",
    "\n",
    "\n",
    "#### usually we do not change the parameters below \n",
    "\n",
    "# Training configuration\n",
    "epoch_max = 25000            # Maximum number of training epochs\n",
    "batch_size = 32            # Batch size used during training\n",
    "learning_rate = 0.000005       # Default learning rate for Adam optimizer 0.001\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "num_folds = 5 # Number of fold during cross-validation\n",
    "NNrepeats = 1 # Repeat the training for NNrepeats times\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c050e93",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "pre-process data"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# input_raw = TS.values.reshape(TS.shape[0],TS.shape[1]*TS.shape[2])\n",
    "# For CNN, we don't need to flatten the data\n",
    "input_raw = TS.values\n",
    "\n",
    "\n",
    "\n",
    "# Define the variable names as a comma-separated string\n",
    "# names_strALL = \"CRE,FLNT,FSNT,LCC,LWCF,SWCF,TCC,TOA\"\n",
    "# If we only want to reconstruct TOA\n",
    "names_strALL = \"TOA_anom\"\n",
    "names_str = \"TOA_anom\"\n",
    "\n",
    "output_raw = datasets[names_str].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49bea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_NN_name():\n",
    "\n",
    "    NN_name = 'CNN'\n",
    "        \n",
    "    activation_function_str = '_'+kernel_acts[0]+ '+'+activation_function_dense[0] \n",
    "    loss_function_str = '_'+str(loss_function)+'loss' if loss_function!='mse' else ''\n",
    "    \n",
    "    if remove_mean + divide_std == 0:\n",
    "        scaler_str = 'NoScaler_'\n",
    "    elif remove_mean + divide_std == 2:\n",
    "        scaler_str = 'StandardScaler_'\n",
    "    elif remove_mean:\n",
    "        scaler_str = 'RemoveMean_'\n",
    "    elif divide_std:\n",
    "        scaler_str = 'DivideSTD_'\n",
    "    LPF_str = f'_LPF{int(LPF_year)}Year' if LPF_year else ''\n",
    "    NN_structure_str = 'x'.join(map(str, kernels)) \n",
    "    reg_str = f'Reg{reg_strength}' + (f'Drop{dropout_rate}' if dropout_rate != 0 else '')\n",
    "    batch_size_str =  f'BS{batch_size}_' if batch_size !=600 else ''\n",
    "    return f\"{NN_name}_{scaler_str}Neur{NN_structure_str}_{batch_size_str}{num_folds}foldCV_{reg_str}{loss_function_str}{activation_function_str}{LPF_str}\"\n",
    "NN_name = create_NN_name()\n",
    "\n",
    "    \n",
    "def construct_output_directory(data_dir, NN_name):\n",
    "    output_dir = os.path.join(data_dir, 'NeuralNet', NN_name, names_str)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "output_dir = construct_output_directory(data_dir, NN_name)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c1afa",
   "metadata": {
    "title": "Define logger that can print useful information into a logfile"
   },
   "outputs": [],
   "source": [
    "logger_name = 'logfile.log'  \n",
    "# Configure logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create handlers\n",
    "file_handler = logging.FileHandler(os.path.join(output_dir, logger_name))\n",
    "console_handler = logging.StreamHandler()\n",
    "\n",
    "# Set level and format for handlers\n",
    "file_handler.setLevel(logging.INFO)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Test the setup\n",
    "logger.info(\"logfile\")\n",
    "\n",
    " \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    logger.info(\"Using GPU for training.\")\n",
    "    logger.info(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "else:\n",
    "    logger.info(\"Using CPU for training.\") \n",
    "\n",
    "logger.info(os.getenv('TF_GPU_ALLOCATOR'))\n",
    "    \n",
    "logger.info(f\"Output path: {'created successfully' if os.path.exists(output_dir) else 'already exists'}\")\n",
    "logger.info(f\"The output path is {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9886a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info(f'Applying {LPF_year}-year low pass filter ...')\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "\n",
    "from scipy.signal import butter, sosfilt\n",
    "\n",
    "def apply_low_pass_filter(data, cutoff_freq, order=5, sampling_rate=1, padding_length=None):\n",
    "    \"\"\"Applies a Butterworth low-pass filter to the given data.\"\"\"\n",
    "    sos = butter(order, cutoff_freq, btype='low', output='sos', analog=False, fs=sampling_rate)\n",
    "    \n",
    "    # Apply Boundary Padding to the data before filtering\n",
    "    padded_data = np.pad(data, [(padding_length, padding_length), (0, 0)], mode='reflect')\n",
    "    \n",
    "    # Apply the filter across each column without explicit looping\n",
    "    filtered_data = sosfilt(sos, padded_data, axis=0)\n",
    "    \n",
    "    # Remove padding\n",
    "    return filtered_data[padding_length:-padding_length]\n",
    "\n",
    "if LPF_year:\n",
    "    # Calculate the sampling rate (monthly data)\n",
    "    sampling_rate = 1  # Data is sampled monthly    \n",
    "    cutoff_freq = 1 / LPF_year\n",
    "    order = 5\n",
    "    padding_length =  3* LPF_year\n",
    "\n",
    "if LPF_year:\n",
    "    input = apply_low_pass_filter(input_raw, cutoff_freq, order, sampling_rate, padding_length)     \n",
    "    output= apply_low_pass_filter(output_raw, cutoff_freq, order, sampling_rate, padding_length)     \n",
    "    \n",
    "    plt.figure(figsize=(30, 6))  \n",
    "    plt.plot(time, output_raw, label=\"Original\", color=\"C0\", linewidth=2)  # Use color and linewidth\n",
    "    plt.plot(time, output, label=\"Low-pass filtered\", color=\"C1\", linestyle=\"-\", linewidth=1.5)  # Dashed line for prediction\n",
    "    plt.xlabel(\"Time (year)\", fontsize=14)\n",
    "    plt.ylabel(names_str, fontsize=14)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.legend(fontsize=12, loc=\"best\")\n",
    "    plt.savefig(os.path.join(output_dir, names_str+\"_LPF.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    input  =  input_raw\n",
    "    output = output_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#                       define the neural network                     #\n",
    "#######################################################################\n",
    "\n",
    "log_path =os.path.join(output_dir, 'training_logs.txt')\n",
    "os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "class PrintTrainingOnTextEvery10EpochsCallback(Callback):\n",
    "    def __init__(self, log_path):\n",
    "        super().__init__()\n",
    "        self.log_path = log_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 10 == 0:  # Log every 10 epochs\n",
    "            with open(self.log_path, \"a\") as log_file:\n",
    "                log_file.write(\n",
    "                    f\"Epoch: {epoch:>3} | \"\n",
    "                    f\"Loss: {logs.get('loss', 0):.2e} | \"\n",
    "#                        f\"Accuracy: {logs.get('accuracy', 0):.2e} | \"\n",
    "                    f\"Validation loss: {logs.get('val_loss', 0):.2e} |\\n \"\n",
    "#                        f\"Validation accuracy: {logs.get('val_accuracy', 0):.2e}\\n\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"Epoch {epoch:>3} - \"\n",
    "                    f\"Loss: {logs.get('loss', 0):.2e}, \"\n",
    "#                        f\"Accuracy: {logs.get('accuracy', 0):.2e}, \"\n",
    "                    f\"Validation loss: {logs.get('val_loss', 0):.2e}, \"\n",
    "#                        f\"Validation accuracy: {logs.get('val_accuracy', 0):.2e}\"\n",
    "                )\n",
    "\n",
    "my_callbacks = [\n",
    "    PrintTrainingOnTextEvery10EpochsCallback(log_path=log_path),\n",
    "]   \n",
    "\n",
    "        \n",
    "#########ORIGINAL#########        \n",
    "# def train_model(X_train, y_train, X_test, y_test,y_mean,y_std):\n",
    "                \n",
    "#     # Create a new model with random initial weights and biases\n",
    "#     inputs = tf.keras.Input(shape=(X_train.shape[1:]),)\n",
    "#     layers = inputs\n",
    "\n",
    "#     for kernel, kernel_act in zip(kernels, kernel_acts):\n",
    "#         layers = Conv2D(\n",
    "#             kernel,\n",
    "#             (kernel_size, kernel_size),\n",
    "#             strides=(stride, stride),\n",
    "#             use_bias=True,\n",
    "#             activation=kernel_act,\n",
    "#             padding=\"same\",\n",
    "#             bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "#             kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "#         )(layers)\n",
    "#         layers = MaxPooling2D((pool_size, pool_size))(layers)\n",
    "    \n",
    "    \n",
    "#     # make final dense layers\n",
    "#     layers = Flatten()(layers)\n",
    "    # for hidden, activation in zip(hiddens, activation_function_dense):\n",
    "    #     layers = Dense(\n",
    "    #         hidden,\n",
    "    #         activation=activation,\n",
    "    #         use_bias=True,\n",
    "    #         bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    #         kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    #     )(layers)\n",
    "    \n",
    "    \n",
    "    # output_layer = Dense(\n",
    "    #     y_train.shape[-1],\n",
    "    #     activation=\"linear\",\n",
    "    #     use_bias=True,\n",
    "    #     bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    #     kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    # )(layers)\n",
    "    \n",
    "    # # CONSTRUCT THE MODEL\n",
    "    # model = Model(inputs, output_layer)\n",
    "\n",
    "    \n",
    "    # if fold_no + ens_no == 2:\n",
    "    #     # show the model summary\n",
    "    #     model.summary()\n",
    "    #     with open(os.path.join(output_dir, 'model_' + names_str  + '.txt'), 'w') as f:\n",
    "    #         model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "    #     dot_img_file = os.path.join(output_dir, 'model_' + names_str  + '.png')\n",
    "    #     plot_model(model, to_file=dot_img_file,\n",
    "    #                show_shapes=True,\n",
    "    #                show_dtype=False,\n",
    "    #                show_layer_names=True,\n",
    "    #                rankdir='LR',\n",
    "    #                expand_nested=False,\n",
    "    #                dpi=300,\n",
    "    #                show_layer_activations=True)\n",
    "        \n",
    "    # # Compile the model with mean squared error loss and Adam optimizer\n",
    "    # model.compile(loss=loss_function, optimizer=Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    # # Define early stopping callback\n",
    "    # early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='auto', restore_best_weights=1, verbose=1)\n",
    "\n",
    "    # # Train the model with early stopping callback\n",
    "    # history = model.fit(X_train, y_train, epochs=epoch_max, batch_size=batch_size,\n",
    "    #                     validation_data=(X_test, y_test), callbacks=[early_stopping,my_callbacks],verbose=0)\n",
    "\n",
    "    # # Evaluate the performance on the testing set (less useful)\n",
    "    # skill = model.evaluate(X_test, y_test, verbose=0) \n",
    "\n",
    "\n",
    "    # # Make the prediction with the actual scale in the testing set\n",
    "    # pred = (model.predict(X_test)*y_std)+y_mean\n",
    "    \n",
    "    # # Calculate the R2 value in the testing set at each latitude \n",
    "    # R2_val = []\n",
    "    # truth = (y_test*y_std)+y_mean \n",
    "    \n",
    "    # for latind in range(y_test.shape[1]):\n",
    "    #     y_lat = truth[:, latind];\n",
    "    #     y_pred_lat = pred[:, latind];\n",
    "    #     R2_val.append(r2_score(y_lat, y_pred_lat))\n",
    "        \n",
    "    # model.save(os.path.join(output_dir,'model_fold'+str(fold_no)+'_ens'+str(ens_no)+'.h5'))\n",
    "\n",
    "\n",
    "    # return skill, history, pred, R2_val\n",
    "        \n",
    "def train_model(X_train, y_train, X_test, y_test, y_mean, y_std):\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(X_train.shape[1:]),)\n",
    "    layers = inputs\n",
    "\n",
    "    # Convolutional Layers\n",
    "    for i, kernel_filters in enumerate(kernels): \n",
    "        layers = Conv2D(\n",
    "            kernel_filters,\n",
    "            (kernel_size, kernel_size),\n",
    "            strides=(stride, stride),     \n",
    "            use_bias=True,\n",
    "            padding=\"same\",\n",
    "            # NO 'activation' argument here if PReLU or another layer follows\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "        )(layers)\n",
    "\n",
    "        # Apply PReLU (or other chosen activation)\n",
    "        if kernel_acts[i].lower() == 'prelu':\n",
    "            layers = PReLU(shared_axes=[1, 2],\n",
    "                           alpha_initializer=tf.keras.initializers.Constant(0.25) # Common starting point\n",
    "                          )(layers) # shared_axes for Conv2D with channels_last\n",
    "        elif kernel_acts[i].lower() == 'gelu':\n",
    "            layers = tf.keras.layers.Activation('gelu')(layers) # Using Activation layer for GeLU\n",
    "        elif kernel_acts[i]: # For 'relu', 'elu', etc.\n",
    "            layers = tf.keras.layers.Activation(kernel_acts[i])(layers)\n",
    "        # If kernel_acts[i] is None or an empty string, no explicit activation layer added here.\n",
    "\n",
    "        layers = MaxPooling2D((pool_size, pool_size))(layers)\n",
    "    \n",
    "    layers = Flatten()(layers)\n",
    "\n",
    "    # Dense Layers\n",
    "    for i, hidden_units in enumerate(hiddens):\n",
    "        layers = Dense(\n",
    "            hidden_units,\n",
    "            use_bias=True,\n",
    "            # NO 'activation' argument here\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "        )(layers)\n",
    "\n",
    "        # Apply PReLU (or other chosen activation)\n",
    "        if activation_function_dense[i].lower() == 'prelu':\n",
    "            layers = PReLU(alpha_initializer=tf.keras.initializers.Constant(0.25))(layers) # No shared_axes for Dense\n",
    "        elif activation_function_dense[i].lower() == 'gelu':\n",
    "            layers = tf.keras.layers.Activation('gelu')(layers)\n",
    "        elif activation_function_dense[i]: # For 'elu', 'relu', etc.\n",
    "            layers = tf.keras.layers.Activation(activation_function_dense[i])(layers)\n",
    "        # If activation_function_dense[i] is None or an empty string, no explicit activation layer.\n",
    "\n",
    "    output_layer = Dense(\n",
    "        y_train.shape[-1], \n",
    "        activation=\"linear\",\n",
    "        use_bias=True,\n",
    "        bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "        kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    )(layers)\n",
    "    \n",
    "    model = Model(inputs, output_layer)\n",
    "\n",
    "    if fold_no + ens_no == 2:\n",
    "        model.summary()\n",
    "\n",
    "    model.compile(loss=loss_function, optimizer=Adam(learning_rate=learning_rate)) \n",
    "\n",
    "    early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='auto', restore_best_weights=True, verbose=1) # restore_best_weights=1 is True\n",
    "\n",
    "    # Make sure my_callbacks is defined in this scope or passed as an argument\n",
    "    history = model.fit(X_train, y_train, epochs=epoch_max, batch_size=batch_size,\n",
    "                        validation_data=(X_test, y_test), callbacks=[early_stopping] + my_callbacks, verbose=0) # Added '+' for list concatenation\n",
    "\n",
    "    skill = model.evaluate(X_test, y_test, verbose=0)\n",
    "    pred = (model.predict(X_test) * y_std) + y_mean # Make sure y_std and y_mean are defined\n",
    "\n",
    "    R2_val = []\n",
    "    truth = (y_test * y_std) + y_mean\n",
    "    for latind in range(truth.shape[1] if truth.ndim > 1 else 1): # Handle if y_test is 1D output\n",
    "        y_lat = truth[:, latind] if truth.ndim > 1 else truth\n",
    "        y_pred_lat = pred[:, latind] if pred.ndim > 1 else pred\n",
    "        R2_val.append(r2_score(y_lat, y_pred_lat))\n",
    "    \n",
    "    model.save(os.path.join(output_dir,'model_fold'+str(fold_no)+'_ens'+str(ens_no)+'.h5'))\n",
    "    \n",
    "    return skill, history, pred, R2_val\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff17f61",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#                             Train loop                              #\n",
    "#######################################################################\n",
    "X = input\n",
    "y = output\n",
    "\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "\n",
    "y_mean = np.mean(y, axis=0)\n",
    "y_std = np.std(y, axis=0)\n",
    "\n",
    "if remove_mean ==0:\n",
    "    X_mean = 0\n",
    "    y_mean = 0\n",
    "\n",
    "    \n",
    "    \n",
    "if divide_std ==0:\n",
    "    X_std = 1\n",
    "    y_std = 1\n",
    "else:\n",
    "    X_std = X_std[:,:, tf.newaxis]\n",
    "    \n",
    "    \n",
    "    \n",
    "sio.savemat(os.path.join(output_dir,'Normalization.mat'), \n",
    "            {'X_mean': X_mean, 'X_std': X_std,'y_mean':y_mean,'y_std':y_std})\n",
    "\n",
    "# if not os.path.exists(os.path.join(output_dir, 'inputs_info.mat')):\n",
    "\n",
    "        \n",
    "trained_models = []\n",
    "y_pred_reconstructed_allfolds = []\n",
    "\n",
    "\n",
    "# Train the neural network multiple times using k-fold cross validation\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "fold_no = 1\n",
    "for trainind, testind in kfold.split(X, y):   \n",
    "    # break\n",
    "    X_train = X[trainind,:,:, tf.newaxis]\n",
    "    y_train = y[trainind,:]\n",
    "    X_test = X[testind,:, :,tf.newaxis]\n",
    "    y_test = y[testind,:]\n",
    " \n",
    "    # Normalize the input and out data based on the information from the entire PI control run\n",
    "    \n",
    "    \n",
    "    X_train = (X_train - X_mean)/X_std\n",
    "    X_test = (X_test - X_mean)/X_std\n",
    "    \n",
    "    y_train = (y_train - y_mean)/y_std\n",
    "    y_test = (y_test - y_mean)/y_std\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Generate a print\n",
    "    logger.info('------------------------------------------------------------------------')\n",
    "    # We use ensemble training for each fold of the cross-validation\n",
    "    for ens_no  in np.arange(1,NNrepeats+1):\n",
    "        logger.info(f'Training for fold {fold_no} ensemble {ens_no}...')\n",
    "        skill, history, pred, R2_val = train_model(X_train, y_train, X_test, y_test,y_mean,y_std)\n",
    "        trained_models.append((skill, skill, history.history, pred, R2_val))\n",
    "\n",
    "        \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "        \n",
    "        \n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info(f'Training with {num_folds}-fold cross-validation finished!') \n",
    "logger.info('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be32e148",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "have a quick look at the performance of the trainined neural network"
   },
   "outputs": [],
   "source": [
    "\n",
    "skills = [trained_model[1] for trained_model in trained_models[0:num_folds*NNrepeats]]\n",
    "median_skill = np.median(skills)\n",
    "median_index = skills.index(median_skill)\n",
    "minimum_skill = np.min(skills)\n",
    "min_index = skills.index(minimum_skill)\n",
    "std_dev_skill = np.std(skills)\n",
    "\n",
    "formatted_skills = [f\"{skill:.4f}\" for skill in skills]\n",
    "logger.info(f\"Losses in testing set: {formatted_skills}\")\n",
    "fold_median = median_index // NNrepeats + 1  # Compute fold number\n",
    "ensemble_median = median_index % NNrepeats + 1  # Compute ensemble number\n",
    "logger.info(f\"Median Loss is: {median_skill:.4f}, which occurs at Fold {fold_median} Ensemble {ensemble_median}\")\n",
    "fold_min = min_index // NNrepeats + 1  # Compute fold number\n",
    "ensemble_min = min_index % NNrepeats + 1  # Compute ensemble number\n",
    "logger.info(f\"Minimum Loss is: {minimum_skill:.4f}, which occurs at Fold {fold_min} Ensemble {ensemble_min}\")\n",
    "logger.info(f\"Standard Deviation of the Loss is: {std_dev_skill:.4f}\")\n",
    "ratio = std_dev_skill/median_skill * 100\n",
    "logger.info(f\"Standard Deviation/Median: {ratio:.2f}%\")\n",
    "\n",
    "# Check if the standard deviation to median skill ratio is higher than 30%\n",
    "if ratio > 30:\n",
    "    warning_message = (\"Warning: The standard deviation of model skill across \"\n",
    "                       \"folds and ensembles is large!\\nStandard Deviation/Median Skill: {ratio:.2f}%\").format(ratio=ratio)\n",
    "    with open(os.path.join(output_dir,'Warning_'+names_str+'.txt'), \"w\") as file:\n",
    "        file.write(warning_message)\n",
    "    with open(os.path.join(output_dir,'..','Warning_'+names_str+'.txt'), \"w\") as file:\n",
    "        file.write(warning_message)\n",
    "\n",
    "\n",
    "# Plot the loss function during training for all folds\n",
    "# First, find the global minimum and maximum loss values across all folds\n",
    "min_loss = min(min(trained_models[i][2]['loss']+trained_models[i][2]['val_loss']) for i in range(num_folds))\n",
    "max_loss = max(max(trained_models[i][2]['loss']+trained_models[i][2]['val_loss']) for i in range(num_folds))\n",
    "\n",
    "plt.figure(figsize=(9, 12))\n",
    "\n",
    "# Plot the first fold outside the loop to avoid repeating the legend setting\n",
    "plt.subplot(num_folds, 1, 1)\n",
    "for nn in range(NNrepeats):\n",
    "    plt.plot(trained_models[nn][2]['loss'],'-k')\n",
    "    plt.plot(trained_models[nn][2]['val_loss'],'-r')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')  # Set logarithmic scale\n",
    "plt.ylim(min_loss, max_loss)  # Set the same y-limits for all subplots\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Now plot the remaining folds\n",
    "for n in range(1, num_folds):\n",
    "    plt.subplot(num_folds, 1, n+1)\n",
    "    for nn in range(NNrepeats):\n",
    "        plt.plot(trained_models[n*NNrepeats+nn][2]['loss'],'-k')\n",
    "        plt.plot(trained_models[n*NNrepeats+nn][2]['val_loss'],'-r')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')  # Set logarithmic scale\n",
    "    plt.ylim(min_loss, max_loss)  # Set the same y-limits for all subplots\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.subplots_adjust(hspace=0.5)  # Adjust space between plots if needed\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'TrainingLoss_' + names_str  + '.png'),dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "R2_each_fold_ens = [trained_models[i][4] for i in range(0,num_folds*NNrepeats)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ad78e-d14f-4e43-a2c7-5b2b93783761",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "## Model_preds = [np.concatenate([trained_models[n*NNrepeats + i][3] for n in range(num_folds)]) for i in range(NNrepeats)]\n",
    "Model_pred = np.mean(Model_preds,axis=0)\n",
    "Model_error = Model_pred - y\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30, 6))  \n",
    "plt.plot(time, y, label=\"Truth\", color=\"C0\", linewidth=2)  # Use color and linewidth\n",
    "plt.plot(time, Model_pred, label=\"Prediction\", color=\"C1\", linestyle=\"-\", linewidth=1.5)  # Dashed line for prediction\n",
    "plt.xlabel(\"Time (year)\", fontsize=14)\n",
    "plt.ylabel(names_str, fontsize=14)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend(fontsize=12, loc=\"best\")\n",
    "r2 = r2_score(y, Model_pred)\n",
    "plt.text(0.05, 0.9, f\"$R^2$: {r2:.3f}\", transform=plt.gca().transAxes,\n",
    "         fontsize=14, bbox=dict(facecolor=\"white\", alpha=0.7, edgecolor=\"black\"))\n",
    "plt.savefig(os.path.join(output_dir, names_str+\"_TruthvsPred.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "######### scatter plot\n",
    "\n",
    "plt.figure(figsize=(8, 8))  # Keep square aspect ratio\n",
    "\n",
    "# Scatter plot with adjusted marker size and transparency\n",
    "plt.plot(Model_pred, y, 'o', markersize=5, alpha=0.6, color=\"C0\", label=\"Data Points\")\n",
    "\n",
    "# Compute common limits based on both Model_pred and y\n",
    "min_val = min(np.min(Model_pred), np.min(y))\n",
    "max_val = max(np.max(Model_pred), np.max(y))\n",
    "\n",
    "# Set same limits for x and y\n",
    "plt.xlim(min_val, max_val)\n",
    "plt.ylim(min_val, max_val)\n",
    "\n",
    "# Define consistent tick locations\n",
    "num_ticks = 6  # Adjust this for more or fewer ticks\n",
    "ticks = np.linspace(min_val, max_val, num_ticks)\n",
    "\n",
    "plt.xticks(ticks)\n",
    "plt.yticks(ticks)\n",
    "\n",
    "# 1:1 Reference Line\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'k--', linewidth=2, label=\"1:1 Reference\")\n",
    "\n",
    "# Labels and formatting\n",
    "plt.xlabel(\"Prediction\", fontsize=16, fontweight='bold')\n",
    "plt.ylabel(\"Truth\", fontsize=16, fontweight='bold')\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Compute and display RÂ² score\n",
    "r2 = r2_score(y, Model_pred)\n",
    "plt.text(0.05, 0.9, f\"$R^2$: {r2:.3f}\", transform=plt.gca().transAxes,\n",
    "         fontsize=14, bbox=dict(facecolor=\"white\", alpha=0.7, edgecolor=\"black\"))\n",
    "\n",
    "# Equal aspect ratio for fair comparison\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "# Add legend (only for reference line)\n",
    "plt.legend(fontsize=12, loc=\"center left\")\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(os.path.join(output_dir, \"TOA_TruthvsPred_scatter.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8fc216",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "copy this script to the directory that stores the trained NN"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def copy_script(target_directory):\n",
    "    # Get the current script file path\n",
    "    current_script = \"C2 training.ipynb\"\n",
    "    # current_script = script_path_and_name\n",
    "    \n",
    "    # Ensure the target directory exists, create if it does not\n",
    "    os.makedirs(target_directory, exist_ok=True)\n",
    "    \n",
    "    # Define the target path for the script\n",
    "    target_path = os.path.join(target_directory, os.path.basename(current_script))\n",
    "    \n",
    "    # Copy the script\n",
    "    shutil.copy(current_script, target_path)\n",
    "    logger.info(f\"Script copied to {target_path}\")\n",
    "\n",
    "# Example usage\n",
    "copy_script(output_dir)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f000a2d-5dd5-4f87-830e-700069c96046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python (tf210)",
   "language": "python",
   "name": "tf210"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
