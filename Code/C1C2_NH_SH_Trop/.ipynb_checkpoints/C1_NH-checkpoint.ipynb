{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89b95079-1049-42f9-a3a0-d9b01d5ad35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is for NH training data for CESM1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Mon Feb 24 10:04:04 2025\n",
    "\n",
    "@author: hwei\n",
    "\"\"\"\n",
    "\"This is for NH training data for CESM1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ebaf0b-cea4-4186-9e29-fde82d440ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:27:21.575119: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-13 11:27:22.921998: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-13 11:27:23.192628: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# V3 25-03-2025 updated normalization\n",
    "\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense,LeakyReLU, Dropout, Add, Activation, Conv2D, Flatten, MaxPooling2D, Dense, PReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from matplotlib.colors import Normalize\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "tf.keras.utils.set_random_seed(0)\n",
    "tf.config.list_physical_devices('GPU')  \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras.engine.training_v1\")\n",
    "import xarray as xr\n",
    "import innvestigate\n",
    "import scipy.io as sio\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c3435-9030-45fd-9f63-4821b48f1294",
   "metadata": {},
   "source": [
    "##IMPORTANT!!\n",
    "NEED TO CHANGE INPUT/OUTPUT path for CESM1/CESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "560eda33",
   "metadata": {
    "title": "load data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (lat: 192, lon: 288, time: 1801)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 -90.0 -89.06 -88.12 -87.17 ... 87.17 88.12 89.06 90.0\n",
      "  * lon      (lon) float64 0.0 1.25 2.5 3.75 5.0 ... 355.0 356.2 357.5 358.8\n",
      "  * time     (time) int64 400 401 402 403 404 405 ... 2196 2197 2198 2199 2200\n",
      "Data variables:\n",
      "    TS       (time, lat, lon) float32 ...\n",
      "    TS_anom  (time, lat, lon) float32 ...\n",
      "Attributes:\n",
      "    script:   /glade/work/dongy24/Python/create_input_for_huaiyu.ipynb\n",
      "    author:   Y. Dong, 03/24/2025\n"
     ]
    }
   ],
   "source": [
    "# load the input variable -- global Surface temperature\n",
    "\n",
    "#CESM1 control\n",
    "ds = xr.open_dataset(\"/ocean/projects/ees250004p/ezhu3/data/CESM1/control/input.B1850C5CN.TS.ANN.0400-2200.new.nc\")\n",
    "\n",
    "#CESM2 data 1: \n",
    "#ds = xr.open_dataset(\"/ocean/projects/ees250004p/ezhu3/data/CESM2/control/input.CESM2-B1850.TS_detrend.ANN.nc\")\n",
    "\n",
    "#CESM2 data2:\n",
    "#ds = xr.open_dataset(\"/ocean/projects/ees250004p/ezhu3/data/CESM2/control/input.CESM2-B1850.TS.ANN.nc\")\n",
    "\n",
    "#ds = xr.open_dataset(\"E:\\\\Yue\\\\CESM2\\\\control\\\\input.CESM2-B1850.TS_detrend.ANN.nc\")# Display dataset info\n",
    "print(ds)\n",
    "\n",
    "# Access a specific variable\n",
    "TS = ds[\"TS_anom\"] # Surface temperature (radiative)     units = 'K'\n",
    "lat = ds[\"lat\"] #latitude\n",
    "lon = ds[\"lon\"] #longitude\n",
    "time = ds[\"time\"]\n",
    "\n",
    "\n",
    "# Define the directory where the output variables are stored\n",
    "data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM1/control' #this is for CESM1\n",
    "# Define the filenames and corresponding variable names\n",
    "files = {\n",
    "    #CESM1:\n",
    "    \"TOA_anom\": \"output.B1850C5CN.TOA.NH.ANN.0400-2200.new.nc\"\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "# Load variables\n",
    "for var, filename in files.items():\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    datasets[var] = ds[var]  # Extract only the variable\n",
    "\n",
    "\n",
    "# Now the variables can be accessed as datasets[\"CRE\"], datasets[\"TOA\"], etc.\n",
    "\n",
    "\n",
    "# path for storing the trained neural networks\n",
    "#data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM2/trained_model_data1_Gelu'#CESM2 data1\n",
    "#data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM2/trained_model_data2_Gelu'#CESM2 data2 gelu\n",
    "#data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM1/trained_model'#CESM1\n",
    "#data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM1/trained_model_Gelu'#CESM1 Gelu\n",
    "\n",
    "#original \n",
    "#data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM1/trained_for_NH'\n",
    "data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM1/trained_for_NH_map'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3dd24db",
   "metadata": {
    "title": "Neural Network Hyperparameters"
   },
   "outputs": [],
   "source": [
    "# Neural network architecture:\n",
    "# Example: [64, 64, 64] means three hidden layers, each containing 64 neurons.\n",
    "kernels = [32, 32]\n",
    "kernel_acts =  [\"gelu\", \"gelu\"]\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "rng_seed = 42\n",
    "hiddens = [16, 8]\n",
    "activation_function_dense = [\"PRelu\", \"PRelu\"]\n",
    "pool_size = 2\n",
    "\n",
    "# Loss function for regression tasks:\n",
    "# Options: 'mse' (mean squared error), 'mae' (mean absolute error), 'mape' (mean absolute percentage error)\n",
    "# Full list of regression losses: https://keras.io/api/losses/\n",
    "loss_function = 'mse'\n",
    "\n",
    "\n",
    "reg_strength = 0         # L2 regularization strength; 1e-1~1e-5\n",
    "dropout_rate = 0.25         # Dropout rate (0.0 to disable dropout)\n",
    "\n",
    "\n",
    "# low-pass filter time scale; 0 means no low-pass filter\n",
    "LPF_year = 0\n",
    "\n",
    "\n",
    "#### normalization\n",
    "remove_mean = 1\n",
    "divide_std = 1\n",
    "\n",
    "\n",
    "#### usually we do not change the parameters below \n",
    "\n",
    "# Training configuration\n",
    "epoch_max = 25000            # Maximum number of training epochs\n",
    "batch_size = 32            # Batch size used during training\n",
    "learning_rate = 0.000005       # Default learning rate for Adam optimizer 0.001\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "num_folds = 5 # Number of fold during cross-validation\n",
    "NNrepeats = 1 # Repeat the training for NNrepeats times\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c050e93",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "pre-process data"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# input_raw = TS.values.reshape(TS.shape[0],TS.shape[1]*TS.shape[2])\n",
    "# For CNN, we don't need to flatten the data\n",
    "input_raw = TS.values\n",
    "\n",
    "\n",
    "\n",
    "# Define the variable names as a comma-separated string\n",
    "# names_strALL = \"CRE,FLNT,FSNT,LCC,LWCF,SWCF,TCC,TOA\"\n",
    "# If we only want to reconstruct TOA\n",
    "# names_strALL = \"TOA_anom\"\n",
    "# names_str = \"TOA_anom\"\n",
    "\n",
    "#change here for FLNT\n",
    "names_strALL = \"TOA_anom\"\n",
    "names_str = \"TOA_anom\"\n",
    "output_raw = datasets[names_str].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b49bea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_NN_name():\n",
    "\n",
    "    NN_name = 'CNN'\n",
    "        \n",
    "    activation_function_str = '_'+kernel_acts[0]+ '+'+activation_function_dense[0] \n",
    "    loss_function_str = '_'+str(loss_function)+'loss' if loss_function!='mse' else ''\n",
    "    \n",
    "    if remove_mean + divide_std == 0:\n",
    "        scaler_str = 'NoScaler_'\n",
    "    elif remove_mean + divide_std == 2:\n",
    "        scaler_str = 'StandardScaler_'\n",
    "    elif remove_mean:\n",
    "        scaler_str = 'RemoveMean_'\n",
    "    elif divide_std:\n",
    "        scaler_str = 'DivideSTD_'\n",
    "    LPF_str = f'_LPF{int(LPF_year)}Year' if LPF_year else ''\n",
    "    NN_structure_str = 'x'.join(map(str, kernels)) \n",
    "    reg_str = f'Reg{reg_strength}' + (f'Drop{dropout_rate}' if dropout_rate != 0 else '')\n",
    "    batch_size_str =  f'BS{batch_size}_' if batch_size !=600 else ''\n",
    "    return f\"{NN_name}_{scaler_str}Neur{NN_structure_str}_{batch_size_str}{num_folds}foldCV_{reg_str}{loss_function_str}{activation_function_str}{LPF_str}\"\n",
    "NN_name = create_NN_name()\n",
    "\n",
    "    \n",
    "def construct_output_directory(data_dir, NN_name):\n",
    "    output_dir = os.path.join(data_dir, 'NeuralNet', NN_name, names_str)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "output_dir = construct_output_directory(data_dir, NN_name)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "840c1afa",
   "metadata": {
    "title": "Define logger that can print useful information into a logfile"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:27:31,446 - logfile\n",
      "2025-08-13 11:27:31.472098: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-13 11:27:32,473 - Using GPU for training.\n",
      "2025-08-13 11:27:32.467163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31088 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b3:00.0, compute capability: 7.0\n",
      "2025-08-13 11:27:32,474 - 1 Physical GPUs, 1 Logical GPUs\n",
      "2025-08-13 11:27:32,474 - None\n",
      "2025-08-13 11:27:32,475 - Output path: created successfully\n",
      "2025-08-13 11:27:32,475 - The output path is /ocean/projects/ees250004p/ezhu3/data/CESM1/trained_for_NH_map/NeuralNet/CNN_StandardScaler_Neur32x32_BS32_5foldCV_Reg0Drop0.25_gelu+PRelu/TOA_anom\n"
     ]
    }
   ],
   "source": [
    "logger_name = 'logfile.log'  \n",
    "# Configure logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create handlers\n",
    "file_handler = logging.FileHandler(os.path.join(output_dir, logger_name))\n",
    "console_handler = logging.StreamHandler()\n",
    "\n",
    "# Set level and format for handlers\n",
    "file_handler.setLevel(logging.INFO)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Test the setup\n",
    "logger.info(\"logfile\")\n",
    "\n",
    " \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    logger.info(\"Using GPU for training.\")\n",
    "    logger.info(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "else:\n",
    "    logger.info(\"Using CPU for training.\") \n",
    "\n",
    "logger.info(os.getenv('TF_GPU_ALLOCATOR'))\n",
    "    \n",
    "logger.info(f\"Output path: {'created successfully' if os.path.exists(output_dir) else 'already exists'}\")\n",
    "logger.info(f\"The output path is {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2d9886a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:27:32,483 - ------------------------------------------------------------------------\n",
      "2025-08-13 11:27:32,484 - ------------------------------------------------------------------------\n",
      "2025-08-13 11:27:32,484 - Applying 0-year low pass filter ...\n",
      "2025-08-13 11:27:32,485 - ------------------------------------------------------------------------\n",
      "2025-08-13 11:27:32,485 - ------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info(f'Applying {LPF_year}-year low pass filter ...')\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "\n",
    "from scipy.signal import butter, sosfilt\n",
    "\n",
    "def apply_low_pass_filter(data, cutoff_freq, order=5, sampling_rate=1, padding_length=None):\n",
    "    \"\"\"Applies a Butterworth low-pass filter to the given data.\"\"\"\n",
    "    sos = butter(order, cutoff_freq, btype='low', output='sos', analog=False, fs=sampling_rate)\n",
    "    \n",
    "    # Apply Boundary Padding to the data before filtering\n",
    "    padded_data = np.pad(data, [(padding_length, padding_length), (0, 0)], mode='reflect')\n",
    "    \n",
    "    # Apply the filter across each column without explicit looping\n",
    "    filtered_data = sosfilt(sos, padded_data, axis=0)\n",
    "    \n",
    "    # Remove padding\n",
    "    return filtered_data[padding_length:-padding_length]\n",
    "\n",
    "if LPF_year:\n",
    "    # Calculate the sampling rate (monthly data)\n",
    "    sampling_rate = 1  # Data is sampled monthly    \n",
    "    cutoff_freq = 1 / LPF_year\n",
    "    order = 5\n",
    "    padding_length =  3* LPF_year\n",
    "\n",
    "if LPF_year:\n",
    "    input = apply_low_pass_filter(input_raw, cutoff_freq, order, sampling_rate, padding_length)     \n",
    "    output= apply_low_pass_filter(output_raw, cutoff_freq, order, sampling_rate, padding_length)     \n",
    "    \n",
    "    plt.figure(figsize=(30, 6))  \n",
    "    plt.plot(time, output_raw, label=\"Original\", color=\"C0\", linewidth=2)  # Use color and linewidth\n",
    "    plt.plot(time, output, label=\"Low-pass filtered\", color=\"C1\", linestyle=\"-\", linewidth=1.5)  # Dashed line for prediction\n",
    "    plt.xlabel(\"Time (year)\", fontsize=14)\n",
    "    plt.ylabel(names_str, fontsize=14)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.legend(fontsize=12, loc=\"best\")\n",
    "    plt.savefig(os.path.join(output_dir, names_str+\"_LPF.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    input  =  input_raw\n",
    "    output = output_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d30f033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#                       define the neural network                     #\n",
    "#######################################################################\n",
    "\n",
    "log_path =os.path.join(output_dir, 'training_logs.txt')\n",
    "os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "class PrintTrainingOnTextEvery10EpochsCallback(Callback):\n",
    "    def __init__(self, log_path):\n",
    "        super().__init__()\n",
    "        self.log_path = log_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 10 == 0:  # Log every 10 epochs\n",
    "            with open(self.log_path, \"a\") as log_file:\n",
    "                log_file.write(\n",
    "                    f\"Epoch: {epoch:>3} | \"\n",
    "                    f\"Loss: {logs.get('loss', 0):.2e} | \"\n",
    "#                        f\"Accuracy: {logs.get('accuracy', 0):.2e} | \"\n",
    "                    f\"Validation loss: {logs.get('val_loss', 0):.2e} |\\n \"\n",
    "#                        f\"Validation accuracy: {logs.get('val_accuracy', 0):.2e}\\n\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"Epoch {epoch:>3} - \"\n",
    "                    f\"Loss: {logs.get('loss', 0):.2e}, \"\n",
    "#                        f\"Accuracy: {logs.get('accuracy', 0):.2e}, \"\n",
    "                    f\"Validation loss: {logs.get('val_loss', 0):.2e}, \"\n",
    "#                        f\"Validation accuracy: {logs.get('val_accuracy', 0):.2e}\"\n",
    "                )\n",
    "\n",
    "my_callbacks = [\n",
    "    PrintTrainingOnTextEvery10EpochsCallback(log_path=log_path),\n",
    "]   \n",
    "\n",
    "        \n",
    "#########ORIGINAL#########        \n",
    "# def train_model(X_train, y_train, X_test, y_test,y_mean,y_std):\n",
    "                \n",
    "#     # Create a new model with random initial weights and biases\n",
    "#     inputs = tf.keras.Input(shape=(X_train.shape[1:]),)\n",
    "#     layers = inputs\n",
    "\n",
    "#     for kernel, kernel_act in zip(kernels, kernel_acts):\n",
    "#         layers = Conv2D(\n",
    "#             kernel,\n",
    "#             (kernel_size, kernel_size),\n",
    "#             strides=(stride, stride),\n",
    "#             use_bias=True,\n",
    "#             activation=kernel_act,\n",
    "#             padding=\"same\",\n",
    "#             bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "#             kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "#         )(layers)\n",
    "#         layers = MaxPooling2D((pool_size, pool_size))(layers)\n",
    "    \n",
    "    \n",
    "#     # make final dense layers\n",
    "#     layers = Flatten()(layers)\n",
    "    # for hidden, activation in zip(hiddens, activation_function_dense):\n",
    "    #     layers = Dense(\n",
    "    #         hidden,\n",
    "    #         activation=activation,\n",
    "    #         use_bias=True,\n",
    "    #         bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    #         kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    #     )(layers)\n",
    "    \n",
    "    \n",
    "    # output_layer = Dense(\n",
    "    #     y_train.shape[-1],\n",
    "    #     activation=\"linear\",\n",
    "    #     use_bias=True,\n",
    "    #     bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    #     kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    # )(layers)\n",
    "    \n",
    "    # # CONSTRUCT THE MODEL\n",
    "    # model = Model(inputs, output_layer)\n",
    "\n",
    "    \n",
    "    # if fold_no + ens_no == 2:\n",
    "    #     # show the model summary\n",
    "    #     model.summary()\n",
    "    #     with open(os.path.join(output_dir, 'model_' + names_str  + '.txt'), 'w') as f:\n",
    "    #         model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "    #     dot_img_file = os.path.join(output_dir, 'model_' + names_str  + '.png')\n",
    "    #     plot_model(model, to_file=dot_img_file,\n",
    "    #                show_shapes=True,\n",
    "    #                show_dtype=False,\n",
    "    #                show_layer_names=True,\n",
    "    #                rankdir='LR',\n",
    "    #                expand_nested=False,\n",
    "    #                dpi=300,\n",
    "    #                show_layer_activations=True)\n",
    "        \n",
    "    # # Compile the model with mean squared error loss and Adam optimizer\n",
    "    # model.compile(loss=loss_function, optimizer=Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    # # Define early stopping callback\n",
    "    # early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='auto', restore_best_weights=1, verbose=1)\n",
    "\n",
    "    # # Train the model with early stopping callback\n",
    "    # history = model.fit(X_train, y_train, epochs=epoch_max, batch_size=batch_size,\n",
    "    #                     validation_data=(X_test, y_test), callbacks=[early_stopping,my_callbacks],verbose=0)\n",
    "\n",
    "    # # Evaluate the performance on the testing set (less useful)\n",
    "    # skill = model.evaluate(X_test, y_test, verbose=0) \n",
    "\n",
    "\n",
    "    # # Make the prediction with the actual scale in the testing set\n",
    "    # pred = (model.predict(X_test)*y_std)+y_mean\n",
    "    \n",
    "    # # Calculate the R2 value in the testing set at each latitude \n",
    "    # R2_val = []\n",
    "    # truth = (y_test*y_std)+y_mean \n",
    "    \n",
    "    # for latind in range(y_test.shape[1]):\n",
    "    #     y_lat = truth[:, latind];\n",
    "    #     y_pred_lat = pred[:, latind];\n",
    "    #     R2_val.append(r2_score(y_lat, y_pred_lat))\n",
    "        \n",
    "    # model.save(os.path.join(output_dir,'model_fold'+str(fold_no)+'_ens'+str(ens_no)+'.h5'))\n",
    "\n",
    "\n",
    "    # return skill, history, pred, R2_val\n",
    "        \n",
    "def train_model(X_train, y_train, X_test, y_test, y_mean, y_std):\n",
    "    inputs = tf.keras.Input(shape=(X_train.shape[1:]),)\n",
    "    layers = inputs\n",
    "\n",
    "    # Convolutional Layers\n",
    "    for i, kernel_filters in enumerate(kernels): # Assuming kernels is a list of filter numbers\n",
    "        layers = Conv2D(\n",
    "            kernel_filters,\n",
    "            (kernel_size, kernel_size), # Assuming kernel_size is defined\n",
    "            strides=(stride, stride),     # Assuming stride is defined\n",
    "            use_bias=True,\n",
    "            padding=\"same\",\n",
    "            # NO 'activation' argument here if PReLU or another layer follows\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "        )(layers)\n",
    "\n",
    "        # Apply PReLU (or other chosen activation)\n",
    "        # Assuming kernel_acts[i] could be 'prelu', 'gelu', 'relu', etc.\n",
    "        if kernel_acts[i].lower() == 'prelu':\n",
    "            layers = PReLU(shared_axes=[1, 2],\n",
    "                           alpha_initializer=tf.keras.initializers.Constant(0.25) # Common starting point\n",
    "                          )(layers) # shared_axes for Conv2D with channels_last\n",
    "        elif kernel_acts[i].lower() == 'gelu':\n",
    "            layers = tf.keras.layers.Activation('gelu')(layers) # Using Activation layer for GeLU\n",
    "        elif kernel_acts[i]: # For 'relu', 'elu', etc.\n",
    "            layers = tf.keras.layers.Activation(kernel_acts[i])(layers)\n",
    "        # If kernel_acts[i] is None or an empty string, no explicit activation layer added here.\n",
    "\n",
    "        layers = MaxPooling2D((pool_size, pool_size))(layers) # Assuming pool_size is defined\n",
    "    \n",
    "    layers = Flatten()(layers)\n",
    "\n",
    "    # Dense Layers\n",
    "    for i, hidden_units in enumerate(hiddens): # Assuming hiddens is a list of unit numbers\n",
    "        layers = Dense(\n",
    "            hidden_units,\n",
    "            use_bias=True,\n",
    "            # NO 'activation' argument here\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "        )(layers)\n",
    "\n",
    "        # Apply PReLU (or other chosen activation)\n",
    "        # Assuming activation_function_dense[i] could be 'prelu', 'gelu', 'elu', etc.\n",
    "        if activation_function_dense[i].lower() == 'prelu':\n",
    "            layers = PReLU(alpha_initializer=tf.keras.initializers.Constant(0.25))(layers) # No shared_axes for Dense\n",
    "        elif activation_function_dense[i].lower() == 'gelu':\n",
    "            layers = tf.keras.layers.Activation('gelu')(layers)\n",
    "        elif activation_function_dense[i]: # For 'elu', 'relu', etc.\n",
    "            layers = tf.keras.layers.Activation(activation_function_dense[i])(layers)\n",
    "        # If activation_function_dense[i] is None or an empty string, no explicit activation layer.\n",
    "\n",
    "    output_layer = Dense(\n",
    "        y_train.shape[-1], # Assuming y_train is (samples, features_out) or (samples,)\n",
    "        activation=\"linear\",\n",
    "        use_bias=True,\n",
    "        bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "        kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    )(layers)\n",
    "    \n",
    "    model = Model(inputs, output_layer)\n",
    "\n",
    "\n",
    "    if fold_no + ens_no == 2: # Assuming these are defined in your script's scope\n",
    "        model.summary()\n",
    "    model.compile(loss=loss_function, optimizer=Adam(learning_rate=learning_rate)) # Assuming these are defined\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='auto', restore_best_weights=True, verbose=1) # restore_best_weights=1 is True\n",
    "\n",
    "    # Make sure my_callbacks is defined in this scope or passed as an argument\n",
    "    history = model.fit(X_train, y_train, epochs=epoch_max, batch_size=batch_size,\n",
    "                        validation_data=(X_test, y_test), callbacks=[early_stopping] + my_callbacks, verbose=0) # Added '+' for list concatenation\n",
    "\n",
    "    skill = model.evaluate(X_test, y_test, verbose=0)\n",
    "    pred = (model.predict(X_test) * y_std) + y_mean # Make sure y_std and y_mean are defined\n",
    "\n",
    "    R2_val = []\n",
    "    truth = (y_test * y_std) + y_mean\n",
    "    for latind in range(truth.shape[1] if truth.ndim > 1 else 1): # Handle if y_test is 1D output\n",
    "        y_lat = truth[:, latind] if truth.ndim > 1 else truth\n",
    "        y_pred_lat = pred[:, latind] if pred.ndim > 1 else pred\n",
    "        R2_val.append(r2_score(y_lat, y_pred_lat))\n",
    "    \n",
    "    model.save(os.path.join(output_dir,'model_fold'+str(fold_no)+'_ens'+str(ens_no)+'.h5'))\n",
    "    \n",
    "    return skill, history, pred, R2_val\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff17f61",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:27:33,356 - ------------------------------------------------------------------------\n",
      "2025-08-13 11:27:33,357 - Training for fold 1 ensemble 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 192, 288, 1)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 192, 288, 32)      320       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 192, 288, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 96, 144, 32)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 96, 144, 32)       9248      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 96, 144, 32)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 48, 72, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 110592)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                1769488   \n",
      "                                                                 \n",
      " p_re_lu (PReLU)             (None, 16)                16        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " p_re_lu_1 (PReLU)           (None, 8)                 8         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,779,225\n",
      "Trainable params: 1,779,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:27:33.729237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31088 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b3:00.0, compute capability: 7.0\n",
      "2025-08-13 11:27:33.751585: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2025-08-13 11:27:35.244661: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8201\n",
      "2025-08-13 11:27:36.837780: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-08-13 11:27:36.838584: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-08-13 11:27:36.838608: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2025-08-13 11:27:36.838952: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-08-13 11:27:36.838998: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 - Loss: 9.73e-01, Validation loss: 1.09e+00, \n",
      "Epoch  10 - Loss: 8.98e-01, Validation loss: 1.02e+00, \n",
      "Epoch  20 - Loss: 8.11e-01, Validation loss: 9.35e-01, \n",
      "Epoch  30 - Loss: 7.29e-01, Validation loss: 8.57e-01, \n",
      "Epoch  40 - Loss: 6.61e-01, Validation loss: 8.05e-01, \n",
      "Epoch  50 - Loss: 6.07e-01, Validation loss: 7.70e-01, \n",
      "Epoch  60 - Loss: 5.63e-01, Validation loss: 7.43e-01, \n",
      "Epoch  70 - Loss: 5.28e-01, Validation loss: 7.23e-01, \n",
      "Epoch  80 - Loss: 4.99e-01, Validation loss: 7.09e-01, \n",
      "Epoch  90 - Loss: 4.74e-01, Validation loss: 7.01e-01, \n",
      "Epoch 100 - Loss: 4.52e-01, Validation loss: 6.96e-01, \n",
      "Epoch 110 - Loss: 4.31e-01, Validation loss: 6.93e-01, \n",
      "Epoch 120 - Loss: 4.12e-01, Validation loss: 6.94e-01, \n",
      "Restoring model weights from the end of the best epoch: 117.\n",
      "Epoch 127: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:30:34,003 - ------------------------------------------------------------------------\n",
      "2025-08-13 11:30:34,004 - Training for fold 2 ensemble 1...\n",
      "2025-08-13 11:30:34.302317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31088 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b3:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 - Loss: 1.02e+00, Validation loss: 9.09e-01, \n",
      "Epoch  10 - Loss: 9.53e-01, Validation loss: 8.52e-01, \n",
      "Epoch  20 - Loss: 8.75e-01, Validation loss: 7.96e-01, \n",
      "Epoch  30 - Loss: 7.98e-01, Validation loss: 7.44e-01, \n",
      "Epoch  40 - Loss: 7.30e-01, Validation loss: 7.04e-01, \n",
      "Epoch  50 - Loss: 6.71e-01, Validation loss: 6.79e-01, \n",
      "Epoch  60 - Loss: 6.22e-01, Validation loss: 6.61e-01, \n",
      "Epoch  70 - Loss: 5.83e-01, Validation loss: 6.54e-01, \n",
      "Epoch  80 - Loss: 5.51e-01, Validation loss: 6.55e-01, \n",
      "Restoring model weights from the end of the best epoch: 74.\n",
      "Epoch 84: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:32:33,493 - ------------------------------------------------------------------------\n",
      "2025-08-13 11:32:33,501 - Training for fold 3 ensemble 1...\n",
      "2025-08-13 11:32:33.792855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31088 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b3:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 - Loss: 9.87e-01, Validation loss: 1.04e+00, \n",
      "Epoch  10 - Loss: 9.30e-01, Validation loss: 9.81e-01, \n",
      "Epoch  20 - Loss: 8.54e-01, Validation loss: 9.13e-01, \n",
      "Epoch  30 - Loss: 7.81e-01, Validation loss: 8.50e-01, \n",
      "Epoch  40 - Loss: 7.18e-01, Validation loss: 8.00e-01, \n",
      "Epoch  50 - Loss: 6.68e-01, Validation loss: 7.63e-01, \n",
      "Epoch  60 - Loss: 6.25e-01, Validation loss: 7.31e-01, \n",
      "Epoch  70 - Loss: 5.88e-01, Validation loss: 7.01e-01, \n",
      "Epoch  80 - Loss: 5.59e-01, Validation loss: 6.84e-01, \n",
      "Epoch  90 - Loss: 5.33e-01, Validation loss: 6.71e-01, \n",
      "Epoch 100 - Loss: 5.12e-01, Validation loss: 6.59e-01, \n",
      "Epoch 110 - Loss: 4.92e-01, Validation loss: 6.55e-01, \n",
      "Epoch 120 - Loss: 4.73e-01, Validation loss: 6.54e-01, \n",
      "Epoch 130 - Loss: 4.58e-01, Validation loss: 6.50e-01, \n",
      "Epoch 140 - Loss: 4.42e-01, Validation loss: 6.49e-01, \n",
      "Restoring model weights from the end of the best epoch: 135.\n",
      "Epoch 145: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:35:57,797 - ------------------------------------------------------------------------\n",
      "2025-08-13 11:35:57,811 - Training for fold 4 ensemble 1...\n",
      "2025-08-13 11:35:58.113171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31088 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b3:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 - Loss: 1.00e+00, Validation loss: 9.79e-01, \n",
      "Epoch  10 - Loss: 9.32e-01, Validation loss: 9.28e-01, \n",
      "Epoch  20 - Loss: 8.57e-01, Validation loss: 8.61e-01, \n",
      "Epoch  30 - Loss: 7.91e-01, Validation loss: 8.00e-01, \n",
      "Epoch  40 - Loss: 7.34e-01, Validation loss: 7.47e-01, \n",
      "Epoch  50 - Loss: 6.86e-01, Validation loss: 6.97e-01, \n",
      "Epoch  60 - Loss: 6.43e-01, Validation loss: 6.56e-01, \n",
      "Epoch  70 - Loss: 6.04e-01, Validation loss: 6.19e-01, \n",
      "Epoch  80 - Loss: 5.74e-01, Validation loss: 5.93e-01, \n",
      "Epoch  90 - Loss: 5.49e-01, Validation loss: 5.70e-01, \n",
      "Epoch 100 - Loss: 5.28e-01, Validation loss: 5.62e-01, \n",
      "Epoch 110 - Loss: 5.09e-01, Validation loss: 5.55e-01, \n",
      "Epoch 120 - Loss: 4.92e-01, Validation loss: 5.44e-01, \n",
      "Epoch 130 - Loss: 4.77e-01, Validation loss: 5.38e-01, \n",
      "Epoch 140 - Loss: 4.63e-01, Validation loss: 5.34e-01, \n",
      "Epoch 150 - Loss: 4.49e-01, Validation loss: 5.32e-01, \n",
      "Epoch 160 - Loss: 4.36e-01, Validation loss: 5.31e-01, \n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "#                             Train loop                              #\n",
    "#######################################################################\n",
    "X = input\n",
    "y = output\n",
    "\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "\n",
    "y_mean = np.mean(y, axis=0)\n",
    "y_std = np.std(y, axis=0)\n",
    "\n",
    "if remove_mean ==0:\n",
    "    X_mean = 0\n",
    "    y_mean = 0\n",
    "\n",
    "    \n",
    "    \n",
    "if divide_std ==0:\n",
    "    X_std = 1\n",
    "    y_std = 1\n",
    "    #X_std = X_std[:,:, tf.newaxis]\n",
    "    \n",
    "    \n",
    "    \n",
    "sio.savemat(os.path.join(output_dir,'Normalization.mat'), \n",
    "            {'X_mean': X_mean, 'X_std': X_std,'y_mean':y_mean,'y_std':y_std})\n",
    "\n",
    "# if not os.path.exists(os.path.join(output_dir, 'inputs_info.mat')):\n",
    "\n",
    "        \n",
    "trained_models = []\n",
    "y_pred_reconstructed_allfolds = []\n",
    "\n",
    "\n",
    "# Train the neural network multiple times using k-fold cross validation\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "fold_no = 1\n",
    "for trainind, testind in kfold.split(X, y):   \n",
    "    # break\n",
    "    X_train = X[trainind,:,:, tf.newaxis]\n",
    "    y_train = y[trainind,:]\n",
    "    X_test = X[testind,:, :,tf.newaxis]\n",
    "    y_test = y[testind,:]\n",
    " \n",
    "    # Normalize the input and out data based on the information from the entire PI control run\n",
    "    \n",
    "    \n",
    "    X_train = (X_train - X_mean[np.newaxis, :, :, np.newaxis]) / X_std[np.newaxis, :, :, np.newaxis]\n",
    "    X_test = (X_test - X_mean[np.newaxis, :, :, np.newaxis]) / X_std[np.newaxis, :, :, np.newaxis]\n",
    "    \n",
    "    y_train = (y_train - y_mean)/y_std\n",
    "    y_test = (y_test - y_mean)/y_std\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Generate a print\n",
    "    logger.info('------------------------------------------------------------------------')\n",
    "    # We use ensemble training for each fold of the cross-validation\n",
    "    for ens_no  in np.arange(1,NNrepeats+1):\n",
    "        logger.info(f'Training for fold {fold_no} ensemble {ens_no}...')\n",
    "        skill, history, pred, R2_val = train_model(X_train, y_train, X_test, y_test,y_mean,y_std)\n",
    "        trained_models.append((skill, skill, history.history, pred, R2_val))\n",
    "\n",
    "        \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "        \n",
    "        \n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info(f'Training with {num_folds}-fold cross-validation finished!') \n",
    "logger.info('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be32e148",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "have a quick look at the performance of the trainined neural network"
   },
   "outputs": [],
   "source": [
    "\n",
    "skills = [trained_model[1] for trained_model in trained_models[0:num_folds*NNrepeats]]\n",
    "median_skill = np.median(skills)\n",
    "median_index = skills.index(median_skill)\n",
    "minimum_skill = np.min(skills)\n",
    "min_index = skills.index(minimum_skill)\n",
    "std_dev_skill = np.std(skills)\n",
    "\n",
    "formatted_skills = [f\"{skill:.4f}\" for skill in skills]\n",
    "logger.info(f\"Losses in testing set: {formatted_skills}\")\n",
    "fold_median = median_index // NNrepeats + 1  # Compute fold number\n",
    "ensemble_median = median_index % NNrepeats + 1  # Compute ensemble number\n",
    "logger.info(f\"Median Loss is: {median_skill:.4f}, which occurs at Fold {fold_median} Ensemble {ensemble_median}\")\n",
    "fold_min = min_index // NNrepeats + 1  # Compute fold number\n",
    "ensemble_min = min_index % NNrepeats + 1  # Compute ensemble number\n",
    "logger.info(f\"Minimum Loss is: {minimum_skill:.4f}, which occurs at Fold {fold_min} Ensemble {ensemble_min}\")\n",
    "logger.info(f\"Standard Deviation of the Loss is: {std_dev_skill:.4f}\")\n",
    "ratio = std_dev_skill/median_skill * 100\n",
    "logger.info(f\"Standard Deviation/Median: {ratio:.2f}%\")\n",
    "\n",
    "# Check if the standard deviation to median skill ratio is higher than 30%\n",
    "if ratio > 30:\n",
    "    warning_message = (\"Warning: The standard deviation of model skill across \"\n",
    "                       \"folds and ensembles is large!\\nStandard Deviation/Median Skill: {ratio:.2f}%\").format(ratio=ratio)\n",
    "    with open(os.path.join(output_dir,'Warning_'+names_str+'.txt'), \"w\") as file:\n",
    "        file.write(warning_message)\n",
    "    with open(os.path.join(output_dir,'..','Warning_'+names_str+'.txt'), \"w\") as file:\n",
    "        file.write(warning_message)\n",
    "\n",
    "\n",
    "# Plot the loss function during training for all folds\n",
    "# First, find the global minimum and maximum loss values across all folds\n",
    "min_loss = min(min(trained_models[i][2]['loss']+trained_models[i][2]['val_loss']) for i in range(num_folds))\n",
    "max_loss = max(max(trained_models[i][2]['loss']+trained_models[i][2]['val_loss']) for i in range(num_folds))\n",
    "\n",
    "plt.figure(figsize=(9, 12))\n",
    "\n",
    "# Plot the first fold outside the loop to avoid repeating the legend setting\n",
    "plt.subplot(num_folds, 1, 1)\n",
    "for nn in range(NNrepeats):\n",
    "    plt.plot(trained_models[nn][2]['loss'],'-k')\n",
    "    plt.plot(trained_models[nn][2]['val_loss'],'-r')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')  # Set logarithmic scale\n",
    "plt.ylim(min_loss, max_loss)  # Set the same y-limits for all subplots\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Now plot the remaining folds\n",
    "for n in range(1, num_folds):\n",
    "    plt.subplot(num_folds, 1, n+1)\n",
    "    for nn in range(NNrepeats):\n",
    "        plt.plot(trained_models[n*NNrepeats+nn][2]['loss'],'-k')\n",
    "        plt.plot(trained_models[n*NNrepeats+nn][2]['val_loss'],'-r')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')  # Set logarithmic scale\n",
    "    plt.ylim(min_loss, max_loss)  # Set the same y-limits for all subplots\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.subplots_adjust(hspace=0.5)  # Adjust space between plots if needed\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'TrainingLoss_' + names_str  + '.png'),dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "R2_each_fold_ens = [trained_models[i][4] for i in range(0,num_folds*NNrepeats)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22894312",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "Model_preds = [np.concatenate([trained_models[n*NNrepeats + i][3] for n in range(num_folds)]) for i in range(NNrepeats)]\n",
    "Model_pred = np.mean(Model_preds,axis=0)\n",
    "Model_error = Model_pred - y\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30, 6))  \n",
    "plt.plot(time, y, label=\"Truth\", color=\"C0\", linewidth=2)  # Use color and linewidth\n",
    "plt.plot(time, Model_pred, label=\"Prediction\", color=\"C1\", linestyle=\"-\", linewidth=1.5)  # Dashed line for prediction\n",
    "plt.xlabel(\"Time (year)\", fontsize=14)\n",
    "plt.ylabel(names_str, fontsize=14)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend(fontsize=12, loc=\"best\")\n",
    "r2 = r2_score(y, Model_pred)\n",
    "plt.text(0.05, 0.9, f\"$R^2$: {r2:.3f}\", transform=plt.gca().transAxes,\n",
    "         fontsize=14, bbox=dict(facecolor=\"white\", alpha=0.7, edgecolor=\"black\"))\n",
    "plt.savefig(os.path.join(output_dir, names_str+\"_TruthvsPred.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "######### scatter plot\n",
    "\n",
    "plt.figure(figsize=(8, 8))  # Keep square aspect ratio\n",
    "\n",
    "# Scatter plot with adjusted marker size and transparency\n",
    "plt.plot(Model_pred, y, 'o', markersize=5, alpha=0.6, color=\"C0\", label=\"Data Points\")\n",
    "\n",
    "# Compute common limits based on both Model_pred and y\n",
    "min_val = min(np.min(Model_pred), np.min(y))\n",
    "max_val = max(np.max(Model_pred), np.max(y))\n",
    "\n",
    "# Set same limits for x and y\n",
    "plt.xlim(min_val, max_val)\n",
    "plt.ylim(min_val, max_val)\n",
    "\n",
    "# Define consistent tick locations\n",
    "num_ticks = 6  # Adjust this for more or fewer ticks\n",
    "ticks = np.linspace(min_val, max_val, num_ticks)\n",
    "\n",
    "plt.xticks(ticks)\n",
    "plt.yticks(ticks)\n",
    "\n",
    "# 1:1 Reference Line\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'k--', linewidth=2, label=\"1:1 Reference\")\n",
    "\n",
    "# Labels and formatting\n",
    "plt.xlabel(\"Prediction\", fontsize=16, fontweight='bold')\n",
    "plt.ylabel(\"Truth\", fontsize=16, fontweight='bold')\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Compute and display R² score\n",
    "r2 = r2_score(y, Model_pred)\n",
    "plt.text(0.05, 0.9, f\"$R^2$: {r2:.3f}\", transform=plt.gca().transAxes,\n",
    "         fontsize=14, bbox=dict(facecolor=\"white\", alpha=0.7, edgecolor=\"black\"))\n",
    "\n",
    "# Equal aspect ratio for fair comparison\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "# Add legend (only for reference line)\n",
    "plt.legend(fontsize=12, loc=\"center left\")\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(os.path.join(output_dir, \"TOA_TruthvsPred_scatter.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e12b1a-4c08-4287-bdc0-bffc8d091e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "import os\n",
    "from keras.models import load_model\n",
    "import innvestigate\n",
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "\n",
    "\n",
    "path_model = \"/ocean/projects/ees250004p/ezhu3/data/CESM1/trained_for_NH/NeuralNet/CNN_StandardScaler_Neur32x32_BS32_5foldCV_Reg0Drop0.25_gelu+PRelu/TOA_anom\"\n",
    "\n",
    "# Helper function for regional plotting\n",
    "def plot_regional_map(lon, lat, data2d, title, extent, cbar_label=\"Relevance\"):\n",
    "    \"\"\"\n",
    "    Plots a 2D regional map using Cartopy.\n",
    "    \"\"\"\n",
    "    vmax = np.nanpercentile(np.abs(data2d), 98)\n",
    "    vmin = -vmax\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6),\n",
    "                           subplot_kw=dict(projection=ccrs.PlateCarree(central_longitude=0)))\n",
    "    \n",
    "    pcm = ax.pcolormesh(lon, lat, data2d, cmap=\"RdBu_r\", transform=ccrs.PlateCarree(),\n",
    "                        vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    ax.coastlines(zorder=2)\n",
    "    ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "    \n",
    "    gl = ax.gridlines(draw_labels=True, linestyle=\"--\", alpha=0.7)\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    \n",
    "    cbar = plt.colorbar(pcm, ax=ax, orientation=\"vertical\", pad=0.03, shrink=0.85)\n",
    "    cbar.set_label(cbar_label, fontsize=14)\n",
    "    ax.set_title(title, fontsize=16, pad=15)\n",
    "    plt.show()\n",
    "\n",
    "# Re-define the LRP analysis function from your notebook\n",
    "def ensemble_lrp_analyze(model_dir, num_folds, data_to_analyze):\n",
    "    all_relevance_maps = []\n",
    "    for i in range(1, num_folds + 1):\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "        model_path = os.path.join(model_dir, f'model_fold{i}_ens1.h5')\n",
    "        print(f\"    Analyzing model: {os.path.basename(model_path)}\")\n",
    "        try:\n",
    "            model = load_model(model_path)\n",
    "            analyzer = innvestigate.create_analyzer(\"lrp.epsilon\", model)\n",
    "            relevance = analyzer.analyze(data_to_analyze)\n",
    "            all_relevance_maps.append(relevance)\n",
    "        except Exception as e:\n",
    "            print(f\"      ❌ ERROR analyzing model {i}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    if not all_relevance_maps:\n",
    "        return None\n",
    "        \n",
    "    ensemble_relevance = np.mean(np.stack(all_relevance_maps), axis=0)\n",
    "    return ensemble_relevance\n",
    "\n",
    "# --- Tropics Attribution Map (In-Sample) ---\n",
    "print(\"\\\\n======================================================\")\n",
    "print(\"▶️ PROCESSING LRP FOR TROPICS (IN-SAMPLE)\")\n",
    "print(\"======================================================\")\n",
    "\n",
    "# Use the X_train from your notebook, which is the in-sample data for the Tropics model\n",
    "tropics_attribution_map = ensemble_lrp_analyze(\n",
    "    model_dir=path_model,\n",
    "    num_folds=num_folds,\n",
    "    data_to_analyze=X_train\n",
    ")\n",
    "\n",
    "if tropics_attribution_map is not None:\n",
    "    tropics_attribution_map = tropics_attribution_map.squeeze()\n",
    "    mean_relevance_tropics = np.mean(tropics_attribution_map, axis=0)\n",
    "    \n",
    "    # Define the extent for the Tropics\n",
    "    tropics_extent = [-180, 180, -30, 30]\n",
    "    \n",
    "    # Apply a latitude mask to the data to match the extent\n",
    "    lat_mask = (lat >= tropics_extent[2]) & (lat <= tropics_extent[3])\n",
    "    masked_lat = lat[lat_mask]\n",
    "    masked_data = mean_relevance_tropics[lat_mask, :]\n",
    "    \n",
    "    plot_regional_map(\n",
    "        lon, masked_lat, masked_data,\n",
    "        title=\"In-Sample Time-Averaged Mean Relevance - Tropics\",\n",
    "        extent=tropics_extent\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python (tf210)",
   "language": "python",
   "name": "tf210"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
