{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d14b88c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 11:11:11.036076: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-29 11:11:11.142144: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-29 11:11:11.176079: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Mon Feb 24 10:04:04 2025\n",
    "\n",
    "@author: hwei\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# V3 25-03-2025 updated normalization\n",
    "\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense,LeakyReLU, Dropout, Add, Activation, Conv2D, Flatten, MaxPooling2D, Dense, PReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from matplotlib.colors import Normalize\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "tf.keras.utils.set_random_seed(0)\n",
    "tf.config.list_physical_devices('GPU')  \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras.engine.training_v1\")\n",
    "import xarray as xr\n",
    "import innvestigate\n",
    "import scipy.io as sio\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c3435-9030-45fd-9f63-4821b48f1294",
   "metadata": {},
   "source": [
    "##IMPORTANT!!\n",
    "NEED TO CHANGE INPUT/OUTPUT path for CESM1/CESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "560eda33",
   "metadata": {
    "title": "load data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (time: 800, bnds: 2, lon: 144, lat: 143)\n",
      "Coordinates:\n",
      "  * time       (time) object 3050-07-01 06:00:00 ... 3849-07-01 06:00:00\n",
      "  * lon        (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
      "  * lat        (lat) float32 -90.0 -88.73 -87.46 -86.2 ... 86.2 87.46 88.73 90.0\n",
      "Dimensions without coordinates: bnds\n",
      "Data variables:\n",
      "    time_bnds  (time, bnds) object ...\n",
      "    ts         (time, lat, lon) float32 ...\n",
      "Attributes: (12/53)\n",
      "    CDI:                    Climate Data Interface version 2.5.0 (https://mpi...\n",
      "    Conventions:            CF-1.7 CMIP-6.2\n",
      "    source:                 IPSL-CM6A-LR (2017):  atmos: LMDZ (NPv6, N96; 144...\n",
      "    institution:            Institut Pierre Simon Laplace, Paris 75252, France\n",
      "    name:                   /ccc/work/cont003/gencmip6/p86mign/IGCM_OUT/IPSLC...\n",
      "    creation_date:          2019-06-28T11:56:10Z\n",
      "    ...                     ...\n",
      "    dr2xml_md5sum:          45d4369d889ddfb8149d771d8625e9ec\n",
      "    model_version:          6.1.9\n",
      "    parent_experiment_id:   piControl-spinup\n",
      "    parent_activity_id:     CMIP\n",
      "    NCO:                    netCDF Operators version 5.3.4 (Homepage = http:/...\n",
      "    CDO:                    Climate Data Operators version 2.5.0 (https://mpi...\n",
      "Aligning data to the shortest length: 800 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/xarray/coding/times.py:716: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/xarray/coding/times.py:716: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/xarray/coding/times.py:716: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/xarray/core/indexing.py:529: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return np.asarray(array[self.key], dtype=None)\n",
      "/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/xarray/coding/times.py:716: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/xarray/coding/times.py:716: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/xarray/coding/times.py:716: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/xarray/core/indexing.py:529: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return np.asarray(array[self.key], dtype=None)\n"
     ]
    }
   ],
   "source": [
    "# load the input variable -- global Surface temperature\n",
    "\n",
    "#CESM1 control\n",
    "#ds = xr.open_dataset(\"/ocean/projects/ees250004p/ezhu3/data/CESM1/control/input.B1850C5CN.TS.ANN.0400-2200.new.nc\")\n",
    "\n",
    "#CESM2 data 1: \n",
    "#ds = xr.open_dataset(\"/ocean/projects/ees250004p/ezhu3/data/CESM2/control/input.CESM2-B1850.TS_detrend.ANN.nc\")\n",
    "\n",
    "#CESM2 data2:\n",
    "#ds = xr.open_dataset(\"/ocean/projects/ees250004p/ezhu3/data/CESM2/control/input.CESM2-B1850.TS.ANN.nc\")\n",
    "\n",
    "#ds = xr.open_dataset(\"E:\\\\Yue\\\\CESM2\\\\control\\\\input.CESM2-B1850.TS_detrend.ANN.nc\")# Display dataset info\n",
    "ds = xr.open_dataset(\"/ocean/projects/ees250004p/ezhu3/data/IPSL-CM6A_LR/control/ts/proc/IPSL_piControl_ts_anom.nc\")\n",
    "\n",
    "\n",
    "print(ds)\n",
    "\n",
    "# Access a specific variable\n",
    "TS = ds[\"ts\"] # Surface temperature (radiative)     units = 'K'\n",
    "lat = ds[\"lat\"] #latitude\n",
    "lon = ds[\"lon\"] #longitude\n",
    "time = ds[\"time\"]\n",
    "\n",
    "\n",
    "# Define the directory where the output variables are stored\n",
    "data_dir = '/ocean/projects/ees250004p/ezhu3/data/IPSL-CM6A_LR/control/rlut/proc/' #this is for CESM1\n",
    "# Define the filenames and corresponding variable names\n",
    "files = {\n",
    "    #CESM1:\n",
    "    #\"TOA_anom\": \"output.B1850C5CN.TOA.gmean.ANN.0400-2200.new.nc\"\n",
    "    \n",
    "    #CESM2 data 1:\n",
    "    #\"TOA_anom\": \"output.CESM2-B1850.TOA_detrend.ANN.nc\"\n",
    "    #CESM2 data 2:\n",
    "    #\"TOA_anom\": \"output.CESM2-B1850.TOA.ANN.nc\"\n",
    "    \"rlut\": \"IPSL_piControl_rlut_annual_anom.nc\"\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "# Load variables\n",
    "for var, filename in files.items():\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    datasets[var] = ds[var]  # Extract only the variable\n",
    "\n",
    "\n",
    "# Now the variables can be accessed as datasets[\"CRE\"], datasets[\"TOA\"], etc.\n",
    "\n",
    "\n",
    "# path for storing the trained neural networks\n",
    "#data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM2/trained_model_data1_Gelu'#CESM2 data1\n",
    "#data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM2/trained_model_data2_Gelu'#CESM2 data2 gelu\n",
    "#data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM1/trained_model'#CESM1\n",
    "data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM1/trained_model_rlut'#CESM1 Gelu\n",
    "\n",
    "# Load X data (ts)\n",
    "ds_X = xr.open_dataset(\"/ocean/projects/ees250004p/ezhu3/data/IPSL-CM6A_LR/control/ts/proc/IPSL_piControl_ts_anom.nc\")\n",
    "ts_data = ds_X[\"ts\"]\n",
    "\n",
    "# Load Y data (rsdt)\n",
    "data_dir_y = '/ocean/projects/ees250004p/ezhu3/data/IPSL-CM6A_LR/control/rlut/proc/'\n",
    "rsdt_filename = \"IPSL_piControl_rlut_annual_anom.nc\" # From your 'files' dictionary\n",
    "ds_Y = xr.open_dataset(os.path.join(data_dir_y, rsdt_filename))\n",
    "rsdt_data = ds_Y[\"rlut\"]\n",
    "\n",
    "# --- Step 2: Find the minimum length and slice both arrays ---\n",
    "# Find the shortest time dimension\n",
    "min_length = min(len(ts_data['time']), len(rsdt_data['time']))\n",
    "print(f\"Aligning data to the shortest length: {min_length} samples.\")\n",
    "\n",
    "# Use .isel() to select the first 'min_length' time steps from both\n",
    "ts_aligned = ts_data.isel(time=slice(0, min_length))\n",
    "rsdt_aligned = rsdt_data.isel(time=slice(0, min_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3dd24db",
   "metadata": {
    "title": "Neural Network Hyperparameters"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "            \n",
    "# Neural network architecture:\n",
    "# Example: [64, 64, 64] means three hidden layers, each containing 64 neurons.\n",
    "kernels = [32, 32]\n",
    "kernel_acts =  [\"gelu\", \"gelu\"]\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "rng_seed = 42\n",
    "hiddens = [16, 8]\n",
    "activation_function_dense = [\"PRelu\", \"PRelu\"]\n",
    "pool_size = 2\n",
    "\n",
    "# Loss function for regression tasks:\n",
    "# Options: 'mse' (mean squared error), 'mae' (mean absolute error), 'mape' (mean absolute percentage error)\n",
    "# Full list of regression losses: https://keras.io/api/losses/\n",
    "loss_function = 'mse'\n",
    "\n",
    "\n",
    "reg_strength = 0         # L2 regularization strength; 1e-1~1e-5\n",
    "dropout_rate = 0.25         # Dropout rate (0.0 to disable dropout)\n",
    "\n",
    "\n",
    "# low-pass filter time scale; 0 means no low-pass filter\n",
    "LPF_year = 0\n",
    "\n",
    "\n",
    "#### normalization\n",
    "remove_mean = 0\n",
    "divide_std = 1\n",
    "\n",
    "\n",
    "#### usually we do not change the parameters below \n",
    "\n",
    "# Training configuration\n",
    "epoch_max = 25000            # Maximum number of training epochs\n",
    "batch_size = 32            # Batch size used during training\n",
    "learning_rate = 0.000005       # Default learning rate for Adam optimizer 0.001\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "num_folds = 5 # Number of fold during cross-validation\n",
    "NNrepeats = 1 # Repeat the training for NNrepeats times\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c050e93",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "pre-process data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape of X: (800, 143, 144)\n",
      "Final shape of y: (800, 1)\n",
      "Corrected input shape: (800, 143, 144)\n",
      "Corrected output shape: (800, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_raw = ts_aligned.values\n",
    "\n",
    "# Prepare y (output_raw) by taking the spatial mean of the ALIGNED data\n",
    "output_raw = rsdt_aligned.mean(dim=['lat', 'lon']).values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# --- Step 4: Verification ---\n",
    "print(\"Final shape of X:\", input_raw.shape)\n",
    "print(\"Final shape of y:\", output_raw.shape)\n",
    "\n",
    "# input_raw = TS.values.reshape(TS.shape[0],TS.shape[1]*TS.shape[2])\n",
    "# For CNN, we don't need to flatten the data\n",
    "# input_raw = TS.values\n",
    "\n",
    "\n",
    "\n",
    "# Define the variable names as a comma-separated string\n",
    "# names_strALL = \"CRE,FLNT,FSNT,LCC,LWCF,SWCF,TCC,TOA\"\n",
    "# If we only want to reconstruct TOA\n",
    "names_strALL = \"rlut\"\n",
    "names_str = \"rlut\"\n",
    "\n",
    "#mean_values = datasets[names_str].mean(dim=['lat', 'lon']).values\n",
    "# --- Verification ---\n",
    "print(\"Corrected input shape:\", input_raw.shape)\n",
    "print(\"Corrected output shape:\", output_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b49bea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligning data to the shortest length: 800 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/xarray/coding/times.py:716: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/xarray/core/indexing.py:529: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return np.asarray(array[self.key], dtype=None)\n",
      "/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/xarray/coding/times.py:716: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/xarray/coding/times.py:716: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/xarray/core/indexing.py:529: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return np.asarray(array[self.key], dtype=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_NN_name():\n",
    "\n",
    "    NN_name = 'CNN'\n",
    "        \n",
    "    activation_function_str = '_'+kernel_acts[0]+ '+'+activation_function_dense[0] \n",
    "    loss_function_str = '_'+str(loss_function)+'loss' if loss_function!='mse' else ''\n",
    "    \n",
    "    if remove_mean + divide_std == 0:\n",
    "        scaler_str = 'NoScaler_'\n",
    "    elif remove_mean + divide_std == 2:\n",
    "        scaler_str = 'StandardScaler_'\n",
    "    elif remove_mean:\n",
    "        scaler_str = 'RemoveMean_'\n",
    "    elif divide_std:\n",
    "        scaler_str = 'DivideSTD_'\n",
    "    LPF_str = f'_LPF{int(LPF_year)}Year' if LPF_year else ''\n",
    "    NN_structure_str = 'x'.join(map(str, kernels)) \n",
    "    reg_str = f'Reg{reg_strength}' + (f'Drop{dropout_rate}' if dropout_rate != 0 else '')\n",
    "    batch_size_str =  f'BS{batch_size}_' if batch_size !=600 else ''\n",
    "    return f\"{NN_name}_{scaler_str}Neur{NN_structure_str}_{batch_size_str}{num_folds}foldCV_{reg_str}{loss_function_str}{activation_function_str}{LPF_str}\"\n",
    "NN_name = create_NN_name()\n",
    "\n",
    "    \n",
    "def construct_output_directory(data_dir, NN_name):\n",
    "    output_dir = os.path.join(data_dir, 'NeuralNet', NN_name, names_str)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "output_dir = construct_output_directory(data_dir, NN_name)\n",
    "        \n",
    "# Load X data (ts)\n",
    "ds_X = xr.open_dataset(\"/ocean/projects/ees250004p/ezhu3/data/IPSL-CM6A_LR/control/ts/proc/IPSL_piControl_ts_anom.nc\")\n",
    "ts_data = ds_X[\"ts\"]\n",
    "\n",
    "# Load Y data (rsdt)\n",
    "data_dir_y = '/ocean/projects/ees250004p/ezhu3/data/IPSL-CM6A_LR/control/rsdt/proc/'\n",
    "rsdt_filename = \"rsdt_pi_anom.nc\" # From your 'files' dictionary\n",
    "ds_Y = xr.open_dataset(os.path.join(data_dir_y, rsdt_filename))\n",
    "rsdt_data = ds_Y[\"rsdt\"]\n",
    "\n",
    "# --- Step 2: Find the minimum length and slice both arrays ---\n",
    "# Find the shortest time dimension\n",
    "min_length = min(len(ts_data['time']), len(rsdt_data['time']))\n",
    "print(f\"Aligning data to the shortest length: {min_length} samples.\")\n",
    "\n",
    "# Use .isel() to select the first 'min_length' time steps from both\n",
    "ts_aligned = ts_data.isel(time=slice(0, min_length))\n",
    "rsdt_aligned = rsdt_data.isel(time=slice(0, min_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "840c1afa",
   "metadata": {
    "title": "Define logger that can print useful information into a logfile"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 11:28:06,537 - logfile\n",
      "2025-08-29 11:28:06,537 - logfile\n",
      "2025-08-29 11:28:06,538 - Using GPU for training.\n",
      "2025-08-29 11:28:06,538 - Using GPU for training.\n",
      "2025-08-29 11:28:06,539 - 1 Physical GPUs, 1 Logical GPUs\n",
      "2025-08-29 11:28:06,539 - 1 Physical GPUs, 1 Logical GPUs\n",
      "2025-08-29 11:28:06,540 - None\n",
      "2025-08-29 11:28:06,540 - None\n",
      "2025-08-29 11:28:06,540 - Output path: created successfully\n",
      "2025-08-29 11:28:06,540 - Output path: created successfully\n",
      "2025-08-29 11:28:06,541 - The output path is /ocean/projects/ees250004p/ezhu3/data/CESM1/trained_model_rlut/NeuralNet/CNN_DivideSTD_Neur32x32_BS32_5foldCV_Reg0Drop0.25_gelu+PRelu/rlut\n",
      "2025-08-29 11:28:06,541 - The output path is /ocean/projects/ees250004p/ezhu3/data/CESM1/trained_model_rlut/NeuralNet/CNN_DivideSTD_Neur32x32_BS32_5foldCV_Reg0Drop0.25_gelu+PRelu/rlut\n"
     ]
    }
   ],
   "source": [
    "logger_name = 'logfile.log'  \n",
    "# Configure logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create handlers\n",
    "file_handler = logging.FileHandler(os.path.join(output_dir, logger_name))\n",
    "console_handler = logging.StreamHandler()\n",
    "\n",
    "# Set level and format for handlers\n",
    "file_handler.setLevel(logging.INFO)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Test the setup\n",
    "logger.info(\"logfile\")\n",
    "\n",
    " \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    logger.info(\"Using GPU for training.\")\n",
    "    logger.info(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "else:\n",
    "    logger.info(\"Using CPU for training.\") \n",
    "\n",
    "logger.info(os.getenv('TF_GPU_ALLOCATOR'))\n",
    "    \n",
    "logger.info(f\"Output path: {'created successfully' if os.path.exists(output_dir) else 'already exists'}\")\n",
    "logger.info(f\"The output path is {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2d9886a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 11:28:08,451 - ------------------------------------------------------------------------\n",
      "2025-08-29 11:28:08,451 - ------------------------------------------------------------------------\n",
      "2025-08-29 11:28:08,452 - ------------------------------------------------------------------------\n",
      "2025-08-29 11:28:08,452 - ------------------------------------------------------------------------\n",
      "2025-08-29 11:28:08,452 - Applying 0-year low pass filter ...\n",
      "2025-08-29 11:28:08,452 - Applying 0-year low pass filter ...\n",
      "2025-08-29 11:28:08,453 - ------------------------------------------------------------------------\n",
      "2025-08-29 11:28:08,453 - ------------------------------------------------------------------------\n",
      "2025-08-29 11:28:08,454 - ------------------------------------------------------------------------\n",
      "2025-08-29 11:28:08,454 - ------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info(f'Applying {LPF_year}-year low pass filter ...')\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "\n",
    "from scipy.signal import butter, sosfilt\n",
    "\n",
    "def apply_low_pass_filter(data, cutoff_freq, order=5, sampling_rate=1, padding_length=None):\n",
    "    \"\"\"Applies a Butterworth low-pass filter to the given data.\"\"\"\n",
    "    sos = butter(order, cutoff_freq, btype='low', output='sos', analog=False, fs=sampling_rate)\n",
    "    \n",
    "    # Apply Boundary Padding to the data before filtering\n",
    "    padded_data = np.pad(data, [(padding_length, padding_length), (0, 0)], mode='reflect')\n",
    "    \n",
    "    # Apply the filter across each column without explicit looping\n",
    "    filtered_data = sosfilt(sos, padded_data, axis=0)\n",
    "    \n",
    "    # Remove padding\n",
    "    return filtered_data[padding_length:-padding_length]\n",
    "\n",
    "if LPF_year:\n",
    "    # Calculate the sampling rate (monthly data)\n",
    "    sampling_rate = 1  # Data is sampled monthly    \n",
    "    cutoff_freq = 1 / LPF_year\n",
    "    order = 5\n",
    "    padding_length =  3* LPF_year\n",
    "\n",
    "if LPF_year:\n",
    "    input = apply_low_pass_filter(input_raw, cutoff_freq, order, sampling_rate, padding_length)     \n",
    "    output= apply_low_pass_filter(output_raw, cutoff_freq, order, sampling_rate, padding_length)     \n",
    "    \n",
    "    plt.figure(figsize=(30, 6))  \n",
    "    plt.plot(time, output_raw, label=\"Original\", color=\"C0\", linewidth=2)  # Use color and linewidth\n",
    "    plt.plot(time, output, label=\"Low-pass filtered\", color=\"C1\", linestyle=\"-\", linewidth=1.5)  # Dashed line for prediction\n",
    "    plt.xlabel(\"Time (year)\", fontsize=14)\n",
    "    plt.ylabel(names_str, fontsize=14)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.legend(fontsize=12, loc=\"best\")\n",
    "    plt.savefig(os.path.join(output_dir, names_str+\"_LPF.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    input  =  input_raw\n",
    "    output = output_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d30f033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#                       define the neural network                     #\n",
    "#######################################################################\n",
    "\n",
    "log_path =os.path.join(output_dir, 'training_logs.txt')\n",
    "os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "class PrintTrainingOnTextEvery10EpochsCallback(Callback):\n",
    "    def __init__(self, log_path):\n",
    "        super().__init__()\n",
    "        self.log_path = log_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 10 == 0:  # Log every 10 epochs\n",
    "            with open(self.log_path, \"a\") as log_file:\n",
    "                log_file.write(\n",
    "                    f\"Epoch: {epoch:>3} | \"\n",
    "                    f\"Loss: {logs.get('loss', 0):.2e} | \"\n",
    "#                        f\"Accuracy: {logs.get('accuracy', 0):.2e} | \"\n",
    "                    f\"Validation loss: {logs.get('val_loss', 0):.2e} |\\n \"\n",
    "#                        f\"Validation accuracy: {logs.get('val_accuracy', 0):.2e}\\n\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"Epoch {epoch:>3} - \"\n",
    "                    f\"Loss: {logs.get('loss', 0):.2e}, \"\n",
    "#                        f\"Accuracy: {logs.get('accuracy', 0):.2e}, \"\n",
    "                    f\"Validation loss: {logs.get('val_loss', 0):.2e}, \"\n",
    "#                        f\"Validation accuracy: {logs.get('val_accuracy', 0):.2e}\"\n",
    "                )\n",
    "\n",
    "my_callbacks = [\n",
    "    PrintTrainingOnTextEvery10EpochsCallback(log_path=log_path),\n",
    "]   \n",
    "\n",
    "        \n",
    "#########ORIGINAL#########        \n",
    "# def train_model(X_train, y_train, X_test, y_test,y_mean,y_std):\n",
    "                \n",
    "#     # Create a new model with random initial weights and biases\n",
    "#     inputs = tf.keras.Input(shape=(X_train.shape[1:]),)\n",
    "#     layers = inputs\n",
    "\n",
    "#     for kernel, kernel_act in zip(kernels, kernel_acts):\n",
    "#         layers = Conv2D(\n",
    "#             kernel,\n",
    "#             (kernel_size, kernel_size),\n",
    "#             strides=(stride, stride),\n",
    "#             use_bias=True,\n",
    "#             activation=kernel_act,\n",
    "#             padding=\"same\",\n",
    "#             bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "#             kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "#         )(layers)\n",
    "#         layers = MaxPooling2D((pool_size, pool_size))(layers)\n",
    "    \n",
    "    \n",
    "#     # make final dense layers\n",
    "#     layers = Flatten()(layers)\n",
    "    # for hidden, activation in zip(hiddens, activation_function_dense):\n",
    "    #     layers = Dense(\n",
    "    #         hidden,\n",
    "    #         activation=activation,\n",
    "    #         use_bias=True,\n",
    "    #         bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    #         kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    #     )(layers)\n",
    "    \n",
    "    \n",
    "    # output_layer = Dense(\n",
    "    #     y_train.shape[-1],\n",
    "    #     activation=\"linear\",\n",
    "    #     use_bias=True,\n",
    "    #     bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    #     kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    # )(layers)\n",
    "    \n",
    "    # # CONSTRUCT THE MODEL\n",
    "    # model = Model(inputs, output_layer)\n",
    "\n",
    "    \n",
    "    # if fold_no + ens_no == 2:\n",
    "    #     # show the model summary\n",
    "    #     model.summary()\n",
    "    #     with open(os.path.join(output_dir, 'model_' + names_str  + '.txt'), 'w') as f:\n",
    "    #         model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "    #     dot_img_file = os.path.join(output_dir, 'model_' + names_str  + '.png')\n",
    "    #     plot_model(model, to_file=dot_img_file,\n",
    "    #                show_shapes=True,\n",
    "    #                show_dtype=False,\n",
    "    #                show_layer_names=True,\n",
    "    #                rankdir='LR',\n",
    "    #                expand_nested=False,\n",
    "    #                dpi=300,\n",
    "    #                show_layer_activations=True)\n",
    "        \n",
    "    # # Compile the model with mean squared error loss and Adam optimizer\n",
    "    # model.compile(loss=loss_function, optimizer=Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    # # Define early stopping callback\n",
    "    # early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='auto', restore_best_weights=1, verbose=1)\n",
    "\n",
    "    # # Train the model with early stopping callback\n",
    "    # history = model.fit(X_train, y_train, epochs=epoch_max, batch_size=batch_size,\n",
    "    #                     validation_data=(X_test, y_test), callbacks=[early_stopping,my_callbacks],verbose=0)\n",
    "\n",
    "    # # Evaluate the performance on the testing set (less useful)\n",
    "    # skill = model.evaluate(X_test, y_test, verbose=0) \n",
    "\n",
    "\n",
    "    # # Make the prediction with the actual scale in the testing set\n",
    "    # pred = (model.predict(X_test)*y_std)+y_mean\n",
    "    \n",
    "    # # Calculate the R2 value in the testing set at each latitude \n",
    "    # R2_val = []\n",
    "    # truth = (y_test*y_std)+y_mean \n",
    "    \n",
    "    # for latind in range(y_test.shape[1]):\n",
    "    #     y_lat = truth[:, latind];\n",
    "    #     y_pred_lat = pred[:, latind];\n",
    "    #     R2_val.append(r2_score(y_lat, y_pred_lat))\n",
    "        \n",
    "    # model.save(os.path.join(output_dir,'model_fold'+str(fold_no)+'_ens'+str(ens_no)+'.h5'))\n",
    "\n",
    "\n",
    "    # return skill, history, pred, R2_val\n",
    "        \n",
    "def train_model(X_train, y_train, X_test, y_test, y_mean, y_std):\n",
    "    # ... (your existing initializations for kernels, kernel_acts, hiddens, etc.) ...\n",
    "    # Ensure 'kernels', 'hiddens' are defined.\n",
    "    # 'kernel_acts' and 'activation_function_dense' will now control WHICH activation layer to add.\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(X_train.shape[1:]),)\n",
    "    layers = inputs\n",
    "\n",
    "    # Convolutional Layers\n",
    "    for i, kernel_filters in enumerate(kernels): # Assuming kernels is a list of filter numbers\n",
    "        layers = Conv2D(\n",
    "            kernel_filters,\n",
    "            (kernel_size, kernel_size), # Assuming kernel_size is defined\n",
    "            strides=(stride, stride),     # Assuming stride is defined\n",
    "            use_bias=True,\n",
    "            padding=\"same\",\n",
    "            # NO 'activation' argument here if PReLU or another layer follows\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "        )(layers)\n",
    "\n",
    "        # Apply PReLU (or other chosen activation)\n",
    "        # Assuming kernel_acts[i] could be 'prelu', 'gelu', 'relu', etc.\n",
    "        if kernel_acts[i].lower() == 'prelu':\n",
    "            layers = PReLU(shared_axes=[1, 2],\n",
    "                           alpha_initializer=tf.keras.initializers.Constant(0.25) # Common starting point\n",
    "                          )(layers) # shared_axes for Conv2D with channels_last\n",
    "        elif kernel_acts[i].lower() == 'gelu':\n",
    "            layers = tf.keras.layers.Activation('gelu')(layers) # Using Activation layer for GeLU\n",
    "        elif kernel_acts[i]: # For 'relu', 'elu', etc.\n",
    "            layers = tf.keras.layers.Activation(kernel_acts[i])(layers)\n",
    "        # If kernel_acts[i] is None or an empty string, no explicit activation layer added here.\n",
    "\n",
    "        layers = MaxPooling2D((pool_size, pool_size))(layers) # Assuming pool_size is defined\n",
    "    \n",
    "    layers = Flatten()(layers)\n",
    "\n",
    "    # Dense Layers\n",
    "    for i, hidden_units in enumerate(hiddens): # Assuming hiddens is a list of unit numbers\n",
    "        layers = Dense(\n",
    "            hidden_units,\n",
    "            use_bias=True,\n",
    "            # NO 'activation' argument here\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "        )(layers)\n",
    "\n",
    "        # Apply PReLU (or other chosen activation)\n",
    "        # Assuming activation_function_dense[i] could be 'prelu', 'gelu', 'elu', etc.\n",
    "        if activation_function_dense[i].lower() == 'prelu':\n",
    "            layers = PReLU(alpha_initializer=tf.keras.initializers.Constant(0.25))(layers) # No shared_axes for Dense\n",
    "        elif activation_function_dense[i].lower() == 'gelu':\n",
    "            layers = tf.keras.layers.Activation('gelu')(layers)\n",
    "        elif activation_function_dense[i]: # For 'elu', 'relu', etc.\n",
    "            layers = tf.keras.layers.Activation(activation_function_dense[i])(layers)\n",
    "        # If activation_function_dense[i] is None or an empty string, no explicit activation layer.\n",
    "\n",
    "    output_layer = Dense(\n",
    "        y_train.shape[-1], # Assuming y_train is (samples, features_out) or (samples,)\n",
    "        activation=\"linear\",\n",
    "        use_bias=True,\n",
    "        bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "        kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    )(layers)\n",
    "    \n",
    "    model = Model(inputs, output_layer)\n",
    "\n",
    "    # ... (rest of your model compilation, summary, plotting, training, saving) ...\n",
    "    # Example:\n",
    "    if fold_no + ens_no == 2: # Assuming these are defined in your script's scope\n",
    "        model.summary()\n",
    "        # ... (your summary saving and plot_model code) ...\n",
    "\n",
    "    model.compile(loss=loss_function, optimizer=Adam(learning_rate=learning_rate)) # Assuming these are defined\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='auto', restore_best_weights=True, verbose=1) # restore_best_weights=1 is True\n",
    "\n",
    "    # Make sure my_callbacks is defined in this scope or passed as an argument\n",
    "    history = model.fit(X_train, y_train, epochs=epoch_max, batch_size=batch_size,\n",
    "                        validation_data=(X_test, y_test), callbacks=[early_stopping] + my_callbacks, verbose=0) # Added '+' for list concatenation\n",
    "\n",
    "    skill = model.evaluate(X_test, y_test, verbose=0)\n",
    "    pred = (model.predict(X_test) * y_std) + y_mean # Make sure y_std and y_mean are defined\n",
    "\n",
    "    R2_val = []\n",
    "    truth = (y_test * y_std) + y_mean\n",
    "    for latind in range(truth.shape[1] if truth.ndim > 1 else 1): # Handle if y_test is 1D output\n",
    "        y_lat = truth[:, latind] if truth.ndim > 1 else truth\n",
    "        y_pred_lat = pred[:, latind] if pred.ndim > 1 else pred\n",
    "        R2_val.append(r2_score(y_lat, y_pred_lat))\n",
    "    \n",
    "    model.save(os.path.join(output_dir,'model_fold'+str(fold_no)+'_ens'+str(ens_no)+'.h5'))\n",
    "    \n",
    "    return skill, history, pred, R2_val\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ff17f61",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 11:29:26,144 - Training for fold 1 ensemble 1...\n",
      "2025-08-29 11:29:26,144 - Training for fold 1 ensemble 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 143, 144, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 143, 144, 32)      320       \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 143, 144, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 71, 72, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 71, 72, 32)        9248      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 71, 72, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 35, 36, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 40320)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                645136    \n",
      "                                                                 \n",
      " p_re_lu_2 (PReLU)           (None, 16)                16        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " p_re_lu_3 (PReLU)           (None, 8)                 8         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 654,873\n",
      "Trainable params: 654,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 11:29:36.678676: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.1KiB (rounded to 1280)requested by op conv2d_2/Conv2D\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-08-29 11:29:36.678725: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] BFCAllocator dump for GPU_0_bfc\n",
      "2025-08-29 11:29:36.678736: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (256): \tTotal Chunks: 61, Chunks in use: 61. 15.2KiB allocated for chunks. 15.2KiB in use in bin. 2.0KiB client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678741: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (512): \tTotal Chunks: 4, Chunks in use: 4. 2.0KiB allocated for chunks. 2.0KiB in use in bin. 2.0KiB client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678745: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1024): \tTotal Chunks: 5, Chunks in use: 5. 6.2KiB allocated for chunks. 6.2KiB in use in bin. 5.5KiB client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678750: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678754: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678757: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678761: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678765: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (32768): \tTotal Chunks: 4, Chunks in use: 4. 144.0KiB allocated for chunks. 144.0KiB in use in bin. 144.0KiB client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678769: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678772: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678776: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678779: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678782: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678787: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2097152): \tTotal Chunks: 5, Chunks in use: 5. 12.36MiB allocated for chunks. 12.36MiB in use in bin. 12.36MiB client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678790: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678793: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678797: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678802: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678805: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678810: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 1. 158.92MiB allocated for chunks. 158.92MiB in use in bin. 80.44MiB client-requested in use in bin.\n",
      "2025-08-29 11:29:36.678813: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679233: I tensorflow/core/common_runtime/bfc_allocator.cc:1056] Bin for 1.2KiB was 1.0KiB, Chunk State: \n",
      "2025-08-29 11:29:36.679245: I tensorflow/core/common_runtime/bfc_allocator.cc:1069] Next region of size 179765248\n",
      "2025-08-29 11:29:36.679256: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc000000 of size 1280 next 1\n",
      "2025-08-29 11:29:36.679259: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc000500 of size 256 next 2\n",
      "2025-08-29 11:29:36.679262: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc000600 of size 1280 next 3\n",
      "2025-08-29 11:29:36.679265: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc000b00 of size 36864 next 4\n",
      "2025-08-29 11:29:36.679268: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc009b00 of size 256 next 5\n",
      "2025-08-29 11:29:36.679271: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc009c00 of size 256 next 6\n",
      "2025-08-29 11:29:36.679274: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc009d00 of size 512 next 7\n",
      "2025-08-29 11:29:36.679277: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc009f00 of size 256 next 8\n",
      "2025-08-29 11:29:36.679280: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc00a000 of size 256 next 9\n",
      "2025-08-29 11:29:36.679283: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc00a100 of size 256 next 10\n",
      "2025-08-29 11:29:36.679286: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc00a200 of size 256 next 11\n",
      "2025-08-29 11:29:36.679289: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc00a300 of size 256 next 12\n",
      "2025-08-29 11:29:36.679292: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc00a400 of size 256 next 13\n",
      "2025-08-29 11:29:36.679294: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc00a500 of size 256 next 14\n",
      "2025-08-29 11:29:36.679297: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc00a600 of size 1280 next 15\n",
      "2025-08-29 11:29:36.679300: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc00ab00 of size 256 next 16\n",
      "2025-08-29 11:29:36.679303: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc00ac00 of size 36864 next 17\n",
      "2025-08-29 11:29:36.679306: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc013c00 of size 256 next 18\n",
      "2025-08-29 11:29:36.679308: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc013d00 of size 256 next 19\n",
      "2025-08-29 11:29:36.679311: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc013e00 of size 2580480 next 20\n",
      "2025-08-29 11:29:36.679314: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc289e00 of size 256 next 21\n",
      "2025-08-29 11:29:36.679317: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc289f00 of size 512 next 22\n",
      "2025-08-29 11:29:36.679320: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc28a100 of size 256 next 23\n",
      "2025-08-29 11:29:36.679322: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc28a200 of size 256 next 24\n",
      "2025-08-29 11:29:36.679325: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc28a300 of size 256 next 25\n",
      "2025-08-29 11:29:36.679328: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc28a400 of size 256 next 26\n",
      "2025-08-29 11:29:36.679331: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc28a500 of size 256 next 27\n",
      "2025-08-29 11:29:36.679334: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc28a600 of size 256 next 28\n",
      "2025-08-29 11:29:36.679336: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc28a700 of size 2580480 next 29\n",
      "2025-08-29 11:29:36.679339: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc500700 of size 256 next 30\n",
      "2025-08-29 11:29:36.679342: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc500800 of size 256 next 31\n",
      "2025-08-29 11:29:36.679345: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc500900 of size 256 next 32\n",
      "2025-08-29 11:29:36.679347: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc500a00 of size 256 next 33\n",
      "2025-08-29 11:29:36.679350: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc500b00 of size 256 next 34\n",
      "2025-08-29 11:29:36.679353: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc500c00 of size 256 next 35\n",
      "2025-08-29 11:29:36.679356: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc500d00 of size 256 next 36\n",
      "2025-08-29 11:29:36.679359: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc500e00 of size 256 next 37\n",
      "2025-08-29 11:29:36.679362: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc500f00 of size 256 next 42\n",
      "2025-08-29 11:29:36.679364: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc501000 of size 1280 next 43\n",
      "2025-08-29 11:29:36.679367: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc501500 of size 36864 next 44\n",
      "2025-08-29 11:29:36.679370: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50a500 of size 256 next 45\n",
      "2025-08-29 11:29:36.679373: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50a600 of size 256 next 38\n",
      "2025-08-29 11:29:36.679375: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50a700 of size 512 next 39\n",
      "2025-08-29 11:29:36.679378: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50a900 of size 256 next 40\n",
      "2025-08-29 11:29:36.679381: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50aa00 of size 256 next 46\n",
      "2025-08-29 11:29:36.679384: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50ab00 of size 256 next 41\n",
      "2025-08-29 11:29:36.679387: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50ac00 of size 256 next 47\n",
      "2025-08-29 11:29:36.679389: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50ad00 of size 256 next 48\n",
      "2025-08-29 11:29:36.679392: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50ae00 of size 256 next 49\n",
      "2025-08-29 11:29:36.679395: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50af00 of size 256 next 50\n",
      "2025-08-29 11:29:36.679398: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50b000 of size 1280 next 51\n",
      "2025-08-29 11:29:36.679401: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50b500 of size 256 next 52\n",
      "2025-08-29 11:29:36.679403: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50b600 of size 36864 next 53\n",
      "2025-08-29 11:29:36.679406: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc514600 of size 256 next 54\n",
      "2025-08-29 11:29:36.679409: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc514700 of size 256 next 55\n",
      "2025-08-29 11:29:36.679412: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc514800 of size 2580480 next 56\n",
      "2025-08-29 11:29:36.679414: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc78a800 of size 256 next 57\n",
      "2025-08-29 11:29:36.679417: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc78a900 of size 512 next 58\n",
      "2025-08-29 11:29:36.679420: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc78ab00 of size 256 next 59\n",
      "2025-08-29 11:29:36.679423: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc78ac00 of size 256 next 60\n",
      "2025-08-29 11:29:36.679425: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc78ad00 of size 256 next 61\n",
      "2025-08-29 11:29:36.679428: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc78ae00 of size 256 next 62\n",
      "2025-08-29 11:29:36.679431: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc78af00 of size 256 next 63\n",
      "2025-08-29 11:29:36.679434: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc78b000 of size 256 next 64\n",
      "2025-08-29 11:29:36.679436: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc78b100 of size 2580480 next 65\n",
      "2025-08-29 11:29:36.679439: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01100 of size 256 next 66\n",
      "2025-08-29 11:29:36.679442: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01200 of size 256 next 67\n",
      "2025-08-29 11:29:36.679445: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01300 of size 256 next 68\n",
      "2025-08-29 11:29:36.679448: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01400 of size 256 next 69\n",
      "2025-08-29 11:29:36.679450: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01500 of size 256 next 70\n",
      "2025-08-29 11:29:36.679453: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01600 of size 256 next 71\n",
      "2025-08-29 11:29:36.679456: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01700 of size 256 next 72\n",
      "2025-08-29 11:29:36.679458: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01800 of size 256 next 73\n",
      "2025-08-29 11:29:36.679461: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01900 of size 256 next 74\n",
      "2025-08-29 11:29:36.679464: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01a00 of size 2635776 next 75\n",
      "2025-08-29 11:29:36.679467: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dcc85200 of size 256 next 76\n",
      "2025-08-29 11:29:36.679470: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dcc85300 of size 256 next 77\n",
      "2025-08-29 11:29:36.679473: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dcc85400 of size 256 next 78\n",
      "2025-08-29 11:29:36.679476: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dcc85500 of size 256 next 79\n",
      "2025-08-29 11:29:36.679479: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dcc85600 of size 166636032 next 18446744073709551615\n",
      "2025-08-29 11:29:36.679482: I tensorflow/core/common_runtime/bfc_allocator.cc:1094]      Summary of in-use Chunks by size: \n",
      "2025-08-29 11:29:36.679486: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 61 Chunks of size 256 totalling 15.2KiB\n",
      "2025-08-29 11:29:36.679490: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 4 Chunks of size 512 totalling 2.0KiB\n",
      "2025-08-29 11:29:36.679493: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 5 Chunks of size 1280 totalling 6.2KiB\n",
      "2025-08-29 11:29:36.679497: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 4 Chunks of size 36864 totalling 144.0KiB\n",
      "2025-08-29 11:29:36.679500: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 4 Chunks of size 2580480 totalling 9.84MiB\n",
      "2025-08-29 11:29:36.679503: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 2635776 totalling 2.51MiB\n",
      "2025-08-29 11:29:36.679507: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 166636032 totalling 158.92MiB\n",
      "2025-08-29 11:29:36.679510: I tensorflow/core/common_runtime/bfc_allocator.cc:1101] Sum Total of in-use chunks: 171.44MiB\n",
      "2025-08-29 11:29:36.679513: I tensorflow/core/common_runtime/bfc_allocator.cc:1103] total_region_allocated_bytes_: 179765248 memory_limit_: 179765248 available bytes: 0 curr_region_allocation_bytes_: 359530496\n",
      "2025-08-29 11:29:36.679520: I tensorflow/core/common_runtime/bfc_allocator.cc:1109] Stats: \n",
      "Limit:                       179765248\n",
      "InUse:                       179765248\n",
      "MaxInUse:                    179765248\n",
      "NumAllocs:                         104\n",
      "MaxAllocSize:                166636032\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-08-29 11:29:36.679532: W tensorflow/core/common_runtime/bfc_allocator.cc:491] *******************************************************xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "2025-08-29 11:29:36.679561: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4B (rounded to 256)requested by op training_2/Adam/Cast\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-08-29 11:29:36.679579: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] BFCAllocator dump for GPU_0_bfc\n",
      "2025-08-29 11:29:36.679590: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (256): \tTotal Chunks: 61, Chunks in use: 61. 15.2KiB allocated for chunks. 15.2KiB in use in bin. 2.0KiB client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679594: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (512): \tTotal Chunks: 4, Chunks in use: 4. 2.0KiB allocated for chunks. 2.0KiB in use in bin. 2.0KiB client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679598: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1024): \tTotal Chunks: 5, Chunks in use: 5. 6.2KiB allocated for chunks. 6.2KiB in use in bin. 5.5KiB client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679602: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679606: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679609: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679613: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679617: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (32768): \tTotal Chunks: 4, Chunks in use: 4. 144.0KiB allocated for chunks. 144.0KiB in use in bin. 144.0KiB client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679621: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679624: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679627: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679631: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679634: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679639: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2097152): \tTotal Chunks: 5, Chunks in use: 5. 12.36MiB allocated for chunks. 12.36MiB in use in bin. 12.36MiB client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679642: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679645: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679649: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679652: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679655: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679660: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 1. 158.92MiB allocated for chunks. 158.92MiB in use in bin. 80.44MiB client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679664: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-29 11:29:36.679667: I tensorflow/core/common_runtime/bfc_allocator.cc:1056] Bin for 256B was 256B, Chunk State: \n",
      "2025-08-29 11:29:36.679670: I tensorflow/core/common_runtime/bfc_allocator.cc:1069] Next region of size 179765248\n",
      "2025-08-29 11:29:36.679676: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc000000 of size 1280 next 1\n",
      "2025-08-29 11:29:36.679679: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc000500 of size 256 next 2\n",
      "2025-08-29 11:29:36.679684: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc000600 of size 1280 next 3\n",
      "2025-08-29 11:29:36.679687: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc000b00 of size 36864 next 4\n",
      "2025-08-29 11:29:36.679690: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc009b00 of size 256 next 5\n",
      "2025-08-29 11:29:36.679693: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc009c00 of size 256 next 6\n",
      "2025-08-29 11:29:36.679696: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc009d00 of size 512 next 7\n",
      "2025-08-29 11:29:36.679699: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc009f00 of size 256 next 8\n",
      "2025-08-29 11:29:36.679702: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc00a000 of size 256 next 9\n",
      "2025-08-29 11:29:36.679705: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc00a100 of size 256 next 10\n",
      "2025-08-29 11:29:36.679707: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc00a200 of size 256 next 11\n",
      "2025-08-29 11:29:36.679710: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc00a300 of size 256 next 12\n",
      "2025-08-29 11:29:36.679729: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc00a400 of size 256 next 13\n",
      "2025-08-29 11:29:36.679732: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc00a500 of size 256 next 14\n",
      "2025-08-29 11:29:36.679735: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc00a600 of size 1280 next 15\n",
      "2025-08-29 11:29:36.679738: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc00ab00 of size 256 next 16\n",
      "2025-08-29 11:29:36.679741: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc00ac00 of size 36864 next 17\n",
      "2025-08-29 11:29:36.679744: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc013c00 of size 256 next 18\n",
      "2025-08-29 11:29:36.679746: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc013d00 of size 256 next 19\n",
      "2025-08-29 11:29:36.679749: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc013e00 of size 2580480 next 20\n",
      "2025-08-29 11:29:36.679753: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc289e00 of size 256 next 21\n",
      "2025-08-29 11:29:36.679756: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc289f00 of size 512 next 22\n",
      "2025-08-29 11:29:36.679759: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc28a100 of size 256 next 23\n",
      "2025-08-29 11:29:36.679762: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc28a200 of size 256 next 24\n",
      "2025-08-29 11:29:36.679765: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc28a300 of size 256 next 25\n",
      "2025-08-29 11:29:36.679768: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc28a400 of size 256 next 26\n",
      "2025-08-29 11:29:36.679770: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc28a500 of size 256 next 27\n",
      "2025-08-29 11:29:36.679773: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc28a600 of size 256 next 28\n",
      "2025-08-29 11:29:36.679776: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc28a700 of size 2580480 next 29\n",
      "2025-08-29 11:29:36.679779: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc500700 of size 256 next 30\n",
      "2025-08-29 11:29:36.679782: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc500800 of size 256 next 31\n",
      "2025-08-29 11:29:36.679785: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc500900 of size 256 next 32\n",
      "2025-08-29 11:29:36.679788: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc500a00 of size 256 next 33\n",
      "2025-08-29 11:29:36.679791: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc500b00 of size 256 next 34\n",
      "2025-08-29 11:29:36.679794: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc500c00 of size 256 next 35\n",
      "2025-08-29 11:29:36.679797: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc500d00 of size 256 next 36\n",
      "2025-08-29 11:29:36.679800: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc500e00 of size 256 next 37\n",
      "2025-08-29 11:29:36.679803: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc500f00 of size 256 next 42\n",
      "2025-08-29 11:29:36.679806: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc501000 of size 1280 next 43\n",
      "2025-08-29 11:29:36.679808: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc501500 of size 36864 next 44\n",
      "2025-08-29 11:29:36.679811: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50a500 of size 256 next 45\n",
      "2025-08-29 11:29:36.679814: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50a600 of size 256 next 38\n",
      "2025-08-29 11:29:36.679817: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at conv_ops.cc:1077 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,1,3,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2025-08-29 11:29:36.679826: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50a700 of size 512 next 39\n",
      "2025-08-29 11:29:36.679830: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50a900 of size 256 next 40\n",
      "2025-08-29 11:29:36.679833: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50aa00 of size 256 next 46\n",
      "2025-08-29 11:29:36.679836: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50ab00 of size 256 next 41\n",
      "2025-08-29 11:29:36.679839: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50ac00 of size 256 next 47\n",
      "2025-08-29 11:29:36.679842: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50ad00 of size 256 next 48\n",
      "2025-08-29 11:29:36.679845: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50ae00 of size 256 next 49\n",
      "2025-08-29 11:29:36.679848: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50af00 of size 256 next 50\n",
      "2025-08-29 11:29:36.679850: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50b000 of size 1280 next 51\n",
      "2025-08-29 11:29:36.679853: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50b500 of size 256 next 52\n",
      "2025-08-29 11:29:36.679856: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc50b600 of size 36864 next 53\n",
      "2025-08-29 11:29:36.679859: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc514600 of size 256 next 54\n",
      "2025-08-29 11:29:36.679862: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc514700 of size 256 next 55\n",
      "2025-08-29 11:29:36.679865: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc514800 of size 2580480 next 56\n",
      "2025-08-29 11:29:36.679868: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc78a800 of size 256 next 57\n",
      "2025-08-29 11:29:36.679871: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc78a900 of size 512 next 58\n",
      "2025-08-29 11:29:36.679874: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc78ab00 of size 256 next 59\n",
      "2025-08-29 11:29:36.679877: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc78ac00 of size 256 next 60\n",
      "2025-08-29 11:29:36.679880: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc78ad00 of size 256 next 61\n",
      "2025-08-29 11:29:36.679883: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc78ae00 of size 256 next 62\n",
      "2025-08-29 11:29:36.679886: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc78af00 of size 256 next 63\n",
      "2025-08-29 11:29:36.679889: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc78b000 of size 256 next 64\n",
      "2025-08-29 11:29:36.679892: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dc78b100 of size 2580480 next 65\n",
      "2025-08-29 11:29:36.679895: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01100 of size 256 next 66\n",
      "2025-08-29 11:29:36.679898: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01200 of size 256 next 67\n",
      "2025-08-29 11:29:36.679901: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01300 of size 256 next 68\n",
      "2025-08-29 11:29:36.679904: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01400 of size 256 next 69\n",
      "2025-08-29 11:29:36.679907: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01500 of size 256 next 70\n",
      "2025-08-29 11:29:36.679909: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01600 of size 256 next 71\n",
      "2025-08-29 11:29:36.679912: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01700 of size 256 next 72\n",
      "2025-08-29 11:29:36.679915: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01800 of size 256 next 73\n",
      "2025-08-29 11:29:36.679918: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01900 of size 256 next 74\n",
      "2025-08-29 11:29:36.679921: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dca01a00 of size 2635776 next 75\n",
      "2025-08-29 11:29:36.679924: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dcc85200 of size 256 next 76\n",
      "2025-08-29 11:29:36.679927: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dcc85300 of size 256 next 77\n",
      "2025-08-29 11:29:36.679930: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dcc85400 of size 256 next 78\n",
      "2025-08-29 11:29:36.679933: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dcc85500 of size 256 next 79\n",
      "2025-08-29 11:29:36.679937: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 1526dcc85600 of size 166636032 next 18446744073709551615\n",
      "2025-08-29 11:29:36.679940: I tensorflow/core/common_runtime/bfc_allocator.cc:1094]      Summary of in-use Chunks by size: \n",
      "2025-08-29 11:29:36.679944: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 61 Chunks of size 256 totalling 15.2KiB\n",
      "2025-08-29 11:29:36.679948: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 4 Chunks of size 512 totalling 2.0KiB\n",
      "2025-08-29 11:29:36.679951: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 5 Chunks of size 1280 totalling 6.2KiB\n",
      "2025-08-29 11:29:36.679955: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 4 Chunks of size 36864 totalling 144.0KiB\n",
      "2025-08-29 11:29:36.679958: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 4 Chunks of size 2580480 totalling 9.84MiB\n",
      "2025-08-29 11:29:36.679962: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 2635776 totalling 2.51MiB\n",
      "2025-08-29 11:29:36.679965: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 166636032 totalling 158.92MiB\n",
      "2025-08-29 11:29:36.679969: I tensorflow/core/common_runtime/bfc_allocator.cc:1101] Sum Total of in-use chunks: 171.44MiB\n",
      "2025-08-29 11:29:36.679972: I tensorflow/core/common_runtime/bfc_allocator.cc:1103] total_region_allocated_bytes_: 179765248 memory_limit_: 179765248 available bytes: 0 curr_region_allocation_bytes_: 359530496\n",
      "2025-08-29 11:29:36.679978: I tensorflow/core/common_runtime/bfc_allocator.cc:1109] Stats: \n",
      "Limit:                       179765248\n",
      "InUse:                       179765248\n",
      "MaxInUse:                    179765248\n",
      "NumAllocs:                         104\n",
      "MaxAllocSize:                166636032\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-08-29 11:29:36.679986: W tensorflow/core/common_runtime/bfc_allocator.cc:491] *******************************************************xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "2025-08-29 11:29:36.680631: W tensorflow/core/framework/op_kernel.cc:1768] RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,1,3,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d_2/Conv2D}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[loss_1/mul/_203]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,1,3,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d_2/Conv2D}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 101\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ens_no  \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m,NNrepeats\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    100\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ensemble \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mens_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m     skill, history, pred, R2_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     trained_models\u001b[38;5;241m.\u001b[39mappend((skill, skill, history\u001b[38;5;241m.\u001b[39mhistory, pred, R2_val))\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Increase fold number\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 205\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(X_train, y_train, X_test, y_test, y_mean, y_std)\u001b[0m\n\u001b[1;32m    202\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# restore_best_weights=1 is True\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Make sure my_callbacks is defined in this scope or passed as an argument\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmy_callbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Added '+' for list concatenation\u001b[39;00m\n\u001b[1;32m    208\u001b[0m skill \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    209\u001b[0m pred \u001b[38;5;241m=\u001b[39m (model\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;241m*\u001b[39m y_std) \u001b[38;5;241m+\u001b[39m y_mean \u001b[38;5;66;03m# Make sure y_std and y_mean are defined\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training_v1.py:855\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    854\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m--> 855\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py:734\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    729\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`validation_steps` should not be specified if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    730\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`validation_data` is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m         )\n\u001b[1;32m    732\u001b[0m     val_x, val_y, val_sample_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_sample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py:419\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m callbacks\u001b[38;5;241m.\u001b[39m_call_batch_hook(\n\u001b[1;32m    415\u001b[0m     mode, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_index, batch_logs\n\u001b[1;32m    416\u001b[0m )\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Get outputs.\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m batch_outs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_outs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    421\u001b[0m     batch_outs \u001b[38;5;241m=\u001b[39m [batch_outs]\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/keras/backend.py:4577\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4568\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4569\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m feed_arrays \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_arrays\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4573\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m session \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\n\u001b[1;32m   4574\u001b[0m ):\n\u001b[1;32m   4575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[0;32m-> 4577\u001b[0m fetched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marray_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches) :])\n\u001b[1;32m   4579\u001b[0m output_structure \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[1;32m   4580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_structure,\n\u001b[1;32m   4581\u001b[0m     fetched[: \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs)],\n\u001b[1;32m   4582\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   4583\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/tensorflow/python/client/session.py:1481\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1480\u001b[0m   run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1481\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRunCallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1484\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m   1485\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,1,3,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d_2/Conv2D}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[loss_1/mul/_203]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,1,3,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d_2/Conv2D}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "#                             Train loop                              #\n",
    "#######################################################################\n",
    "# X = input\n",
    "# y = output\n",
    "\n",
    "# X_mean = np.mean(X, axis=0)\n",
    "# X_std = np.std(X, axis=0)\n",
    "\n",
    "# y_mean = np.mean(y, axis=0)\n",
    "# y_std = np.std(y, axis=0)\n",
    "\n",
    "# if remove_mean ==0:\n",
    "#     X_mean = 0\n",
    "#     y_mean = 0\n",
    "\n",
    "    \n",
    "    \n",
    "# if divide_std ==0:\n",
    "#     X_std = 1\n",
    "#     y_std = 1\n",
    "# else:\n",
    "#     X_std = X_std[:,:, tf.newaxis]\n",
    "    \n",
    "    \n",
    "    \n",
    "# sio.savemat(os.path.join(output_dir,'Normalization.mat'), \n",
    "#             {'X_mean': X_mean, 'X_std': X_std,'y_mean':y_mean,'y_std':y_std})\n",
    "\n",
    "# # if not os.path.exists(os.path.join(output_dir, 'inputs_info.mat')):\n",
    "\n",
    "        \n",
    "trained_models = []\n",
    "# y_pred_reconstructed_allfolds = []\n",
    "\n",
    "\n",
    "# # Train the neural network multiple times using k-fold cross validation\n",
    "# # Define the K-fold Cross Validator\n",
    "# kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "# fold_no = 1\n",
    "# for trainind, testind in kfold.split(X, y):   \n",
    "#     # break\n",
    "#     X_train = X[trainind,:,:, tf.newaxis]\n",
    "#     y_train = y[trainind,:]\n",
    "#     X_test = X[testind,:, :,tf.newaxis]\n",
    "#     y_test = y[testind,:]\n",
    " \n",
    "#     # Normalize the input and out data based on the information from the entire PI control run\n",
    "    \n",
    "    \n",
    "#     X_train = (X_train - X_mean)/X_std\n",
    "#     X_test = (X_test - X_mean)/X_std\n",
    "    \n",
    "#     y_train = (y_train - y_mean)/y_std\n",
    "#     y_test = (y_test - y_mean)/y_std\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Generate a print\n",
    "#     logger.info('------------------------------------------------------------------------')\n",
    "#     # We use ensemble training for each fold of the cross-validation\n",
    "X = input\n",
    "y = output\n",
    "\n",
    "# Train the neural network multiple times using k-fold cross validation\n",
    "kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "fold_no = 1\n",
    "for trainind, testind in kfold.split(X, y):\n",
    "    # --- Step 1: Split the data FIRST ---\n",
    "    X_train_raw = X[trainind]\n",
    "    y_train_raw = y[trainind]\n",
    "    X_test_raw = X[testind]\n",
    "    y_test_raw = y[testind]\n",
    "\n",
    "    # --- Step 2: Calculate normalization stats ONLY from the training data ---\n",
    "    X_mean = np.mean(X_train_raw, axis=0)\n",
    "    X_std = np.std(X_train_raw, axis=0)\n",
    "    y_mean = np.mean(y_train_raw, axis=0)\n",
    "    y_std = np.std(y_train_raw, axis=0)\n",
    "    \n",
    "    # Handle cases where you don't want to normalize\n",
    "    if remove_mean == 0:\n",
    "        X_mean = 0\n",
    "        y_mean = 0\n",
    "    if divide_std == 0:\n",
    "        X_std = 1\n",
    "        y_std = 1\n",
    "        \n",
    "    # --- Step 3: Normalize BOTH sets using the TRAINING stats ---\n",
    "    X_train = (X_train_raw - X_mean) / X_std\n",
    "    X_test = (X_test_raw - X_mean) / X_std # Use the mean/std from the train set\n",
    "\n",
    "    y_train = (y_train_raw - y_mean) / y_std\n",
    "    y_test = (y_test_raw - y_mean) / y_std # Use the mean/std from the train set\n",
    "    \n",
    "    # --- Step 4: Add the channel dimension for the CNN ---\n",
    "    X_train = X_train[..., tf.newaxis]\n",
    "    X_test = X_test[..., tf.newaxis]\n",
    "    for ens_no  in np.arange(1,NNrepeats+1):\n",
    "        logger.info(f'Training for fold {fold_no} ensemble {ens_no}...')\n",
    "        skill, history, pred, R2_val = train_model(X_train, y_train, X_test, y_test,y_mean,y_std)\n",
    "        trained_models.append((skill, skill, history.history, pred, R2_val))\n",
    "\n",
    "        \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "        \n",
    "        \n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info(f'Training with {num_folds}-fold cross-validation finished!') \n",
    "logger.info('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be32e148",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "have a quick look at the performance of the trainined neural network"
   },
   "outputs": [],
   "source": [
    "\n",
    "skills = [trained_model[1] for trained_model in trained_models[0:num_folds*NNrepeats]]\n",
    "median_skill = np.median(skills)\n",
    "median_index = skills.index(median_skill)\n",
    "minimum_skill = np.min(skills)\n",
    "min_index = skills.index(minimum_skill)\n",
    "std_dev_skill = np.std(skills)\n",
    "\n",
    "formatted_skills = [f\"{skill:.4f}\" for skill in skills]\n",
    "logger.info(f\"Losses in testing set: {formatted_skills}\")\n",
    "fold_median = median_index // NNrepeats + 1  # Compute fold number\n",
    "ensemble_median = median_index % NNrepeats + 1  # Compute ensemble number\n",
    "logger.info(f\"Median Loss is: {median_skill:.4f}, which occurs at Fold {fold_median} Ensemble {ensemble_median}\")\n",
    "fold_min = min_index // NNrepeats + 1  # Compute fold number\n",
    "ensemble_min = min_index % NNrepeats + 1  # Compute ensemble number\n",
    "logger.info(f\"Minimum Loss is: {minimum_skill:.4f}, which occurs at Fold {fold_min} Ensemble {ensemble_min}\")\n",
    "logger.info(f\"Standard Deviation of the Loss is: {std_dev_skill:.4f}\")\n",
    "ratio = std_dev_skill/median_skill * 100\n",
    "logger.info(f\"Standard Deviation/Median: {ratio:.2f}%\")\n",
    "\n",
    "# Check if the standard deviation to median skill ratio is higher than 30%\n",
    "if ratio > 30:\n",
    "    warning_message = (\"Warning: The standard deviation of model skill across \"\n",
    "                       \"folds and ensembles is large!\\nStandard Deviation/Median Skill: {ratio:.2f}%\").format(ratio=ratio)\n",
    "    with open(os.path.join(output_dir,'Warning_'+names_str+'.txt'), \"w\") as file:\n",
    "        file.write(warning_message)\n",
    "    with open(os.path.join(output_dir,'..','Warning_'+names_str+'.txt'), \"w\") as file:\n",
    "        file.write(warning_message)\n",
    "\n",
    "\n",
    "# Plot the loss function during training for all folds\n",
    "# First, find the global minimum and maximum loss values across all folds\n",
    "min_loss = min(min(trained_models[i][2]['loss']+trained_models[i][2]['val_loss']) for i in range(num_folds))\n",
    "max_loss = max(max(trained_models[i][2]['loss']+trained_models[i][2]['val_loss']) for i in range(num_folds))\n",
    "\n",
    "plt.figure(figsize=(9, 12))\n",
    "\n",
    "# Plot the first fold outside the loop to avoid repeating the legend setting\n",
    "plt.subplot(num_folds, 1, 1)\n",
    "for nn in range(NNrepeats):\n",
    "    plt.plot(trained_models[nn][2]['loss'],'-k')\n",
    "    plt.plot(trained_models[nn][2]['val_loss'],'-r')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')  # Set logarithmic scale\n",
    "plt.ylim(min_loss, max_loss)  # Set the same y-limits for all subplots\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Now plot the remaining folds\n",
    "for n in range(1, num_folds):\n",
    "    plt.subplot(num_folds, 1, n+1)\n",
    "    for nn in range(NNrepeats):\n",
    "        plt.plot(trained_models[n*NNrepeats+nn][2]['loss'],'-k')\n",
    "        plt.plot(trained_models[n*NNrepeats+nn][2]['val_loss'],'-r')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')  # Set logarithmic scale\n",
    "    plt.ylim(min_loss, max_loss)  # Set the same y-limits for all subplots\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.subplots_adjust(hspace=0.5)  # Adjust space between plots if needed\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'TrainingLoss_' + names_str  + '.png'),dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "R2_each_fold_ens = [trained_models[i][4] for i in range(0,num_folds*NNrepeats)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22894312",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "Model_preds = [np.concatenate([trained_models[n*NNrepeats + i][3] for n in range(num_folds)]) for i in range(NNrepeats)]\n",
    "Model_pred = np.mean(Model_preds,axis=0)\n",
    "Model_error = Model_pred - y\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30, 6))  \n",
    "plt.plot(time, y, label=\"Truth\", color=\"C0\", linewidth=2)  # Use color and linewidth\n",
    "plt.plot(time, Model_pred, label=\"Prediction\", color=\"C1\", linestyle=\"-\", linewidth=1.5)  # Dashed line for prediction\n",
    "plt.xlabel(\"Time (year)\", fontsize=14)\n",
    "plt.ylabel(names_str, fontsize=14)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend(fontsize=12, loc=\"best\")\n",
    "r2 = r2_score(y, Model_pred)\n",
    "plt.text(0.05, 0.9, f\"$R^2$: {r2:.3f}\", transform=plt.gca().transAxes,\n",
    "         fontsize=14, bbox=dict(facecolor=\"white\", alpha=0.7, edgecolor=\"black\"))\n",
    "plt.savefig(os.path.join(output_dir, names_str+\"_TruthvsPred.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "######### scatter plot\n",
    "\n",
    "plt.figure(figsize=(8, 8))  # Keep square aspect ratio\n",
    "\n",
    "# Scatter plot with adjusted marker size and transparency\n",
    "plt.plot(Model_pred, y, 'o', markersize=5, alpha=0.6, color=\"C0\", label=\"Data Points\")\n",
    "\n",
    "# Compute common limits based on both Model_pred and y\n",
    "min_val = min(np.min(Model_pred), np.min(y))\n",
    "max_val = max(np.max(Model_pred), np.max(y))\n",
    "\n",
    "# Set same limits for x and y\n",
    "plt.xlim(min_val, max_val)\n",
    "plt.ylim(min_val, max_val)\n",
    "\n",
    "# Define consistent tick locations\n",
    "num_ticks = 6  # Adjust this for more or fewer ticks\n",
    "ticks = np.linspace(min_val, max_val, num_ticks)\n",
    "\n",
    "plt.xticks(ticks)\n",
    "plt.yticks(ticks)\n",
    "\n",
    "# 1:1 Reference Line\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'k--', linewidth=2, label=\"1:1 Reference\")\n",
    "\n",
    "# Labels and formatting\n",
    "plt.xlabel(\"Prediction\", fontsize=16, fontweight='bold')\n",
    "plt.ylabel(\"Truth\", fontsize=16, fontweight='bold')\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Compute and display R² score\n",
    "r2 = r2_score(y, Model_pred)\n",
    "plt.text(0.05, 0.9, f\"$R^2$: {r2:.3f}\", transform=plt.gca().transAxes,\n",
    "         fontsize=14, bbox=dict(facecolor=\"white\", alpha=0.7, edgecolor=\"black\"))\n",
    "\n",
    "# Equal aspect ratio for fair comparison\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "# Add legend (only for reference line)\n",
    "plt.legend(fontsize=12, loc=\"center left\")\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(os.path.join(output_dir, \"TOA_TruthvsPred_scatter.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8fc216",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "copy this script to the directory that stores the trained NN"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def copy_script(target_directory):\n",
    "    # Get the current script file path\n",
    "    current_script = \"try_withGelu.ipynb\"\n",
    "    # current_script = script_path_and_name\n",
    "    \n",
    "    # Ensure the target directory exists, create if it does not\n",
    "    os.makedirs(target_directory, exist_ok=True)\n",
    "    \n",
    "    # Define the target path for the script\n",
    "    target_path = os.path.join(target_directory, os.path.basename(current_script))\n",
    "    \n",
    "    # Copy the script\n",
    "    shutil.copy(current_script, target_path)\n",
    "    logger.info(f\"Script copied to {target_path}\")\n",
    "\n",
    "# Example usage\n",
    "copy_script(output_dir)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e12b1a-4c08-4287-bdc0-bffc8d091e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python (tf210)",
   "language": "python",
   "name": "tf210"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
