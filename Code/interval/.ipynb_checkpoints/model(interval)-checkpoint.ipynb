{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d14b88c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 06:07:54.511494: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-03 06:07:54.626871: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-03 06:07:54.662760: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Mon Feb 24 10:04:04 2025\n",
    "\n",
    "@author: hwei\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# V3 25-03-2025 updated normalization\n",
    "\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense,LeakyReLU, Dropout, Add, Activation, Conv2D, Flatten, MaxPooling2D, Dense, PReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from matplotlib.colors import Normalize\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "tf.keras.utils.set_random_seed(0)\n",
    "tf.config.list_physical_devices('GPU')  \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras.engine.training_v1\")\n",
    "import xarray as xr\n",
    "import innvestigate\n",
    "import scipy.io as sio\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95f4a3d5-fda9-40b2-ae97-0846b96767bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/jet/home/ezhu3/Code\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a76ab0a-70e8-40d3-ba53-aaa340a77781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set. Models will be saved in: /ocean/projects/ees250004p/ezhu3/data/CESM1/trained_models_by_interval\n"
     ]
    }
   ],
   "source": [
    "# --- Experiment Configuration ---\n",
    "# 1. DEFINE a list of how you want to divide the data.\n",
    "#    This corresponds to your colleague's 'PartialDataTot'.\n",
    "#    e.g., [1, 2, 4, 10, 20] will run the experiment for the full dataset,\n",
    "#    then for 2 halves, 4 quarters, 10 tenths, and 20 twentieths.\n",
    "INTERVAL_DIVISIONS = [1, 2, 4, 10, 20]\n",
    "\n",
    "# 2. DEFINE the directory where you want to save all the trained models.\n",
    "OUTPUT_MODEL_DIR = '/ocean/projects/ees250004p/ezhu3/data/CESM1/trained_models_by_interval'\n",
    "\n",
    "# 3. This code creates the directory if it doesn't already exist.\n",
    "os.makedirs(OUTPUT_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# 4. (Optional) This dictionary will store the file paths of every model we train.\n",
    "#    It will be very useful in the next phase (evaluation).\n",
    "all_model_paths = {}\n",
    "\n",
    "print(\"Configuration set. Models will be saved in:\", OUTPUT_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c3435-9030-45fd-9f63-4821b48f1294",
   "metadata": {},
   "source": [
    "##IMPORTANT!!\n",
    "NEED TO CHANGE INPUT/OUTPUT path for CESM1/CESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "560eda33",
   "metadata": {
    "title": "load data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (lat: 192, lon: 288, time: 1801)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 -90.0 -89.06 -88.12 -87.17 ... 87.17 88.12 89.06 90.0\n",
      "  * lon      (lon) float64 0.0 1.25 2.5 3.75 5.0 ... 355.0 356.2 357.5 358.8\n",
      "  * time     (time) int64 400 401 402 403 404 405 ... 2196 2197 2198 2199 2200\n",
      "Data variables:\n",
      "    TS       (time, lat, lon) float32 ...\n",
      "    TS_anom  (time, lat, lon) float32 ...\n",
      "Attributes:\n",
      "    script:   /glade/work/dongy24/Python/create_input_for_huaiyu.ipynb\n",
      "    author:   Y. Dong, 03/24/2025\n"
     ]
    }
   ],
   "source": [
    "# load the input variable -- global Surface temperature\n",
    "\n",
    "#CESM1 control\n",
    "ds = xr.open_dataset(\"/ocean/projects/ees250004p/ezhu3/data/CESM1/control/input.B1850C5CN.TS.ANN.0400-2200.new.nc\")\n",
    "\n",
    "#CESM2 data 1: \n",
    "#ds = xr.open_dataset(\"/ocean/projects/ees250004p/ezhu3/data/CESM2/control/input.CESM2-B1850.TS_detrend.ANN.nc\")\n",
    "\n",
    "#CESM2 data2:\n",
    "#ds = xr.open_dataset(\"/ocean/projects/ees250004p/ezhu3/data/CESM2/control/input.CESM2-B1850.TS.ANN.nc\")\n",
    "\n",
    "#ds = xr.open_dataset(\"E:\\\\Yue\\\\CESM2\\\\control\\\\input.CESM2-B1850.TS_detrend.ANN.nc\")# Display dataset info\n",
    "print(ds)\n",
    "\n",
    "# Access a specific variable\n",
    "TS = ds[\"TS_anom\"] # Surface temperature (radiative)     units = 'K'\n",
    "lat = ds[\"lat\"] #latitude\n",
    "lon = ds[\"lon\"] #longitude\n",
    "time = ds[\"time\"]\n",
    "\n",
    "\n",
    "# Define the directory where the output variables are stored\n",
    "data_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM1/control' #this is for CESM1\n",
    "# Define the filenames and corresponding variable names\n",
    "files = {\n",
    "    #CESM1:\n",
    "    \"TOA_anom\": \"output.B1850C5CN.TOA.gmean.ANN.0400-2200.new.nc\"\n",
    "    \n",
    "    #CESM2 data 1:\n",
    "    #\"TOA_anom\": \"output.CESM2-B1850.TOA_detrend.ANN.nc\"\n",
    "    #CESM2 data 2:\n",
    "    #\"TOA_anom\": \"output.CESM2-B1850.TOA.ANN.nc\"\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "# Load variables\n",
    "for var, filename in files.items():\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    datasets[var] = ds[var]  # Extract only the variable\n",
    "\n",
    "\n",
    "# Now the variables can be accessed as datasets[\"CRE\"], datasets[\"TOA\"], etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3dd24db",
   "metadata": {
    "title": "Neural Network Hyperparameters"
   },
   "outputs": [],
   "source": [
    "# Neural network architecture:\n",
    "# Example: [64, 64, 64] means three hidden layers, each containing 64 neurons.\n",
    "kernels = [32, 32]\n",
    "kernel_acts =  [\"gelu\", \"gelu\"]\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "rng_seed = 42\n",
    "hiddens = [16, 8]\n",
    "activation_function_dense = [\"PRelu\", \"PRelu\"]\n",
    "pool_size = 2\n",
    "\n",
    "# Loss function for regression tasks:\n",
    "# Options: 'mse' (mean squared error), 'mae' (mean absolute error), 'mape' (mean absolute percentage error)\n",
    "# Full list of regression losses: https://keras.io/api/losses/\n",
    "loss_function = 'mse'\n",
    "\n",
    "\n",
    "reg_strength = 0         # L2 regularization strength; 1e-1~1e-5\n",
    "dropout_rate = 0.25         # Dropout rate (0.0 to disable dropout)\n",
    "\n",
    "\n",
    "# low-pass filter time scale; 0 means no low-pass filter\n",
    "LPF_year = 0\n",
    "\n",
    "\n",
    "#### normalization\n",
    "remove_mean = 0\n",
    "divide_std = 1\n",
    "\n",
    "\n",
    "#### usually we do not change the parameters below \n",
    "\n",
    "# Training configuration\n",
    "epoch_max = 25000            # Maximum number of training epochs\n",
    "batch_size = 32            # Batch size used during training\n",
    "learning_rate = 0.000005       # Default learning rate for Adam optimizer 0.001\n",
    "num_folds = 5 # Number of fold during cross-validation\n",
    "NNrepeats = 1 # Repeat the training for NNrepeats times\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c050e93",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "pre-process data"
   },
   "outputs": [],
   "source": [
    "# input_raw = TS.values.reshape(TS.shape[0],TS.shape[1]*TS.shape[2])\n",
    "# For CNN, we don't need to flatten the data\n",
    "input_raw = TS.values\n",
    "\n",
    "\n",
    "\n",
    "# Define the variable names as a comma-separated string\n",
    "# names_strALL = \"CRE,FLNT,FSNT,LCC,LWCF,SWCF,TCC,TOA\"\n",
    "# If we only want to reconstruct TOA\n",
    "names_strALL = \"TOA_anom\"\n",
    "names_str = \"TOA_anom\"\n",
    "\n",
    "output_raw = datasets[names_str].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b49bea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_NN_name():\n",
    "\n",
    "    NN_name = 'CNN'\n",
    "        \n",
    "    activation_function_str = '_'+kernel_acts[0]+ '+'+activation_function_dense[0] \n",
    "    loss_function_str = '_'+str(loss_function)+'loss' if loss_function!='mse' else ''\n",
    "    \n",
    "    if remove_mean + divide_std == 0:\n",
    "        scaler_str = 'NoScaler_'\n",
    "    elif remove_mean + divide_std == 2:\n",
    "        scaler_str = 'StandardScaler_'\n",
    "    elif remove_mean:\n",
    "        scaler_str = 'RemoveMean_'\n",
    "    elif divide_std:\n",
    "        scaler_str = 'DivideSTD_'\n",
    "    LPF_str = f'_LPF{int(LPF_year)}Year' if LPF_year else ''\n",
    "    NN_structure_str = 'x'.join(map(str, kernels)) \n",
    "    reg_str = f'Reg{reg_strength}' + (f'Drop{dropout_rate}' if dropout_rate != 0 else '')\n",
    "    batch_size_str =  f'BS{batch_size}_' if batch_size !=600 else ''\n",
    "    return f\"{NN_name}_{scaler_str}Neur{NN_structure_str}_{batch_size_str}{num_folds}foldCV_{reg_str}{loss_function_str}{activation_function_str}{LPF_str}\"\n",
    "NN_name = create_NN_name()\n",
    "\n",
    "    \n",
    "def construct_output_directory(data_dir, NN_name):\n",
    "    output_dir = os.path.join(data_dir, 'NeuralNet', NN_name, names_str)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "output_dir = construct_output_directory(data_dir, NN_name)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "840c1afa",
   "metadata": {
    "title": "Define logger that can print useful information into a logfile"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 05:52:48,584 - logfile\n",
      "2025-07-03 05:52:48.590404: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-03 05:52:49,632 - Using GPU for training.\n",
      "2025-07-03 05:52:49.625887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31088 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
      "2025-07-03 05:52:49,633 - 1 Physical GPUs, 1 Logical GPUs\n",
      "2025-07-03 05:52:49,634 - None\n",
      "2025-07-03 05:52:49,635 - Output path: created successfully\n",
      "2025-07-03 05:52:49,635 - The output path is /ocean/projects/ees250004p/ezhu3/data/CESM1/control/NeuralNet/CNN_DivideSTD_Neur32x32_BS32_5foldCV_Reg0Drop0.25_gelu+PRelu/TOA_anom\n"
     ]
    }
   ],
   "source": [
    "logger_name = 'logfile.log'  \n",
    "# Configure logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create handlers\n",
    "file_handler = logging.FileHandler(os.path.join(output_dir, logger_name))\n",
    "console_handler = logging.StreamHandler()\n",
    "\n",
    "# Set level and format for handlers\n",
    "file_handler.setLevel(logging.INFO)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Test the setup\n",
    "logger.info(\"logfile\")\n",
    "\n",
    " \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    logger.info(\"Using GPU for training.\")\n",
    "    logger.info(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "else:\n",
    "    logger.info(\"Using CPU for training.\") \n",
    "\n",
    "logger.info(os.getenv('TF_GPU_ALLOCATOR'))\n",
    "    \n",
    "logger.info(f\"Output path: {'created successfully' if os.path.exists(output_dir) else 'already exists'}\")\n",
    "logger.info(f\"The output path is {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2d9886a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 05:52:49,643 - ------------------------------------------------------------------------\n",
      "2025-07-03 05:52:49,644 - ------------------------------------------------------------------------\n",
      "2025-07-03 05:52:49,645 - Applying 0-year low pass filter ...\n",
      "2025-07-03 05:52:49,645 - ------------------------------------------------------------------------\n",
      "2025-07-03 05:52:49,646 - ------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full input data shape: (1801, 192, 288)\n",
      "Full output data shape: (1801, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info(f'Applying {LPF_year}-year low pass filter ...')\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info('------------------------------------------------------------------------')\n",
    "\n",
    "from scipy.signal import butter, sosfilt\n",
    "\n",
    "def apply_low_pass_filter(data, cutoff_freq, order=5, sampling_rate=1, padding_length=None):\n",
    "    \"\"\"Applies a Butterworth low-pass filter to the given data.\"\"\"\n",
    "    sos = butter(order, cutoff_freq, btype='low', output='sos', analog=False, fs=sampling_rate)\n",
    "    \n",
    "    # Apply Boundary Padding to the data before filtering\n",
    "    padded_data = np.pad(data, [(padding_length, padding_length), (0, 0)], mode='reflect')\n",
    "    \n",
    "    # Apply the filter across each column without explicit looping\n",
    "    filtered_data = sosfilt(sos, padded_data, axis=0)\n",
    "    \n",
    "    # Remove padding\n",
    "    return filtered_data[padding_length:-padding_length]\n",
    "\n",
    "if LPF_year:\n",
    "    # Calculate the sampling rate (monthly data)\n",
    "    sampling_rate = 1  # Data is sampled monthly    \n",
    "    cutoff_freq = 1 / LPF_year\n",
    "    order = 5\n",
    "    padding_length =  3* LPF_year\n",
    "\n",
    "if LPF_year:\n",
    "    input = apply_low_pass_filter(input_raw, cutoff_freq, order, sampling_rate, padding_length)     \n",
    "    output= apply_low_pass_filter(output_raw, cutoff_freq, order, sampling_rate, padding_length)     \n",
    "    \n",
    "    plt.figure(figsize=(30, 6))  \n",
    "    plt.plot(time, output_raw, label=\"Original\", color=\"C0\", linewidth=2)  # Use color and linewidth\n",
    "    plt.plot(time, output, label=\"Low-pass filtered\", color=\"C1\", linestyle=\"-\", linewidth=1.5)  # Dashed line for prediction\n",
    "    plt.xlabel(\"Time (year)\", fontsize=14)\n",
    "    plt.ylabel(names_str, fontsize=14)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.legend(fontsize=12, loc=\"best\")\n",
    "    plt.savefig(os.path.join(output_dir, names_str+\"_LPF.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    input  =  input_raw\n",
    "    output = output_raw\n",
    "\n",
    "input_full = input\n",
    "output_full = output\n",
    "\n",
    "# Add a confirmation print statement\n",
    "print(f\"Full input data shape: {input_full.shape}\")\n",
    "print(f\"Full output data shape: {output_full.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d30f033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#                       define the neural network                     #\n",
    "#######################################################################\n",
    "\n",
    "log_path =os.path.join(output_dir, 'training_logs.txt')\n",
    "os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "class PrintTrainingOnTextEvery10EpochsCallback(Callback):\n",
    "    def __init__(self, log_path):\n",
    "        super().__init__()\n",
    "        self.log_path = log_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 10 == 0:  # Log every 10 epochs\n",
    "            with open(self.log_path, \"a\") as log_file:\n",
    "                log_file.write(\n",
    "                    f\"Epoch: {epoch:>3} | \"\n",
    "                    f\"Loss: {logs.get('loss', 0):.2e} | \"\n",
    "#                        f\"Accuracy: {logs.get('accuracy', 0):.2e} | \"\n",
    "                    f\"Validation loss: {logs.get('val_loss', 0):.2e} |\\n \"\n",
    "#                        f\"Validation accuracy: {logs.get('val_accuracy', 0):.2e}\\n\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"Epoch {epoch:>3} - \"\n",
    "                    f\"Loss: {logs.get('loss', 0):.2e}, \"\n",
    "#                        f\"Accuracy: {logs.get('accuracy', 0):.2e}, \"\n",
    "                    f\"Validation loss: {logs.get('val_loss', 0):.2e}, \"\n",
    "#                        f\"Validation accuracy: {logs.get('val_accuracy', 0):.2e}\"\n",
    "                )\n",
    "\n",
    "my_callbacks = [\n",
    "    PrintTrainingOnTextEvery10EpochsCallback(log_path=log_path),\n",
    "]   \n",
    "     \n",
    "def train_model(X_train, y_train, X_test, y_test, y_mean, y_std, model_save_path):\n",
    "    # ... (your existing initializations for kernels, kernel_acts, hiddens, etc.) ...\n",
    "    # Ensure 'kernels', 'hiddens' are defined.\n",
    "    # 'kernel_acts' and 'activation_function_dense' will now control WHICH activation layer to add.\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(X_train.shape[1:]),)\n",
    "    layers = inputs\n",
    "\n",
    "    # Convolutional Layers\n",
    "    for i, kernel_filters in enumerate(kernels): # Assuming kernels is a list of filter numbers\n",
    "        layers = Conv2D(\n",
    "            kernel_filters,\n",
    "            (kernel_size, kernel_size), # Assuming kernel_size is defined\n",
    "            strides=(stride, stride),     # Assuming stride is defined\n",
    "            use_bias=True,\n",
    "            padding=\"same\",\n",
    "            # NO 'activation' argument here if PReLU or another layer follows\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "        )(layers)\n",
    "\n",
    "        # Apply PReLU (or other chosen activation)\n",
    "        # Assuming kernel_acts[i] could be 'prelu', 'gelu', 'relu', etc.\n",
    "        if kernel_acts[i].lower() == 'prelu':\n",
    "            layers = PReLU(shared_axes=[1, 2],\n",
    "                           alpha_initializer=tf.keras.initializers.Constant(0.25) # Common starting point\n",
    "                          )(layers) # shared_axes for Conv2D with channels_last\n",
    "        elif kernel_acts[i].lower() == 'gelu':\n",
    "            layers = tf.keras.layers.Activation('gelu')(layers) # Using Activation layer for GeLU\n",
    "        elif kernel_acts[i]: # For 'relu', 'elu', etc.\n",
    "            layers = tf.keras.layers.Activation(kernel_acts[i])(layers)\n",
    "        # If kernel_acts[i] is None or an empty string, no explicit activation layer added here.\n",
    "\n",
    "        layers = MaxPooling2D((pool_size, pool_size))(layers) # Assuming pool_size is defined\n",
    "    \n",
    "    layers = Flatten()(layers)\n",
    "\n",
    "    # Dense Layers\n",
    "    for i, hidden_units in enumerate(hiddens): # Assuming hiddens is a list of unit numbers\n",
    "        layers = Dense(\n",
    "            hidden_units,\n",
    "            use_bias=True,\n",
    "            # NO 'activation' argument here\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "        )(layers)\n",
    "\n",
    "        # Apply PReLU (or other chosen activation)\n",
    "        # Assuming activation_function_dense[i] could be 'prelu', 'gelu', 'elu', etc.\n",
    "        if activation_function_dense[i].lower() == 'prelu':\n",
    "            layers = PReLU(alpha_initializer=tf.keras.initializers.Constant(0.25))(layers) # No shared_axes for Dense\n",
    "        elif activation_function_dense[i].lower() == 'gelu':\n",
    "            layers = tf.keras.layers.Activation('gelu')(layers)\n",
    "        elif activation_function_dense[i]: # For 'elu', 'relu', etc.\n",
    "            layers = tf.keras.layers.Activation(activation_function_dense[i])(layers)\n",
    "        # If activation_function_dense[i] is None or an empty string, no explicit activation layer.\n",
    "\n",
    "    output_layer = Dense(\n",
    "        y_train.shape[-1], # Assuming y_train is (samples, features_out) or (samples,)\n",
    "        activation=\"linear\",\n",
    "        use_bias=True,\n",
    "        bias_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "        kernel_initializer=tf.keras.initializers.RandomNormal(seed=rng_seed),\n",
    "    )(layers)\n",
    "    \n",
    "    model = Model(inputs, output_layer)\n",
    "\n",
    "    # ... (rest of your model compilation, summary, plotting, training, saving) ...\n",
    "    # Example:\n",
    "    if fold_no + ens_no == 2: # Assuming these are defined in your script's scope\n",
    "        model.summary()\n",
    "        # ... (your summary saving and plot_model code) ...\n",
    "\n",
    "    model.compile(loss=loss_function, optimizer=Adam(learning_rate=learning_rate)) # Assuming these are defined\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='auto', restore_best_weights=True, verbose=1) # restore_best_weights=1 is True\n",
    "\n",
    "    # Make sure my_callbacks is defined in this scope or passed as an argument\n",
    "    history = model.fit(X_train, y_train, epochs=epoch_max, batch_size=batch_size,\n",
    "                        validation_data=(X_test, y_test), callbacks=[early_stopping] + my_callbacks, verbose=0) # Added '+' for list concatenation\n",
    "    history_path = model_save_path.replace('.h5', '_history.npy')\n",
    "    np.save(history_path, history.history)\n",
    "    skill = model.evaluate(X_test, y_test, verbose=0)\n",
    "    pred = (model.predict(X_test) * y_std) + y_mean # Make sure y_std and y_mean are defined\n",
    "\n",
    "    R2_val = []\n",
    "    truth = (y_test * y_std) + y_mean\n",
    "    for latind in range(truth.shape[1] if truth.ndim > 1 else 1): # Handle if y_test is 1D output\n",
    "        y_lat = truth[:, latind] if truth.ndim > 1 else truth\n",
    "        y_pred_lat = pred[:, latind] if pred.ndim > 1 else pred\n",
    "        R2_val.append(r2_score(y_lat, y_pred_lat))\n",
    "    \n",
    "    model.save(model_save_path)\n",
    "    # Use the model_save_path to create matching file names\n",
    "    pred_path = model_save_path.replace('.h5', '_pred.npy')\n",
    "    truth_path = model_save_path.replace('.h5', '_truth.npy')\n",
    "    np.save(pred_path, pred)\n",
    "    np.save(truth_path, truth)\n",
    "    \n",
    "    return skill, history, pred, R2_val\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff17f61",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING EXPERIMENT FOR 1 TOTAL SEGMENT(S)\n",
      "============================================================\n",
      "--- Training on Segment #1 of 1 ---\n",
      "   Data for this run has shape: (1801, 192, 288)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 05:52:50,355 - ------------------------------------------------------------------------\n",
      "2025-07-03 05:52:50,356 - Training for fold 1 ensemble 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 192, 288, 1)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 192, 288, 32)      320       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 192, 288, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 96, 144, 32)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 96, 144, 32)       9248      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 96, 144, 32)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 48, 72, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 110592)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                1769488   \n",
      "                                                                 \n",
      " p_re_lu (PReLU)             (None, 16)                16        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " p_re_lu_1 (PReLU)           (None, 8)                 8         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,779,225\n",
      "Trainable params: 1,779,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 05:52:50.731317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31088 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
      "2025-07-03 05:52:50.753458: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2025-07-03 05:52:52.297852: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8201\n",
      "2025-07-03 05:52:53.890042: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-07-03 05:52:53.890835: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-07-03 05:52:53.890852: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2025-07-03 05:52:53.891247: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-07-03 05:52:53.891296: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 - Loss: 9.73e-01, Validation loss: 1.06e+00, \n",
      "Epoch  10 - Loss: 6.54e-01, Validation loss: 7.42e-01, \n",
      "Epoch  20 - Loss: 4.04e-01, Validation loss: 4.87e-01, \n",
      "Epoch  30 - Loss: 3.13e-01, Validation loss: 3.94e-01, \n",
      "Epoch  40 - Loss: 2.58e-01, Validation loss: 3.50e-01, \n",
      "Epoch  50 - Loss: 2.22e-01, Validation loss: 3.18e-01, \n",
      "Epoch  60 - Loss: 1.99e-01, Validation loss: 3.00e-01, \n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "#                             Train loop                              #\n",
    "#######################################################################\n",
    "# Master loop for iterating through different interval divisions (e.g., 1, 2, 4, 10, 20)\n",
    "for total_segments in INTERVAL_DIVISIONS:\n",
    "    print(f\"============================================================\")\n",
    "    print(f\"STARTING EXPERIMENT FOR {total_segments} TOTAL SEGMENT(S)\")\n",
    "    print(f\"============================================================\")\n",
    "    \n",
    "    all_model_paths[total_segments] = []\n",
    "    segment_length = input_full.shape[0] // total_segments\n",
    "\n",
    "    # Loop for each individual segment within a division\n",
    "    for segment_num in range(1, total_segments + 1):\n",
    "        print(f\"--- Training on Segment #{segment_num} of {total_segments} ---\")\n",
    "\n",
    "        # 1. SLICE THE DATA for this specific experiment\n",
    "        start_index = (segment_num - 1) * segment_length\n",
    "        end_index = segment_num * segment_length\n",
    "        \n",
    "        X = input_full[start_index:end_index] # This now defines X for the K-fold loop\n",
    "        y = output_full[start_index:end_index] # This now defines y for the K-fold loop\n",
    "        \n",
    "        print(f\"   Data for this run has shape: {X.shape}\")\n",
    "    \n",
    "        X_mean = np.mean(X, axis=0)\n",
    "        X_std = np.std(X, axis=0)\n",
    "        \n",
    "        y_mean = np.mean(y, axis=0)\n",
    "        y_std = np.std(y, axis=0)\n",
    "        \n",
    "        if remove_mean ==0:\n",
    "            X_mean = 0\n",
    "            y_mean = 0            \n",
    "        if divide_std ==0:\n",
    "            X_std = 1\n",
    "            y_std = 1\n",
    "        else:\n",
    "            X_std = X_std[:,:, tf.newaxis]           \n",
    "        sio.savemat(os.path.join(output_dir,'Normalization.mat'), \n",
    "                    {'X_mean': X_mean, 'X_std': X_std,'y_mean':y_mean,'y_std':y_std})        \n",
    "        # if not os.path.exists(os.path.join(output_dir, 'inputs_info.mat')):\n",
    "\n",
    "        trained_models = []\n",
    "        y_pred_reconstructed_allfolds = []\n",
    "           \n",
    "        # Train the neural network multiple times using k-fold cross validation\n",
    "        # Define the K-fold Cross Validator\n",
    "        kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "        fold_no = 1\n",
    "        for trainind, testind in kfold.split(X, y):   \n",
    "            # break\n",
    "            X_train = X[trainind,:,:, tf.newaxis]\n",
    "            y_train = y[trainind,:]\n",
    "            X_test = X[testind,:, :,tf.newaxis]\n",
    "            y_test = y[testind,:]\n",
    "         \n",
    "            # Normalize the input and out data based on the information from the entire PI control run       \n",
    "            X_train = (X_train - X_mean)/X_std\n",
    "            X_test = (X_test - X_mean)/X_std\n",
    "            \n",
    "            y_train = (y_train - y_mean)/y_std\n",
    "            y_test = (y_test - y_mean)/y_std\n",
    "            \n",
    "            # Generate a print\n",
    "            logger.info('------------------------------------------------------------------------')\n",
    "            # We use ensemble training for each fold of the cross-validation\n",
    "            # Create the full, unique path for the model\n",
    "            for ens_no  in np.arange(1,NNrepeats+1):\n",
    "                model_name = f\"model_total_{total_segments}_segs_segment_{segment_num}_fold_{fold_no}_ens_{ens_no}.h5\"\n",
    "                model_path = os.path.join(OUTPUT_MODEL_DIR, model_name)\n",
    "                logger.info(f'Training for fold {fold_no} ensemble {ens_no}...')\n",
    "                # Pass the new path to the function\n",
    "                skill, history, pred, R2_val = train_model(X_train, y_train, X_test, y_test, y_mean, y_std, model_path)\n",
    "\n",
    "                # Also add the path to our dictionary for later\n",
    "                all_model_paths[total_segments].append(model_path)          \n",
    "            # Increase fold number\n",
    "            fold_no = fold_no + 1\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()      \n",
    "            \n",
    "logger.info('------------------------------------------------------------------------')\n",
    "logger.info(f'Training with {num_folds}-fold cross-validation finished!') \n",
    "logger.info('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e12b1a-4c08-4287-bdc0-bffc8d091e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#                                                                     #\n",
    "#                  SUMMARY PLOTTING AND ANALYSIS(Loss Curve)          #\n",
    "#                                                                     #\n",
    "#######################################################################\n",
    "\n",
    "print(\"--- Starting Summary Plot Generation ---\")\n",
    "\n",
    "# Loop through each of the interval configurations that we tested.\n",
    "# e.g., total_segments will be 1, then 2, then 4, etc.\n",
    "for total_segments in INTERVAL_DIVISIONS:\n",
    "    \n",
    "    # --- 1. Setup the Plot ---\n",
    "    # Create a new figure for this specific interval size.\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get the list of all model paths that were trained for this configuration.\n",
    "    # We retrieve this from the dictionary we populated during training.\n",
    "    model_paths_for_interval = all_model_paths.get(total_segments, [])\n",
    "    \n",
    "    if not model_paths_for_interval:\n",
    "        print(f\"No models found for {total_segments} segments. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # This list will hold the validation loss history from each model run\n",
    "    all_val_losses = []\n",
    "    shortest_epoch_count = float('inf') # We need to find the shortest training run\n",
    "\n",
    "    # --- 2. Plot Individual Loss Curves ---\n",
    "    # Loop through each model file path for the current interval size.\n",
    "    for model_path in model_paths_for_interval:\n",
    "        # Construct the path to the corresponding history file\n",
    "        history_path = model_path.replace('.h5', '_history.npy')\n",
    "        \n",
    "        try:\n",
    "            # Load the history dictionary from the .npy file\n",
    "            # allow_pickle=True is required to load a dictionary object.\n",
    "            # .item() extracts the dictionary from the numpy array wrapper.\n",
    "            history = np.load(history_path, allow_pickle=True).item()\n",
    "            \n",
    "            # Get the validation loss from the history\n",
    "            val_loss = history['val_loss']\n",
    "            \n",
    "            # Plot this model's validation loss curve.\n",
    "            # We use a low alpha to make it semi-transparent. This helps see density.\n",
    "            plt.plot(val_loss, color='dodgerblue', alpha=0.3)\n",
    "            \n",
    "            # Store the validation loss history for calculating the median later\n",
    "            all_val_losses.append(val_loss)\n",
    "            \n",
    "            # Keep track of the length of the shortest training run.\n",
    "            # This is necessary because EarlyStopping makes each run have a different length.\n",
    "            if len(val_loss) < shortest_epoch_count:\n",
    "                shortest_epoch_count = len(val_loss)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Could not find history file: {history_path}\")\n",
    "            continue\n",
    "\n",
    "    # --- 3. Plot the Median Loss Curve ---\n",
    "    # This gives us a robust \"average\" of all the training runs.\n",
    "    if all_val_losses:\n",
    "        # Create a new list where all loss histories are truncated to the same length\n",
    "        truncated_losses = [loss[:shortest_epoch_count] for loss in all_val_losses]\n",
    "        \n",
    "        # Convert the list of lists into a 2D NumPy array\n",
    "        loss_matrix = np.array(truncated_losses)\n",
    "        \n",
    "        # Calculate the median loss at each epoch across all runs\n",
    "        median_loss_curve = np.median(loss_matrix, axis=0)\n",
    "        \n",
    "        # Plot the median curve with a thick, solid, contrasting line to make it stand out\n",
    "        plt.plot(median_loss_curve, color='crimson', linewidth=3, label=f'Median Validation Loss ({len(all_val_losses)} runs)')\n",
    "\n",
    "    # --- 4. Finalize and Save the Plot ---\n",
    "    plt.yscale('log') # Use a logarithmic scale for the y-axis to better see changes\n",
    "    plt.title(f'Validation Loss Curves for {total_segments} Data Segment(s)', fontsize=16)\n",
    "    plt.xlabel('Epochs', fontsize=12)\n",
    "    plt.ylabel('Validation Loss (Log Scale)', fontsize=12)\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Create a descriptive filename for the plot\n",
    "    plot_filename = f'summary_loss_curve_total_{total_segments}_segments.png'\n",
    "    # Save the figure to the same directory where the models are stored\n",
    "    plt.savefig(os.path.join(OUTPUT_MODEL_DIR, plot_filename), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Display the plot in the notebook\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n--- All Summary Plots Have Been Generated. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96965843-8e85-4aa7-8204-85241564dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#                                                                     #\n",
    "#                  SUMMARY SCATTER PLOT GENERATION(Scatter plot)      #\n",
    "#                                                                     #\n",
    "#######################################################################\n",
    "\n",
    "print(\"--- Starting Summary Scatter Plot Generation ---\")\n",
    "\n",
    "# Loop through each of the interval configurations (e.g., 1, 2, 4, 10, 20)\n",
    "for total_segments in INTERVAL_DIVISIONS:\n",
    "    \n",
    "    # These lists will collect the data from all folds and ensembles for this interval size\n",
    "    all_predictions = []\n",
    "    all_truths = []\n",
    "    \n",
    "    print(f\"\\n--- Generating plot for {total_segments} segment(s) ---\")\n",
    "    \n",
    "    # Now, we reconstruct the file paths to load the data for each fold\n",
    "    for segment_num in range(1, total_segments + 1):\n",
    "        for fold_no in range(1, num_folds + 1):\n",
    "            for ens_no in range(1, NNrepeats + 1):\n",
    "                # Recreate the unique model name to find the corresponding data files\n",
    "                model_name = f\"model_total_{total_segments}_segs_segment_{segment_num}_fold_{fold_no}_ens_{ens_no}.h5\"\n",
    "                model_path = os.path.join(OUTPUT_MODEL_DIR, model_name)\n",
    "                \n",
    "                pred_path = model_path.replace('.h5', '_pred.npy')\n",
    "                truth_path = model_path.replace('.h5', '_truth.npy')\n",
    "                \n",
    "                try:\n",
    "                    # Load the saved prediction and truth arrays\n",
    "                    pred = np.load(pred_path)\n",
    "                    truth = np.load(truth_path)\n",
    "                    \n",
    "                    # Add the loaded data to our master lists\n",
    "                    all_predictions.append(pred)\n",
    "                    all_truths.append(truth)\n",
    "\n",
    "                except FileNotFoundError:\n",
    "                    # This handles cases where a specific fold/ensemble might not have run\n",
    "                    print(f\"Warning: Could not find data for {model_name}\")\n",
    "                    continue\n",
    "\n",
    "    if not all_predictions:\n",
    "        print(\"No data found to plot for this interval.\")\n",
    "        continue\n",
    "        \n",
    "    # --- Data Aggregation and Plotting ---\n",
    "    \n",
    "    # Concatenate all the arrays in the lists into single large arrays\n",
    "    final_predictions = np.concatenate(all_predictions)\n",
    "    final_truths = np.concatenate(all_truths)\n",
    "\n",
    "    # --- Plotting - Styled like your example ---\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # Scatter plot of the data points\n",
    "    plt.plot(final_predictions, final_truths, 'o', markersize=5, alpha=0.6, label=\"Data Points\")\n",
    "\n",
    "    # Determine plot limits to make the plot square\n",
    "    min_val = min(np.min(final_predictions), np.min(final_truths))\n",
    "    max_val = max(np.max(final_predictions), np.max(final_truths))\n",
    "    plt.xlim(min_val, max_val)\n",
    "    plt.ylim(min_val, max_val)\n",
    "    \n",
    "    # 1:1 Reference Line\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'k--', linewidth=2, label=\"1:1 Reference\")\n",
    "    \n",
    "    # Calculate and display the overall R² score\n",
    "    r2 = r2_score(final_truths, final_predictions)\n",
    "    plt.text(0.05, 0.9, f\"$R^2$: {r2:.3f}\", transform=plt.gca().transAxes,\n",
    "             fontsize=14, bbox=dict(facecolor=\"white\", alpha=0.8, edgecolor=\"black\"))\n",
    "\n",
    "    # Labels, title, and formatting\n",
    "    plt.xlabel(\"Prediction\", fontsize=16, fontweight='bold')\n",
    "    plt.ylabel(\"Truth\", fontsize=16, fontweight='bold')\n",
    "    plt.title(f'Truth vs. Prediction for {total_segments} Data Segment(s)', fontsize=16)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.legend(fontsize=12, loc=\"lower right\")\n",
    "    plt.axis(\"equal\") # Ensure the plot is square\n",
    "    \n",
    "    # Save and show the plot\n",
    "    plot_filename = f'summary_scatter_total_{total_segments}_segments.png'\n",
    "    plt.savefig(os.path.join(OUTPUT_MODEL_DIR, plot_filename), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n--- All Summary Scatter Plots Have Been Generated. ---\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "tf210",
   "language": "python",
   "name": "tf210"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
