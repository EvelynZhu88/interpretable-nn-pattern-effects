{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbba3047-f803-4acb-a677-8feba2a8497d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 02:57:31.315361: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-06 02:57:31.421680: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-06 02:57:31.454552: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up for Zonal-Mean Out-of-Sample Test ---\n",
      "Loading normalization data...\n",
      "✅ Normalization data loaded successfully.\n",
      "[[[220.6603 ]\n",
      "  [220.86879]\n",
      "  [220.88463]\n",
      "  ...\n",
      "  [220.85564]\n",
      "  [220.6545 ]\n",
      "  [220.89789]]\n",
      "\n",
      " [[221.35854]\n",
      "  [221.40932]\n",
      "  [221.33986]\n",
      "  ...\n",
      "  [221.18336]\n",
      "  [221.2454 ]\n",
      "  [221.24274]]\n",
      "\n",
      " [[221.73988]\n",
      "  [221.76971]\n",
      "  [221.81885]\n",
      "  ...\n",
      "  [221.57878]\n",
      "  [221.65894]\n",
      "  [221.67375]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[250.342  ]\n",
      "  [250.36276]\n",
      "  [250.38281]\n",
      "  ...\n",
      "  [250.28914]\n",
      "  [250.30632]\n",
      "  [250.32236]]\n",
      "\n",
      " [[250.1685 ]\n",
      "  [250.17897]\n",
      "  [250.18936]\n",
      "  ...\n",
      "  [250.13574]\n",
      "  [250.14691]\n",
      "  [250.15811]]\n",
      "\n",
      " [[250.01157]\n",
      "  [250.01212]\n",
      "  [250.01262]\n",
      "  ...\n",
      "  [250.0096 ]\n",
      "  [250.0104 ]\n",
      "  [250.01102]]]\n",
      "[[[0.53236264]\n",
      "  [0.5272247 ]\n",
      "  [0.52714396]\n",
      "  ...\n",
      "  [0.52715623]\n",
      "  [0.53207755]\n",
      "  [0.52702093]]\n",
      "\n",
      " [[0.5789421 ]\n",
      "  [0.57852644]\n",
      "  [0.58601373]\n",
      "  ...\n",
      "  [0.57942045]\n",
      "  [0.58237875]\n",
      "  [0.58337593]]\n",
      "\n",
      " [[0.6280233 ]\n",
      "  [0.61705726]\n",
      "  [0.6132673 ]\n",
      "  ...\n",
      "  [0.62475264]\n",
      "  [0.6240839 ]\n",
      "  [0.61795306]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1.0660723 ]\n",
      "  [1.068216  ]\n",
      "  [1.070278  ]\n",
      "  ...\n",
      "  [1.0605257 ]\n",
      "  [1.0623667 ]\n",
      "  [1.0641106 ]]\n",
      "\n",
      " [[1.0350556 ]\n",
      "  [1.0358478 ]\n",
      "  [1.0366572 ]\n",
      "  ...\n",
      "  [1.0323557 ]\n",
      "  [1.0332991 ]\n",
      "  [1.0342047 ]]\n",
      "\n",
      " [[1.0114971 ]\n",
      "  [1.0116093 ]\n",
      "  [1.0117126 ]\n",
      "  ...\n",
      "  [1.0110854 ]\n",
      "  [1.0112362 ]\n",
      "  [1.0113711 ]]]\n",
      "[[-4.96990870e-05 -4.15147497e-06 -3.53764990e-05  3.88544177e-05\n",
      "   9.47087028e-05  4.36328492e-07 -2.07785561e-05 -6.62456805e-05\n",
      "   1.86816378e-05 -9.91270554e-07 -1.61949894e-04  3.47368314e-07\n",
      "  -5.66168019e-05  1.01033351e-04  4.82503083e-06 -4.58653267e-05\n",
      "   1.50808683e-05  5.35328472e-05  2.08802248e-05 -3.92441470e-05\n",
      "   7.43156415e-05  5.30965190e-05  9.59541430e-05 -7.32735352e-05\n",
      "  -6.42504310e-05 -3.01024302e-05 -5.29906138e-05  1.22807414e-05\n",
      "   3.33558310e-05  8.03564617e-05 -5.56212945e-06 -1.53096244e-05\n",
      "  -4.54967776e-05 -3.75666132e-05  2.19604553e-05  1.81542309e-05\n",
      "   2.24836258e-05 -4.84070479e-05 -8.26694213e-06  8.69903488e-06\n",
      "   1.78343980e-05 -1.07705364e-05 -2.11682873e-05 -7.98947167e-06\n",
      "   2.38127341e-05  1.27096564e-05  1.92312855e-05  0.00000000e+00\n",
      "  -2.74293893e-06 -2.77894651e-06  1.04951832e-05  1.24618382e-05\n",
      "   3.40007932e-06  1.14112618e-06  4.41901739e-06  2.94766642e-06\n",
      "   3.67291705e-07  1.35220796e-06 -4.02703677e-07 -8.63655089e-07\n",
      "   4.41835573e-06 -1.02658987e-05 -8.02071372e-06 -1.14933373e-05\n",
      "   2.54224892e-06  2.69739985e-06  1.29415885e-05  3.10979376e-05\n",
      "  -7.46947808e-06  2.72546458e-05 -3.54569852e-06 -1.36860999e-05\n",
      "   3.27458179e-06  1.93806113e-06  9.16501631e-06  1.21833082e-05\n",
      "   2.95135997e-05  1.04824749e-05 -1.99397891e-05 -1.73472345e-05\n",
      "   2.22781709e-05 -1.79064136e-05  5.35794461e-05  4.17625670e-05\n",
      "   2.79673859e-05  9.47214085e-06  3.31906199e-06 -1.53265682e-05\n",
      "  -3.56327873e-05  2.47923545e-05 -3.53743817e-05 -3.13965902e-05\n",
      "  -2.10030757e-05 -3.15723919e-05  3.41014015e-06  2.86303512e-05\n",
      "  -2.94076945e-05  2.59297740e-05  1.03003176e-05  5.46469664e-07\n",
      "   7.62727632e-06  1.69426785e-05 -3.82380495e-05 -1.40429995e-06\n",
      "   2.23247685e-06  2.67092346e-05 -3.30783587e-05 -4.91822721e-06\n",
      "   1.23697009e-06  1.83639224e-06 -1.72625107e-06 -1.10755427e-05\n",
      "   9.54627467e-06  2.26255397e-05 -8.96485653e-06 -2.73647875e-05\n",
      "   1.08827953e-05  2.17846537e-06 -9.02310421e-06  1.29129940e-05\n",
      "  -1.87981334e-06  1.01928244e-05  4.52584936e-06  7.21212882e-06\n",
      "  -7.37416349e-06 -2.22771109e-06 -1.21766898e-05  2.68045505e-06\n",
      "   3.14008247e-07 -7.88118359e-06 -1.05004790e-06  9.35008575e-06\n",
      "   4.39187943e-06  3.94515882e-06  2.69064827e-07  1.90728244e-07\n",
      "   1.36504900e-06 -2.97725364e-07 -3.76836374e-06 -2.41728117e-06\n",
      "   6.00110570e-06  7.43982446e-06  1.75855221e-05 -1.57226532e-05\n",
      "   3.30635339e-06  1.45937056e-05  2.39874771e-05  2.58143373e-05\n",
      "  -2.66096849e-05 -1.51846552e-05 -1.26662349e-06  4.10466491e-05\n",
      "   2.18587866e-06  5.07454279e-05  2.60907509e-05  1.99736792e-06\n",
      "  -4.11906803e-05 -3.44868968e-05 -1.08086615e-05 -7.02298275e-05\n",
      "  -1.63051300e-05 -2.86833038e-05 -4.40988333e-05 -7.02446559e-05\n",
      "  -3.22078231e-05  3.94008894e-05 -7.38962553e-05 -3.14283607e-05\n",
      "   1.28441552e-05  8.72784076e-05  2.62263093e-05  1.94314453e-05\n",
      "  -2.34897234e-05  9.71868794e-05  1.50639235e-05 -1.16567491e-04\n",
      "   7.77935566e-05  1.23353879e-04 -4.60940828e-05  1.39858108e-04\n",
      "   1.20028468e-04  1.32093162e-04 -4.55942100e-05  4.71700769e-05\n",
      "  -9.84323196e-05 -4.95465829e-05  4.68947255e-05 -4.42428645e-05\n",
      "  -6.27253976e-05 -5.63626272e-05 -3.77445322e-05  4.98388836e-05]]\n",
      "[[1.0973109  1.0824621  1.0563043  1.0395298  1.007003   0.99404824\n",
      "  0.9929977  1.0172836  1.0252074  1.02295    1.0039859  0.97871083\n",
      "  0.9603413  1.0915927  1.155512   1.1342509  1.1117208  1.0810401\n",
      "  1.0350811  1.0410167  1.0657034  1.1423917  1.2854011  1.2903559\n",
      "  1.3984332  1.6196867  1.806462   1.5292431  1.1787045  1.1541508\n",
      "  1.2048951  1.1369466  1.0927495  1.1177882  1.1171037  0.98477334\n",
      "  0.8034918  0.706149   0.6769369  0.70219606 0.7353815  0.7688084\n",
      "  0.820716   0.8714903  0.91958934 0.9573134  0.9864344  1.0093541\n",
      "  1.0123469  0.99358416 0.97180814 0.94098413 0.90709233 0.8668251\n",
      "  0.83395773 0.8132406  0.78913367 0.7626166  0.7426297  0.74496555\n",
      "  0.74298626 0.7314164  0.7232275  0.7189779  0.7165103  0.7151861\n",
      "  0.71454567 0.7114773  0.7043037  0.6980428  0.69583356 0.6917958\n",
      "  0.6992176  0.724615   0.7551604  0.8000969  0.83987355 0.87118167\n",
      "  0.89374    0.9100166  0.9242894  0.89606524 0.86030173 0.8166181\n",
      "  0.7924389  0.72929597 0.66933954 0.62764305 0.63467175 0.6286943\n",
      "  0.7205463  0.8064955  0.90534383 0.9542627  1.0034527  1.041581\n",
      "  1.0801077  1.1341037  1.2127336  1.1899427  1.0368385  0.94627887\n",
      "  1.0071045  1.1476067  1.3172494  1.3353314  1.2565435  1.1766863\n",
      "  1.1373677  1.0733236  1.0298827  0.9509775  0.86813295 0.78662366\n",
      "  0.74326605 0.70152354 0.67734367 0.67548066 0.67746574 0.6819068\n",
      "  0.68443745 0.6976299  0.7105197  0.7344394  0.77668804 0.80349153\n",
      "  0.78380406 0.7544863  0.7411168  0.729058   0.7278986  0.7244216\n",
      "  0.74427086 0.7213294  0.6773121  0.64191896 0.65842545 0.64720404\n",
      "  0.64053833 0.65743273 0.66968906 0.6609214  0.6463825  0.63683903\n",
      "  0.6432292  0.65011126 0.6441236  0.6491223  0.65659845 0.6591661\n",
      "  0.6476441  0.6301889  0.6286639  0.64086986 0.6597247  0.6719179\n",
      "  0.6963264  0.7256853  0.7732042  0.7804626  0.75709724 0.7826509\n",
      "  0.8210744  0.8429399  0.88934094 0.92208207 0.99939954 1.0540527\n",
      "  1.0749724  1.1053805  1.204717   1.2860148  1.324984   1.3262571\n",
      "  1.3552252  1.3557556  1.4321339  1.4128596  1.3532271  1.3016504\n",
      "  1.3148628  1.3319511  1.3814138  1.4226357  1.4716313  1.5182605\n",
      "  1.578276   1.6483326  1.7207509  1.799886   1.884692   1.9520644 ]]\n",
      "\n",
      "Loading and preprocessing 4xCO2 test data...\n",
      "✅ Test data preprocessed.\n",
      "\n",
      "--- Running Ensemble Predictions for 4xCO2 ---\n",
      "    Loading and predicting with model: /ocean/projects/ees250004p/ezhu3/data/CESM1/trained_model_2D_changedLRandKS/model_fold1_ens1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 02:57:34.666864: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-06 02:57:35.099741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 469 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:8a:00.0, compute capability: 7.0\n",
      "2025-08-06 02:57:45.907360: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 216.00MiB (rounded to 226492416)requested by op model/conv2d/Conv2D\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-08-06 02:57:45.907399: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] BFCAllocator dump for GPU_0_bfc\n",
      "2025-08-06 02:57:45.907409: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (256): \tTotal Chunks: 32, Chunks in use: 32. 8.0KiB allocated for chunks. 8.0KiB in use in bin. 892B client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907429: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (512): \tTotal Chunks: 17, Chunks in use: 17. 9.2KiB allocated for chunks. 9.2KiB in use in bin. 9.2KiB client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907433: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1024): \tTotal Chunks: 4, Chunks in use: 4. 5.0KiB allocated for chunks. 5.0KiB in use in bin. 4.4KiB client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907438: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907448: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907458: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907462: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907467: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (32768): \tTotal Chunks: 3, Chunks in use: 3. 132.2KiB allocated for chunks. 132.2KiB in use in bin. 108.0KiB client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907472: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (65536): \tTotal Chunks: 6, Chunks in use: 6. 577.5KiB allocated for chunks. 577.5KiB in use in bin. 480.0KiB client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907475: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907479: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907482: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907486: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907489: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907493: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4194304): \tTotal Chunks: 1, Chunks in use: 1. 6.75MiB allocated for chunks. 6.75MiB in use in bin. 6.75MiB client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907496: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907501: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16777216): \tTotal Chunks: 1, Chunks in use: 1. 31.64MiB allocated for chunks. 31.64MiB in use in bin. 31.64MiB client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907507: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (33554432): \tTotal Chunks: 2, Chunks in use: 2. 108.00MiB allocated for chunks. 108.00MiB in use in bin. 108.00MiB client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907512: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (67108864): \tTotal Chunks: 2, Chunks in use: 1. 177.24MiB allocated for chunks. 107.59MiB in use in bin. 54.00MiB client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907516: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 0. 145.09MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907520: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 02:57:45.907526: I tensorflow/core/common_runtime/bfc_allocator.cc:1056] Bin for 216.00MiB was 128.00MiB, Chunk State: \n",
      "2025-08-06 02:57:45.907534: I tensorflow/core/common_runtime/bfc_allocator.cc:1062]   Size: 145.09MiB | Requested Size: 128B | in_use: 0 | bin_num: 19, prev:   Size: 54.00MiB | Requested Size: 54.00MiB | in_use: 1 | bin_num: -1\n",
      "2025-08-06 02:57:45.907537: I tensorflow/core/common_runtime/bfc_allocator.cc:1069] Next region of size 492240896\n",
      "2025-08-06 02:57:45.907544: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a000000 of size 1280 next 1\n",
      "2025-08-06 02:57:45.907548: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a000500 of size 256 next 2\n",
      "2025-08-06 02:57:45.907551: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a000600 of size 256 next 3\n",
      "2025-08-06 02:57:45.907554: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a000700 of size 256 next 5\n",
      "2025-08-06 02:57:45.907556: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a000800 of size 256 next 6\n",
      "2025-08-06 02:57:45.907559: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a000900 of size 256 next 4\n",
      "2025-08-06 02:57:45.907562: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a000a00 of size 256 next 7\n",
      "2025-08-06 02:57:45.907565: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a000b00 of size 256 next 12\n",
      "2025-08-06 02:57:45.907568: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a000c00 of size 256 next 10\n",
      "2025-08-06 02:57:45.907571: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a000d00 of size 256 next 11\n",
      "2025-08-06 02:57:45.907574: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a000e00 of size 512 next 17\n",
      "2025-08-06 02:57:45.907577: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a001000 of size 512 next 15\n",
      "2025-08-06 02:57:45.907580: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a001200 of size 256 next 16\n",
      "2025-08-06 02:57:45.907582: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a001300 of size 256 next 8\n",
      "2025-08-06 02:57:45.907585: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a001400 of size 512 next 18\n",
      "2025-08-06 02:57:45.907589: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a001600 of size 111360 next 13\n",
      "2025-08-06 02:57:45.907592: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a01c900 of size 512 next 21\n",
      "2025-08-06 02:57:45.907594: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a01cb00 of size 256 next 14\n",
      "2025-08-06 02:57:45.907597: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a01cc00 of size 256 next 20\n",
      "2025-08-06 02:57:45.907600: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a01cd00 of size 256 next 22\n",
      "2025-08-06 02:57:45.907603: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a01ce00 of size 256 next 25\n",
      "2025-08-06 02:57:45.907606: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a01cf00 of size 256 next 31\n",
      "2025-08-06 02:57:45.907609: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a01d000 of size 256 next 32\n",
      "2025-08-06 02:57:45.907612: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a01d100 of size 256 next 28\n",
      "2025-08-06 02:57:45.907615: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a01d200 of size 256 next 26\n",
      "2025-08-06 02:57:45.907618: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a01d300 of size 1280 next 27\n",
      "2025-08-06 02:57:45.907620: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a01d800 of size 256 next 9\n",
      "2025-08-06 02:57:45.907623: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a01d900 of size 61696 next 23\n",
      "2025-08-06 02:57:45.907626: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a02ca00 of size 98304 next 24\n",
      "2025-08-06 02:57:45.907629: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a044a00 of size 768 next 30\n",
      "2025-08-06 02:57:45.907632: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a044d00 of size 256 next 33\n",
      "2025-08-06 02:57:45.907635: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a044e00 of size 256 next 34\n",
      "2025-08-06 02:57:45.907638: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a044f00 of size 256 next 35\n",
      "2025-08-06 02:57:45.907641: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a045000 of size 256 next 37\n",
      "2025-08-06 02:57:45.907643: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a045100 of size 512 next 61\n",
      "2025-08-06 02:57:45.907646: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a045300 of size 512 next 39\n",
      "2025-08-06 02:57:45.907649: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a045500 of size 512 next 42\n",
      "2025-08-06 02:57:45.907652: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a045700 of size 512 next 43\n",
      "2025-08-06 02:57:45.907654: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a045900 of size 103936 next 45\n",
      "2025-08-06 02:57:45.907657: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a05ef00 of size 768 next 44\n",
      "2025-08-06 02:57:45.907660: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a05f200 of size 1280 next 40\n",
      "2025-08-06 02:57:45.907663: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a05f700 of size 256 next 46\n",
      "2025-08-06 02:57:45.907666: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a05f800 of size 36864 next 47\n",
      "2025-08-06 02:57:45.907669: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a068800 of size 256 next 48\n",
      "2025-08-06 02:57:45.907672: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a068900 of size 512 next 50\n",
      "2025-08-06 02:57:45.907674: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a068b00 of size 512 next 51\n",
      "2025-08-06 02:57:45.907677: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a068d00 of size 113920 next 29\n",
      "2025-08-06 02:57:45.907680: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d22a084a00 of size 112819968 next 19\n",
      "2025-08-06 02:57:45.907683: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d230c1c900 of size 65536 next 41\n",
      "2025-08-06 02:57:45.907686: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d230c2c900 of size 512 next 53\n",
      "2025-08-06 02:57:45.907689: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d230c2cb00 of size 512 next 54\n",
      "2025-08-06 02:57:45.907692: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d230c2cd00 of size 98304 next 55\n",
      "2025-08-06 02:57:45.907696: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d230c44d00 of size 768 next 56\n",
      "2025-08-06 02:57:45.907698: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d230c45000 of size 33177600 next 57\n",
      "2025-08-06 02:57:45.907702: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d232be9000 of size 256 next 52\n",
      "2025-08-06 02:57:45.907705: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d232be9100 of size 256 next 62\n",
      "2025-08-06 02:57:45.907707: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d232be9200 of size 256 next 63\n",
      "2025-08-06 02:57:45.907710: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d232be9300 of size 256 next 64\n",
      "2025-08-06 02:57:45.907713: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d232be9400 of size 256 next 65\n",
      "2025-08-06 02:57:45.907716: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d232be9500 of size 512 next 66\n",
      "2025-08-06 02:57:45.907719: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d232be9700 of size 512 next 67\n",
      "2025-08-06 02:57:45.907721: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d232be9900 of size 7077888 next 68\n",
      "2025-08-06 02:57:45.907724: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 14d2332a9900 of size 73028864 next 58\n",
      "2025-08-06 02:57:45.907727: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d23784ee00 of size 256 next 59\n",
      "2025-08-06 02:57:45.907730: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d23784ef00 of size 1280 next 60\n",
      "2025-08-06 02:57:45.907733: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d23784f400 of size 36864 next 36\n",
      "2025-08-06 02:57:45.907736: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d237858400 of size 56623104 next 38\n",
      "2025-08-06 02:57:45.907739: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14d23ae58400 of size 56623104 next 49\n",
      "2025-08-06 02:57:45.907742: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 14d23e458400 of size 152140800 next 18446744073709551615\n",
      "2025-08-06 02:57:45.907744: I tensorflow/core/common_runtime/bfc_allocator.cc:1094]      Summary of in-use Chunks by size: \n",
      "2025-08-06 02:57:45.907749: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 32 Chunks of size 256 totalling 8.0KiB\n",
      "2025-08-06 02:57:45.907753: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 14 Chunks of size 512 totalling 7.0KiB\n",
      "2025-08-06 02:57:45.907756: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 3 Chunks of size 768 totalling 2.2KiB\n",
      "2025-08-06 02:57:45.907759: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 4 Chunks of size 1280 totalling 5.0KiB\n",
      "2025-08-06 02:57:45.907762: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 36864 totalling 72.0KiB\n",
      "2025-08-06 02:57:45.907766: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 61696 totalling 60.2KiB\n",
      "2025-08-06 02:57:45.907769: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 65536 totalling 64.0KiB\n",
      "2025-08-06 02:57:45.907772: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 98304 totalling 192.0KiB\n",
      "2025-08-06 02:57:45.907776: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 103936 totalling 101.5KiB\n",
      "2025-08-06 02:57:45.907779: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 111360 totalling 108.8KiB\n",
      "2025-08-06 02:57:45.907782: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 113920 totalling 111.2KiB\n",
      "2025-08-06 02:57:45.907786: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 7077888 totalling 6.75MiB\n",
      "2025-08-06 02:57:45.907789: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 33177600 totalling 31.64MiB\n",
      "2025-08-06 02:57:45.907792: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 56623104 totalling 108.00MiB\n",
      "2025-08-06 02:57:45.907796: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 112819968 totalling 107.59MiB\n",
      "2025-08-06 02:57:45.907800: I tensorflow/core/common_runtime/bfc_allocator.cc:1101] Sum Total of in-use chunks: 254.70MiB\n",
      "2025-08-06 02:57:45.907803: I tensorflow/core/common_runtime/bfc_allocator.cc:1103] total_region_allocated_bytes_: 492240896 memory_limit_: 492240896 available bytes: 0 curr_region_allocation_bytes_: 984481792\n",
      "2025-08-06 02:57:45.907810: I tensorflow/core/common_runtime/bfc_allocator.cc:1109] Stats: \n",
      "Limit:                       492240896\n",
      "InUse:                       267071232\n",
      "MaxInUse:                    283438592\n",
      "NumAllocs:                         129\n",
      "MaxAllocSize:                112819968\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-08-06 02:57:45.907818: W tensorflow/core/common_runtime/bfc_allocator.cc:491] ************xxxxxxxxxxx*********______________************************______________________________\n",
      "2025-08-06 02:57:45.907845: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at conv_ops.cc:686 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,32,192,288] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/conv2d/Conv2D' defined at (most recent call last):\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/tmp/ipykernel_42437/175387373.py\", line 82, in <module>\n      pred_4xco2_norm = model.predict(TS_4xCO2_norm)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 2253, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 2041, in predict_function\n      return step_function(self, iterator)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 2027, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 2015, in run_step\n      outputs = model.predict_step(data)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 1983, in predict_step\n      return self(x, training=False)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n      return tf.nn.convolution(\nNode: 'model/conv2d/Conv2D'\nOOM when allocating tensor with shape[32,32,192,288] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/conv2d/Conv2D}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_predict_function_576]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 82\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Loading and predicting with model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(model_path)\n\u001b[0;32m---> 82\u001b[0m pred_4xco2_norm \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTS_4xCO2_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m pred_4xco2_unnorm \u001b[38;5;241m=\u001b[39m pred_4xco2_norm \u001b[38;5;241m*\u001b[39m y_std \u001b[38;5;241m+\u001b[39m y_mean\n\u001b[1;32m     84\u001b[0m predictions_from_folds\u001b[38;5;241m.\u001b[39mappend(pred_4xco2_unnorm)\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model/conv2d/Conv2D' defined at (most recent call last):\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/tmp/ipykernel_42437/175387373.py\", line 82, in <module>\n      pred_4xco2_norm = model.predict(TS_4xCO2_norm)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 2253, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 2041, in predict_function\n      return step_function(self, iterator)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 2027, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 2015, in run_step\n      outputs = model.predict_step(data)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 1983, in predict_step\n      return self(x, training=False)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n      return tf.nn.convolution(\nNode: 'model/conv2d/Conv2D'\nOOM when allocating tensor with shape[32,32,192,288] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/conv2d/Conv2D}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_predict_function_576]"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# FINAL OUT-OF-SAMPLE TEST SCRIPT FOR 2D ZONAL-MEAN MODELS\n",
    "# ==========================================================\n",
    "# This script contains all necessary code and corrected paths.\n",
    "# Please use this to replace your entire out-of-sample notebook.\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from keras.models import load_model\n",
    "import xarray as xr\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# --- 1. Configuration: Set ONE Correct Path for Model Results ---\n",
    "print(\"--- Setting up for Zonal-Mean Out-of-Sample Test ---\")\n",
    "\n",
    "# For CESM1:\n",
    "path_model_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM1/trained_model_2D_changedLRandKS'\n",
    "# For CESM2:\n",
    "#path_model_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM2/trained_model_2D_changedLRandKS'\n",
    "\n",
    "# Define variable names for your test files\n",
    "In_name = \"TS\"\n",
    "Out_name = \"TOA_anom\"\n",
    "\n",
    "# Define paths to your TWO separate 4xCO2 test files\n",
    "# # For CESM1:\n",
    "file_4xCO2_input = \"/ocean/projects/ees250004p/ezhu3/data/CESM1/test/test.4xCO2.ANN.new.nc\"\n",
    "file_4xCO2_output = \"/ocean/projects/ees250004p/ezhu3/data/CESM1/test/test.4xCO2.zmean.ANN.new.nc\"\n",
    "\n",
    "# For CESM2:\n",
    "#file_4xCO2_input = \"/ocean/projects/ees250004p/ezhu3/data/CESM2/test/test.CESM2-4xCO2.ANN.nc\"\n",
    "#file_4xCO2_output = \"/ocean/projects/ees250004p/ezhu3/data/CESM2/test/test.CESM2-4xCO2.zmean.ANN.nc\"\n",
    "\n",
    "# --- 2. Load Normalization Data from the Training Run ---\n",
    "print(\"Loading normalization data...\")\n",
    "normalization_path = os.path.join(path_model_dir, 'Normalization_zonal.mat')\n",
    "normalization = sio.loadmat(normalization_path)\n",
    "X_mean = normalization['X_mean']\n",
    "X_std = normalization['X_std']\n",
    "y_mean = normalization['y_mean']\n",
    "y_std = normalization['y_std']\n",
    "print(\"✅ Normalization data loaded successfully.\")\n",
    "print(X_mean)\n",
    "print(X_std)\n",
    "print(y_mean)\n",
    "print(y_std)\n",
    "# --- 3. Load and Preprocess 4xCO2 Test Data ---\n",
    "print(\"\\nLoading and preprocessing 4xCO2 test data...\")\n",
    "ds_4xCO2_X = xr.open_dataset(file_4xCO2_input)\n",
    "ds_4xCO2_y = xr.open_dataset(file_4xCO2_output)\n",
    "\n",
    "# Extract variables from the correct files\n",
    "TS_4xCO2_raw = ds_4xCO2_X[In_name]\n",
    "TOA_4xCO2_truth = ds_4xCO2_y[Out_name].values\n",
    "lat = ds_4xCO2_X['lat'].values\n",
    "time_4xCO2 = ds_4xCO2_X['year'].values if 'year' in ds_4xCO2_X else ds_4xCO2_X['time'].values\n",
    "\n",
    "# Normalize inputs correctly by adding the channel dimension first\n",
    "TS_4xCO2_norm = (TS_4xCO2_raw.values[..., np.newaxis] - X_mean) / X_std\n",
    "print(\"✅ Test data preprocessed.\")\n",
    "\n",
    "# --- 4. Prediction Loop ---\n",
    "print(\"\\n--- Running Ensemble Predictions for 4xCO2 ---\")\n",
    "n_folds = 5\n",
    "predictions_from_folds = []\n",
    "\n",
    "for fold_no in range(1, n_folds + 1):\n",
    "    K.clear_session(); gc.collect()\n",
    "    \n",
    "    # Use the corrected, direct path to load the model\n",
    "    model_path = os.path.join(path_model_dir, f'model_fold{fold_no}_ens1.h5')\n",
    "    print(f\"    Loading and predicting with model: {model_path}\")\n",
    "    \n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    pred_4xco2_norm = model.predict(TS_4xCO2_norm)\n",
    "    pred_4xco2_unnorm = pred_4xco2_norm * y_std + y_mean\n",
    "    predictions_from_folds.append(pred_4xco2_unnorm)\n",
    "\n",
    "# Average predictions across the folds\n",
    "Model_pred_4xco2 = np.mean(np.stack(predictions_from_folds), axis=0)\n",
    "print(\"\\n✅ Prediction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c60dd36-d92c-4163-b95f-971daad94051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Out-of-Sample Analysis and Visualization (Corrected Plotting Axis)\n",
    "# ==========================================================\n",
    "print(\"\\n--- Starting Out-of-Sample Analysis ---\")\n",
    "\n",
    "# --- Task 1: Calculate Overall Pattern Correlation ---\n",
    "print(\"\\n    Calculating Overall Pattern Correlation...\")\n",
    "truth_flat = TOA_4xCO2_truth.flatten()\n",
    "pred_flat = Model_pred_4xco2.flatten()\n",
    "pattern_r, _ = pearsonr(truth_flat, pred_flat)\n",
    "print(f\"✅ Overall Pattern Correlation (r) = {pattern_r:.4f}\")\n",
    "\n",
    "# --- Task 2: Plot R-squared as a Function of Latitude ---\n",
    "print(\"\\n    Calculating and plotting R-squared per latitude...\")\n",
    "r2_by_latitude = [r2_score(TOA_4xCO2_truth[:, i], Model_pred_4xco2[:, i]) for i in range(len(lat))]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lat, r2_by_latitude, marker='o', linestyle='-')\n",
    "plt.title('Out-of-Sample Performance (R²) by Latitude - 4xCO2', fontsize=16)\n",
    "plt.xlabel('Latitude', fontsize=12)\n",
    "plt.ylabel('R-squared Score', fontsize=12)\n",
    "plt.grid(True, linestyle='--'); plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# --- Task 3: Plot Truth vs. Prediction as a 2D Contour Map ---\n",
    "print(\"\\n    Plotting Truth vs. Prediction as contour maps...\")\n",
    "\n",
    "# This creates a simple numerical axis [0, 1, 2, ...] for plotting\n",
    "time_axis_for_plot = np.arange(TOA_4xCO2_truth.shape[0])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6), sharey=True)\n",
    "vmax = np.percentile(np.abs(TOA_4xCO2_truth), 98)\n",
    "vmin = -vmax\n",
    "\n",
    "axes[0].set_title('Ground Truth TOA Zonal Mean', fontsize=16)\n",
    "# Use the new simple time axis for plotting\n",
    "cf1 = axes[0].contourf(time_axis_for_plot, lat, TOA_4xCO2_truth.T, levels=20, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
    "axes[0].set_xlabel('Time (Model Years)', fontsize=12)\n",
    "axes[0].set_ylabel('Latitude', fontsize=12)\n",
    "\n",
    "axes[1].set_title('Predicted TOA Zonal Mean', fontsize=16)\n",
    "# Use the new simple time axis for plotting here as well\n",
    "cf2 = axes[1].contourf(time_axis_for_plot, lat, Model_pred_4xco2.T, levels=20, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
    "axes[1].set_xlabel('Time (Model Years)', fontsize=12)\n",
    "\n",
    "fig.colorbar(cf1, ax=axes.ravel().tolist(), shrink=0.8, label='TOA Anomaly (W/m²)')\n",
    "fig.suptitle(\"Out-of-Sample Results - 4xCO2 Scenario\", fontsize=18, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470f59c-3312-4733-9169-81bb557753ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Task 4: Calculate and Compare Global Mean Time Series\n",
    "# =========================================================\n",
    "print(\"\\n--- Calculating and Comparing Weighted Global Means ---\")\n",
    "\n",
    "# --- Step 1: Calculate Latitude Weights ---\n",
    "# To get a true global mean, we must weight each latitude by the cosine\n",
    "# of its angle to account for the smaller grid cell areas near the poles.\n",
    "lat_radians = np.deg2rad(lat)\n",
    "weights = np.cos(lat_radians)\n",
    "# Ensure weights have the correct shape for broadcasting during the average\n",
    "weights = weights[np.newaxis, :]\n",
    "\n",
    "# --- Step 2: Calculate Weighted Average for Truth and Prediction ---\n",
    "# We average over the latitude axis (axis=1) to get a single global\n",
    "# mean value for each time step.\n",
    "global_mean_truth = np.average(TOA_4xCO2_truth, axis=1, weights=weights.flatten())\n",
    "global_mean_pred = np.average(Model_pred_4xco2, axis=1, weights=weights.flatten())\n",
    "\n",
    "print(\"✅ Weighted global means calculated.\")\n",
    "\n",
    "# --- Step 3: Print and Compare the Overall Mean Values ---\n",
    "# This gives a single number summary of the entire time series\n",
    "print(f\"    Overall Mean of Ground Truth: {np.mean(global_mean_truth):.4f} W/m²\")\n",
    "print(f\"    Overall Mean of Prediction:   {np.mean(global_mean_pred):.4f} W/m²\")\n",
    "\n",
    "# --- Step 4: Plot the Global Mean Time Series for Comparison ---\n",
    "print(\"\\n    Plotting global mean time series comparison...\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "time_axis_for_plot = np.arange(global_mean_truth.shape[0])\n",
    "\n",
    "plt.plot(time_axis_for_plot, global_mean_truth, label='Ground Truth', color='black', linewidth=2)\n",
    "plt.plot(time_axis_for_plot, global_mean_pred, label='Model Prediction', color='red', linestyle='--')\n",
    "\n",
    "plt.title('Out-of-Sample: Global Mean TOA Anomaly Time Series', fontsize=16)\n",
    "plt.xlabel('Time (Model Years)', fontsize=12)\n",
    "plt.ylabel('Global Mean Anomaly (W/m²)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025f18b-9ebb-4058-ae42-5357b6e20c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Task 4: Calculate and Compare Global Mean Time Series\n",
    "# =========================================================\n",
    "print(\"\\n--- Calculating and Comparing Weighted Global Means ---\")\n",
    "\n",
    "# --- Step 1: Calculate Latitude Weights ---\n",
    "# To get a true global mean, we must weight each latitude by the cosine\n",
    "# of its angle to account for the smaller grid cell areas near the poles.\n",
    "lat_radians = np.deg2rad(lat)\n",
    "weights = np.cos(lat_radians)\n",
    "# Ensure weights have the correct shape for broadcasting during the average\n",
    "weights = weights[np.newaxis, :]\n",
    "\n",
    "# --- Step 2: Calculate Weighted Average for Truth and Prediction ---\n",
    "# We average over the latitude axis (axis=1) to get a single global\n",
    "# mean value for each time step.\n",
    "global_mean_truth = np.average(TOA_4xCO2_truth, axis=1, weights=weights.flatten())\n",
    "global_mean_pred = np.average(Model_pred_4xco2, axis=1, weights=weights.flatten())\n",
    "\n",
    "print(\"✅ Weighted global means calculated.\")\n",
    "\n",
    "# --- Step 3: Print and Compare the Overall Mean Values ---\n",
    "# This gives a single number summary of the entire time series\n",
    "print(f\"    Overall Mean of Ground Truth: {np.mean(global_mean_truth):.4f} W/m²\")\n",
    "print(f\"    Overall Mean of Prediction:   {np.mean(global_mean_pred):.4f} W/m²\")\n",
    "\n",
    "# --- Step 4: Plot the Global Mean Time Series for Comparison ---\n",
    "print(\"\\n    Plotting global mean time series comparison...\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "time_axis_for_plot = np.arange(global_mean_truth.shape[0])\n",
    "\n",
    "plt.plot(time_axis_for_plot, global_mean_truth, label='Ground Truth', color='black', linewidth=2)\n",
    "plt.plot(time_axis_for_plot, global_mean_pred, label='Model Prediction', color='red', linestyle='--')\n",
    "\n",
    "plt.title('Out-of-Sample: Global Mean TOA Anomaly Time Series', fontsize=16)\n",
    "plt.xlabel('Time (Model Years)', fontsize=12)\n",
    "plt.ylabel('Global Mean Anomaly (W/m²)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dbe86a-9ace-4a66-893d-6231e0ecefc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# Task 5: Calculate and Plot Climate Feedback Parameter (λ)\n",
    "# =================================================================\n",
    "from scipy.stats import linregress\n",
    "\n",
    "print(\"\\n--- Calculating and Plotting Climate Feedback Parameter (λ) ---\")\n",
    "\n",
    "# --- Step 1: Calculate the X-axis data (Area-Weighted Global Mean TS Anomaly) ---\n",
    "# We already have the latitude weights from the previous task.\n",
    "# First, take the mean across the longitude axis of the raw input data.\n",
    "TS_zonal_mean_truth = np.mean(TS_4xCO2_raw.values, axis=2)\n",
    "\n",
    "# Now, calculate the latitude-weighted average to get the global mean.\n",
    "global_mean_TS_truth = np.average(TS_zonal_mean_truth, axis=1, weights=weights.flatten())\n",
    "\n",
    "# --- Step 2: Set up the side-by-side plots ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8), sharey=True)\n",
    "fig.suptitle('Climate Feedback Parameter (λ) - 4xCO2 Out-of-Sample', fontsize=20, weight='bold')\n",
    "\n",
    "# --- Step 3: Plot for the Ground Truth ---\n",
    "ax1 = axes[0]\n",
    "# Perform linear regression to find the slope (lambda)\n",
    "slope_truth, intercept_truth, r_value_truth, _, _ = linregress(global_mean_TS_truth, global_mean_truth)\n",
    "lambda_truth = slope_truth\n",
    "r2_truth = r_value_truth**2\n",
    "\n",
    "# Scatter plot\n",
    "ax1.scatter(global_mean_TS_truth, global_mean_truth, alpha=0.6, label='Yearly Data (Ground Truth)')\n",
    "# Best-fit line\n",
    "fit_line_truth = slope_truth * global_mean_TS_truth + intercept_truth\n",
    "ax1.plot(global_mean_TS_truth, fit_line_truth, color='red', linestyle='--', label='Linear Best Fit')\n",
    "\n",
    "# Add text box with results\n",
    "text_truth = (f'λ = {lambda_truth:.3f} W/m²/K\\n'\n",
    "              f'$R^2$ = {r2_truth:.3f}')\n",
    "ax1.text(0.05, 0.95, text_truth, transform=ax1.transAxes, fontsize=14,\n",
    "         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "ax1.set_title('Ground Truth Data', fontsize=16)\n",
    "ax1.set_xlabel('Area-Weighted Global Mean TS Anomaly (K)', fontsize=12)\n",
    "ax1.set_ylabel('Global Mean TOA Anomaly (W/m²)', fontsize=12)\n",
    "ax1.grid(True, linestyle=':')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "# --- Step 4: Plot for the Model Prediction ---\n",
    "ax2 = axes[1]\n",
    "# Perform linear regression\n",
    "slope_pred, intercept_pred, r_value_pred, _, _ = linregress(global_mean_TS_truth, global_mean_pred)\n",
    "lambda_pred = slope_pred\n",
    "r2_pred = r_value_pred**2\n",
    "\n",
    "# Scatter plot\n",
    "ax2.scatter(global_mean_TS_truth, global_mean_pred, alpha=0.6, label='Yearly Data (Model Prediction)')\n",
    "# Best-fit line\n",
    "fit_line_pred = slope_pred * global_mean_TS_truth + intercept_pred\n",
    "ax2.plot(global_mean_TS_truth, fit_line_pred, color='red', linestyle='--', label='Linear Best Fit')\n",
    "\n",
    "# Add text box with results\n",
    "text_pred = (f'λ = {lambda_pred:.3f} W/m²/K\\n'\n",
    "             f'$R^2$ = {r2_pred:.3f}')\n",
    "ax2.text(0.05, 0.95, text_pred, transform=ax2.transAxes, fontsize=14,\n",
    "         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "ax2.set_title('Model Prediction', fontsize=16)\n",
    "ax2.set_xlabel('Area-Weighted Global Mean TS Anomaly (K)', fontsize=12)\n",
    "# No Y-label needed as it's shared with ax1\n",
    "ax2.grid(True, linestyle=':')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to make room for suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e74817-43af-4398-a960-f0860a2e8879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# NEW ANALYSIS: Global Mean of Zonal-Mean Prediction vs. Truth\n",
    "# ==========================================================\n",
    "print(\"\\n--- Starting new analysis: Comparing the area-weighted global mean ---\")\n",
    "\n",
    "# --- 1. Helper Function for Area-Weighted Mean ---\n",
    "# This function correctly calculates the global mean from zonal-mean [time, lat] data.\n",
    "def calculate_area_weighted_global_mean_zonal(data_2d, lat_coords):\n",
    "    \"\"\"\n",
    "    Calculates the area-weighted global mean from a [time, lat] array.\n",
    "    \"\"\"\n",
    "    # The weights for each latitude band are proportional to the cosine of the latitude.\n",
    "    weights = np.cos(np.deg2rad(lat_coords))\n",
    "    # np.average calculates the weighted average over the latitude axis (axis=1).\n",
    "    global_mean_timeseries = np.average(data_2d, axis=1, weights=weights)\n",
    "    return global_mean_timeseries\n",
    "\n",
    "# --- 2. Calculate the Global Mean Time Series ---\n",
    "# This assumes 'TOA_4xCO2_truth' and 'Model_pred_4xco2' are in memory from the previous cell.\n",
    "# It also assumes 'lat' (your latitude coordinate array) is in memory.\n",
    "\n",
    "print(\"    Calculating global mean for both truth and prediction...\")\n",
    "global_mean_truth = calculate_area_weighted_global_mean_zonal(TOA_4xCO2_truth, lat)\n",
    "global_mean_pred = calculate_area_weighted_global_mean_zonal(Model_pred_4xco2, lat)\n",
    "\n",
    "# --- 3. Plot the Comparison Graph ---\n",
    "print(\"    Plotting the global mean comparison graph...\")\n",
    "\n",
    "# Calculate the R-squared score for the global mean time series\n",
    "r2_global_mean = r2_score(global_mean_truth, global_mean_pred)\n",
    "\n",
    "# === THIS IS THE FIX ===\n",
    "# Instead of using the complex time objects from the file, we create a simple numerical axis.\n",
    "# This will be an array like [0, 1, 2, ...] that has the correct length for plotting.\n",
    "time_axis_for_plot = np.arange(len(global_mean_truth))\n",
    "# ======================\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Use the new, simple time axis for plotting\n",
    "ax.plot(time_axis_for_plot, global_mean_truth, label=\"Truth (Global Mean)\", color=\"k\", linewidth=2.5)\n",
    "ax.plot(time_axis_for_plot, global_mean_pred, label=\"Prediction (Global Mean)\", color=\"C3\", linestyle='--')\n",
    "\n",
    "# Add R-squared annotation\n",
    "ax.text(0.02, 0.95, f\"$R^2$ = {r2_global_mean:.3f}\", transform=ax.transAxes,\n",
    "        fontsize=16, bbox=dict(facecolor=\"white\", edgecolor=\"black\", alpha=0.7))\n",
    "\n",
    "# Style the plot\n",
    "ax.set_xlabel(\"Time (Model Years)\", fontsize=16) # Label updated for clarity\n",
    "ax.set_ylabel(\"Global Mean TOA Anomaly (W/m²)\", fontsize=16)\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "ax.tick_params(axis='both', labelsize=14)\n",
    "ax.set_title(\"Model Performance on Global Mean (from Zonal-Mean Prediction)\", fontsize=18, pad=15)\n",
    "ax.legend(fontsize=14, loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf210)",
   "language": "python",
   "name": "tf210"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
