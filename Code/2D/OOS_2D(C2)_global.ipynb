{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbba3047-f803-4acb-a677-8feba2a8497d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 01:42:21.552538: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-06 01:42:21.657882: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-06 01:42:21.690978: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up for Zonal-Mean Out-of-Sample Test ---\n",
      "Loading normalization data...\n",
      "✅ Normalization data loaded successfully.\n",
      "[[[222.94443]\n",
      "  [222.94443]\n",
      "  [222.94443]\n",
      "  ...\n",
      "  [222.94443]\n",
      "  [222.94443]\n",
      "  [222.94443]]\n",
      "\n",
      " [[223.51381]\n",
      "  [223.47977]\n",
      "  [223.34406]\n",
      "  ...\n",
      "  [223.54082]\n",
      "  [223.53523]\n",
      "  [223.52567]]\n",
      "\n",
      " [[224.13083]\n",
      "  [224.09398]\n",
      "  [224.07494]\n",
      "  ...\n",
      "  [224.33713]\n",
      "  [224.29172]\n",
      "  [224.22809]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255.92625]\n",
      "  [255.94229]\n",
      "  [255.95798]\n",
      "  ...\n",
      "  [255.88461]\n",
      "  [255.89777]\n",
      "  [255.91098]]\n",
      "\n",
      " [[255.81615]\n",
      "  [255.82231]\n",
      "  [255.82944]\n",
      "  ...\n",
      "  [255.79529]\n",
      "  [255.80214]\n",
      "  [255.80928]]\n",
      "\n",
      " [[255.6866 ]\n",
      "  [255.68732]\n",
      "  [255.68805]\n",
      "  ...\n",
      "  [255.68419]\n",
      "  [255.6851 ]\n",
      "  [255.68585]]]\n",
      "[[[0.70500857]\n",
      "  [0.70500576]\n",
      "  [0.70500827]\n",
      "  ...\n",
      "  [0.70500445]\n",
      "  [0.7050062 ]\n",
      "  [0.7050094 ]]\n",
      "\n",
      " [[0.74037784]\n",
      "  [0.7402208 ]\n",
      "  [0.74013007]\n",
      "  ...\n",
      "  [0.7406546 ]\n",
      "  [0.74058586]\n",
      "  [0.74052685]]\n",
      "\n",
      " [[0.76330787]\n",
      "  [0.7628692 ]\n",
      "  [0.76243496]\n",
      "  ...\n",
      "  [0.7645274 ]\n",
      "  [0.7641407 ]\n",
      "  [0.76372313]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1.0075827 ]\n",
      "  [1.009736  ]\n",
      "  [1.0118273 ]\n",
      "  ...\n",
      "  [1.0020808 ]\n",
      "  [1.0038892 ]\n",
      "  [1.0056243 ]]\n",
      "\n",
      " [[0.9827783 ]\n",
      "  [0.98358   ]\n",
      "  [0.98441947]\n",
      "  ...\n",
      "  [0.9801285 ]\n",
      "  [0.9810324 ]\n",
      "  [0.9819265 ]]\n",
      "\n",
      " [[0.9659744 ]\n",
      "  [0.9660457 ]\n",
      "  [0.96610993]\n",
      "  ...\n",
      "  [0.96571904]\n",
      "  [0.9658127 ]\n",
      "  [0.96589845]]]\n",
      "[[ 1.02481841e-04  1.41601558e-05  1.76136018e-04 -1.04003906e-04\n",
      "  -2.63862603e-05 -4.16030889e-05  9.58557139e-05  3.79066478e-05\n",
      "   8.85009740e-07  1.53160090e-05  1.76681526e-04  1.29318241e-05\n",
      "   5.76591483e-05 -2.43339546e-05  7.14607231e-05 -3.27911366e-05\n",
      "   1.41868595e-05  5.87425238e-05  5.98297120e-05 -2.18391415e-05\n",
      "   2.06604000e-05  6.82716345e-05 -4.42543023e-05  3.13072196e-05\n",
      "  -2.74276736e-05 -1.42776495e-04 -3.19557184e-05  4.82254036e-05\n",
      "   7.70568874e-07 -6.42166124e-05  2.31819158e-05  8.71429438e-05\n",
      "  -8.00552370e-05  2.87628177e-06  1.85146328e-05  2.25391395e-05\n",
      "   9.67216511e-06  2.79350279e-05 -8.78715491e-06  3.14865101e-05\n",
      "  -9.16480985e-06  1.59320825e-05 -1.68666847e-05  3.80210877e-05\n",
      "  -2.77614599e-05 -1.29508971e-05  8.74137913e-06  1.81655887e-05\n",
      "   1.91211711e-06  2.53973012e-05 -2.01988223e-06  3.45573426e-05\n",
      "   3.11622607e-05  1.94597251e-05 -1.25913621e-05 -2.85196302e-06\n",
      "  -4.19425942e-06 -4.12821777e-07 -1.96218494e-07  5.90205218e-07\n",
      "   1.40666955e-07 -1.30224225e-06  1.81245809e-06 -1.60813324e-05\n",
      "  -1.74827583e-05  2.09312439e-05  8.64505728e-06 -5.08022322e-06\n",
      "   1.01842879e-05  2.53963481e-06  7.27653514e-07 -4.38022607e-06\n",
      "   1.45244599e-06 -1.41639712e-05 -2.71606450e-06 -3.60012064e-06\n",
      "   1.03588109e-05 -2.79769902e-05  4.15058130e-05 -5.75065633e-06\n",
      "  -3.21197513e-06 -2.89649961e-05  7.62996642e-05 -9.93347203e-06\n",
      "  -2.39028923e-05 -1.41162873e-05 -2.04029075e-05  9.30023180e-06\n",
      "  -1.29318232e-06 -3.06663496e-05 -3.27930466e-05  8.37707557e-06\n",
      "   4.18472300e-05 -2.80036929e-05  1.36756898e-05 -3.90663154e-05\n",
      "  -1.47514347e-05  1.10515597e-04 -3.47328169e-05 -8.93783545e-06\n",
      "   4.55875379e-05  3.48625181e-05 -1.33171079e-05  2.33364099e-05\n",
      "   3.78990171e-06  9.74082923e-06 -9.75036619e-06 -3.63197323e-05\n",
      "   3.69453437e-06 -1.67522430e-05  2.48546603e-05  1.38320920e-05\n",
      "  -3.25317378e-05  1.05285644e-05  1.46064758e-05  6.58416729e-06\n",
      "  -2.58531563e-05  3.47290043e-05 -4.65650555e-05 -6.99806196e-06\n",
      "  -2.77109139e-05 -2.21233367e-05  4.60720048e-06 -1.24311446e-05\n",
      "   6.46018998e-06 -7.71236409e-06  8.12673534e-06 -1.59740443e-06\n",
      "  -3.06844709e-06 -7.70759561e-06  6.97278983e-06  8.07833658e-06\n",
      "   6.27040845e-06 -1.01208684e-06  3.54933741e-06 -2.26771840e-06\n",
      "  -2.40206717e-08 -9.44733642e-07 -1.85966485e-07  8.75854494e-06\n",
      "  -1.43527984e-07  2.87008279e-05 -6.43062594e-06  1.94501881e-05\n",
      "  -1.94892891e-05  5.44261911e-06 -4.31823719e-06  4.62780008e-05\n",
      "   1.23729706e-05 -2.17914585e-05  2.93731688e-07  5.76210005e-06\n",
      "  -7.05719003e-07  2.65426643e-05  1.25370025e-05  9.12761680e-05\n",
      "  -4.70542909e-06  9.96398921e-06 -4.59213261e-05 -5.67131028e-05\n",
      "  -1.15146635e-04  3.52554307e-05 -6.45980836e-05 -6.37626654e-05\n",
      "   4.85763558e-05  7.19451918e-06  1.42776495e-04 -3.56788623e-05\n",
      "  -5.49621582e-05 -5.77239989e-05  1.60980221e-06 -3.35388177e-05\n",
      "   1.34983056e-04 -1.04091647e-04 -8.01010101e-05 -6.36100740e-05\n",
      "   6.27937334e-05 -8.75930782e-05 -1.71295163e-04  1.49967193e-04\n",
      "   3.24745160e-05 -6.22100852e-05 -7.50350955e-06  1.29699711e-05\n",
      "  -4.50134257e-06  1.61972042e-04  5.37757878e-05  6.82678219e-05\n",
      "  -1.04259489e-04 -2.22778326e-05  1.26602172e-04  7.11250323e-05]]\n",
      "[[1.1908861  1.1703049  1.1202638  1.0629339  1.0172046  0.9930116\n",
      "  0.981361   0.964592   0.950623   0.9364285  0.92162734 0.9016738\n",
      "  0.8859335  0.96001035 1.0527908  1.04716    1.0332865  1.0378579\n",
      "  0.999709   0.93430024 0.93132716 1.0137584  1.1374654  1.1288849\n",
      "  1.1514851  1.2876607  1.4486275  1.4092035  1.3140324  1.2392188\n",
      "  1.1444274  1.0879781  1.0682241  1.0477573  1.0136533  0.9796961\n",
      "  0.97612983 0.99212384 1.0216887  1.0526297  1.0813006  1.1164253\n",
      "  1.1526071  1.1816671  1.2065198  1.2135463  1.2049475  1.1766922\n",
      "  1.1392503  1.1018404  1.0700076  1.0584174  1.0635347  1.0826921\n",
      "  1.1079438  1.1234088  1.1363395  1.1405363  1.141826   1.1224829\n",
      "  1.0888541  1.0458575  1.0125406  0.9906523  0.98579663 0.979395\n",
      "  0.9810889  0.9815201  0.9854952  0.99402726 1.0049293  1.0182068\n",
      "  1.0488755  1.0925137  1.141603   1.1989318  1.2661486  1.3140082\n",
      "  1.3370222  1.3449683  1.3313879  1.3077971  1.2939503  1.2928842\n",
      "  1.2836075  1.256644   1.1975064  1.1339375  1.046427   0.99247193\n",
      "  0.98709077 1.0122788  1.1136461  1.209023   1.3477013  1.4860644\n",
      "  1.5555027  1.5536956  1.5116719  1.5138106  1.579254   1.7354126\n",
      "  1.8776183  1.8584403  1.7104572  1.5345204  1.4446322  1.4429908\n",
      "  1.4421122  1.4365951  1.4310868  1.3946226  1.3218278  1.2405659\n",
      "  1.1455464  1.0527068  0.9703265  0.91903377 0.89651924 0.87862986\n",
      "  0.86244017 0.8504029  0.8405591  0.83474463 0.83699024 0.8346227\n",
      "  0.83541876 0.84168565 0.8477213  0.8609551  0.8810363  0.8802153\n",
      "  0.87117946 0.83890885 0.796182   0.7815402  0.7822322  0.78328854\n",
      "  0.78475446 0.7780989  0.7702172  0.7821052  0.7830036  0.7885807\n",
      "  0.7914651  0.8027496  0.81737196 0.8028551  0.78428453 0.7675443\n",
      "  0.7528726  0.7430172  0.74155456 0.7460755  0.755592   0.7637623\n",
      "  0.77839065 0.79725224 0.813924   0.823009   0.8353693  0.86914915\n",
      "  0.88951397 0.89594746 0.91166323 0.9281012  0.96215206 0.9773177\n",
      "  0.98102206 0.9953645  1.0656226  1.1194993  1.1280179  1.1231178\n",
      "  1.1205136  1.1068228  1.0865864  1.0996418  1.1182922  1.1586276\n",
      "  1.2131782  1.2422943  1.2762554  1.3555279  1.431204   1.4993526\n",
      "  1.556763   1.6068789  1.654959   1.7049515  1.7558464  1.8101698 ]]\n",
      "\n",
      "Loading and preprocessing 4xCO2 test data...\n",
      "✅ Test data preprocessed.\n",
      "\n",
      "--- Running Ensemble Predictions for 4xCO2 ---\n",
      "    Loading and predicting with model: /ocean/projects/ees250004p/ezhu3/data/CESM2/trained_model_2D_changedLRandKS/model_fold1_ens1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 01:42:24.697249: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-06 01:42:25.123154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 471 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:8a:00.0, compute capability: 7.0\n",
      "2025-08-06 01:42:35.843262: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 216.00MiB (rounded to 226492416)requested by op model/conv2d/Conv2D\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-08-06 01:42:35.843294: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] BFCAllocator dump for GPU_0_bfc\n",
      "2025-08-06 01:42:35.843324: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (256): \tTotal Chunks: 32, Chunks in use: 32. 8.0KiB allocated for chunks. 8.0KiB in use in bin. 892B client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843329: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (512): \tTotal Chunks: 17, Chunks in use: 17. 9.2KiB allocated for chunks. 9.2KiB in use in bin. 9.2KiB client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843333: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1024): \tTotal Chunks: 4, Chunks in use: 4. 5.0KiB allocated for chunks. 5.0KiB in use in bin. 4.4KiB client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843337: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843341: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843344: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843356: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843361: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (32768): \tTotal Chunks: 3, Chunks in use: 3. 132.2KiB allocated for chunks. 132.2KiB in use in bin. 108.0KiB client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843365: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (65536): \tTotal Chunks: 6, Chunks in use: 6. 577.5KiB allocated for chunks. 577.5KiB in use in bin. 480.0KiB client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843368: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843372: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843375: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843378: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843381: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843385: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4194304): \tTotal Chunks: 1, Chunks in use: 1. 6.75MiB allocated for chunks. 6.75MiB in use in bin. 6.75MiB client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843389: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843393: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16777216): \tTotal Chunks: 1, Chunks in use: 1. 31.64MiB allocated for chunks. 31.64MiB in use in bin. 31.64MiB client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843399: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (33554432): \tTotal Chunks: 2, Chunks in use: 2. 108.00MiB allocated for chunks. 108.00MiB in use in bin. 108.00MiB client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843404: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (67108864): \tTotal Chunks: 2, Chunks in use: 1. 177.24MiB allocated for chunks. 107.59MiB in use in bin. 54.00MiB client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843408: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 0. 147.09MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843411: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-06 01:42:35.843417: I tensorflow/core/common_runtime/bfc_allocator.cc:1056] Bin for 216.00MiB was 128.00MiB, Chunk State: \n",
      "2025-08-06 01:42:35.843425: I tensorflow/core/common_runtime/bfc_allocator.cc:1062]   Size: 147.09MiB | Requested Size: 128B | in_use: 0 | bin_num: 19, prev:   Size: 54.00MiB | Requested Size: 54.00MiB | in_use: 1 | bin_num: -1\n",
      "2025-08-06 01:42:35.843428: I tensorflow/core/common_runtime/bfc_allocator.cc:1069] Next region of size 494338048\n",
      "2025-08-06 01:42:35.843435: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506000000 of size 1280 next 1\n",
      "2025-08-06 01:42:35.843438: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506000500 of size 256 next 2\n",
      "2025-08-06 01:42:35.843441: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506000600 of size 256 next 3\n",
      "2025-08-06 01:42:35.843459: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506000700 of size 256 next 5\n",
      "2025-08-06 01:42:35.843462: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506000800 of size 256 next 6\n",
      "2025-08-06 01:42:35.843465: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506000900 of size 256 next 4\n",
      "2025-08-06 01:42:35.843468: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506000a00 of size 256 next 7\n",
      "2025-08-06 01:42:35.843471: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506000b00 of size 256 next 12\n",
      "2025-08-06 01:42:35.843473: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506000c00 of size 256 next 10\n",
      "2025-08-06 01:42:35.843476: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506000d00 of size 256 next 11\n",
      "2025-08-06 01:42:35.843480: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506000e00 of size 512 next 17\n",
      "2025-08-06 01:42:35.843483: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506001000 of size 512 next 15\n",
      "2025-08-06 01:42:35.843485: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506001200 of size 256 next 16\n",
      "2025-08-06 01:42:35.843488: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506001300 of size 256 next 8\n",
      "2025-08-06 01:42:35.843491: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506001400 of size 512 next 18\n",
      "2025-08-06 01:42:35.843494: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506001600 of size 111360 next 13\n",
      "2025-08-06 01:42:35.843497: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750601c900 of size 512 next 21\n",
      "2025-08-06 01:42:35.843500: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750601cb00 of size 256 next 14\n",
      "2025-08-06 01:42:35.843503: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750601cc00 of size 256 next 20\n",
      "2025-08-06 01:42:35.843506: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750601cd00 of size 256 next 22\n",
      "2025-08-06 01:42:35.843509: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750601ce00 of size 256 next 25\n",
      "2025-08-06 01:42:35.843512: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750601cf00 of size 256 next 31\n",
      "2025-08-06 01:42:35.843514: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750601d000 of size 256 next 32\n",
      "2025-08-06 01:42:35.843517: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750601d100 of size 256 next 28\n",
      "2025-08-06 01:42:35.843520: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750601d200 of size 256 next 26\n",
      "2025-08-06 01:42:35.843523: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750601d300 of size 1280 next 27\n",
      "2025-08-06 01:42:35.843526: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750601d800 of size 256 next 9\n",
      "2025-08-06 01:42:35.843529: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750601d900 of size 61696 next 23\n",
      "2025-08-06 01:42:35.843532: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750602ca00 of size 98304 next 24\n",
      "2025-08-06 01:42:35.843535: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506044a00 of size 768 next 30\n",
      "2025-08-06 01:42:35.843537: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506044d00 of size 256 next 33\n",
      "2025-08-06 01:42:35.843540: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506044e00 of size 256 next 34\n",
      "2025-08-06 01:42:35.843543: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506044f00 of size 256 next 35\n",
      "2025-08-06 01:42:35.843546: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506045000 of size 256 next 37\n",
      "2025-08-06 01:42:35.843548: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506045100 of size 512 next 61\n",
      "2025-08-06 01:42:35.843551: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506045300 of size 512 next 39\n",
      "2025-08-06 01:42:35.843554: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506045500 of size 512 next 42\n",
      "2025-08-06 01:42:35.843557: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506045700 of size 512 next 43\n",
      "2025-08-06 01:42:35.843559: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506045900 of size 103936 next 45\n",
      "2025-08-06 01:42:35.843562: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750605ef00 of size 768 next 44\n",
      "2025-08-06 01:42:35.843565: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750605f200 of size 1280 next 40\n",
      "2025-08-06 01:42:35.843568: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750605f700 of size 256 next 46\n",
      "2025-08-06 01:42:35.843571: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750605f800 of size 36864 next 47\n",
      "2025-08-06 01:42:35.843574: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506068800 of size 256 next 48\n",
      "2025-08-06 01:42:35.843576: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506068900 of size 512 next 50\n",
      "2025-08-06 01:42:35.843579: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506068b00 of size 512 next 51\n",
      "2025-08-06 01:42:35.843582: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506068d00 of size 113920 next 29\n",
      "2025-08-06 01:42:35.843585: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147506084a00 of size 112819968 next 19\n",
      "2025-08-06 01:42:35.843588: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750cc1c900 of size 65536 next 41\n",
      "2025-08-06 01:42:35.843591: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750cc2c900 of size 512 next 53\n",
      "2025-08-06 01:42:35.843593: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750cc2cb00 of size 512 next 54\n",
      "2025-08-06 01:42:35.843596: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750cc2cd00 of size 98304 next 55\n",
      "2025-08-06 01:42:35.843599: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750cc44d00 of size 768 next 56\n",
      "2025-08-06 01:42:35.843602: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750cc45000 of size 33177600 next 57\n",
      "2025-08-06 01:42:35.843605: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750ebe9000 of size 256 next 52\n",
      "2025-08-06 01:42:35.843608: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750ebe9100 of size 256 next 62\n",
      "2025-08-06 01:42:35.843611: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750ebe9200 of size 256 next 63\n",
      "2025-08-06 01:42:35.843614: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750ebe9300 of size 256 next 64\n",
      "2025-08-06 01:42:35.843616: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750ebe9400 of size 256 next 65\n",
      "2025-08-06 01:42:35.843619: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750ebe9500 of size 7077888 next 66\n",
      "2025-08-06 01:42:35.843622: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750f2a9500 of size 512 next 67\n",
      "2025-08-06 01:42:35.843625: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14750f2a9700 of size 512 next 68\n",
      "2025-08-06 01:42:35.843628: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 14750f2a9900 of size 73028864 next 58\n",
      "2025-08-06 01:42:35.843630: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14751384ee00 of size 256 next 59\n",
      "2025-08-06 01:42:35.843633: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14751384ef00 of size 1280 next 60\n",
      "2025-08-06 01:42:35.843636: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 14751384f400 of size 36864 next 36\n",
      "2025-08-06 01:42:35.843639: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147513858400 of size 56623104 next 38\n",
      "2025-08-06 01:42:35.843642: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 147516e58400 of size 56623104 next 49\n",
      "2025-08-06 01:42:35.843645: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 14751a458400 of size 154237952 next 18446744073709551615\n",
      "2025-08-06 01:42:35.843647: I tensorflow/core/common_runtime/bfc_allocator.cc:1094]      Summary of in-use Chunks by size: \n",
      "2025-08-06 01:42:35.843652: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 32 Chunks of size 256 totalling 8.0KiB\n",
      "2025-08-06 01:42:35.843655: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 14 Chunks of size 512 totalling 7.0KiB\n",
      "2025-08-06 01:42:35.843659: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 3 Chunks of size 768 totalling 2.2KiB\n",
      "2025-08-06 01:42:35.843662: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 4 Chunks of size 1280 totalling 5.0KiB\n",
      "2025-08-06 01:42:35.843665: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 36864 totalling 72.0KiB\n",
      "2025-08-06 01:42:35.843668: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 61696 totalling 60.2KiB\n",
      "2025-08-06 01:42:35.843671: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 65536 totalling 64.0KiB\n",
      "2025-08-06 01:42:35.843675: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 98304 totalling 192.0KiB\n",
      "2025-08-06 01:42:35.843678: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 103936 totalling 101.5KiB\n",
      "2025-08-06 01:42:35.843681: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 111360 totalling 108.8KiB\n",
      "2025-08-06 01:42:35.843684: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 113920 totalling 111.2KiB\n",
      "2025-08-06 01:42:35.843688: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 7077888 totalling 6.75MiB\n",
      "2025-08-06 01:42:35.843691: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 33177600 totalling 31.64MiB\n",
      "2025-08-06 01:42:35.843694: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 56623104 totalling 108.00MiB\n",
      "2025-08-06 01:42:35.843698: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 112819968 totalling 107.59MiB\n",
      "2025-08-06 01:42:35.843701: I tensorflow/core/common_runtime/bfc_allocator.cc:1101] Sum Total of in-use chunks: 254.70MiB\n",
      "2025-08-06 01:42:35.843704: I tensorflow/core/common_runtime/bfc_allocator.cc:1103] total_region_allocated_bytes_: 494338048 memory_limit_: 494338048 available bytes: 0 curr_region_allocation_bytes_: 988676096\n",
      "2025-08-06 01:42:35.843711: I tensorflow/core/common_runtime/bfc_allocator.cc:1109] Stats: \n",
      "Limit:                       494338048\n",
      "InUse:                       267071232\n",
      "MaxInUse:                    283438592\n",
      "NumAllocs:                         129\n",
      "MaxAllocSize:                112819968\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-08-06 01:42:35.843718: W tensorflow/core/common_runtime/bfc_allocator.cc:491] ************xxxxxxxxxx**********_____________************************_______________________________\n",
      "2025-08-06 01:42:35.843748: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at conv_ops.cc:686 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,32,192,288] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/conv2d/Conv2D' defined at (most recent call last):\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/tmp/ipykernel_37652/555068419.py\", line 82, in <module>\n      pred_4xco2_norm = model.predict(TS_4xCO2_norm)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 2253, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 2041, in predict_function\n      return step_function(self, iterator)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 2027, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 2015, in run_step\n      outputs = model.predict_step(data)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 1983, in predict_step\n      return self(x, training=False)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n      return tf.nn.convolution(\nNode: 'model/conv2d/Conv2D'\nOOM when allocating tensor with shape[32,32,192,288] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/conv2d/Conv2D}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_predict_function_576]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 82\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Loading and predicting with model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(model_path)\n\u001b[0;32m---> 82\u001b[0m pred_4xco2_norm \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTS_4xCO2_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m pred_4xco2_unnorm \u001b[38;5;241m=\u001b[39m pred_4xco2_norm \u001b[38;5;241m*\u001b[39m y_std \u001b[38;5;241m+\u001b[39m y_mean\n\u001b[1;32m     84\u001b[0m predictions_from_folds\u001b[38;5;241m.\u001b[39mappend(pred_4xco2_unnorm)\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/tf210/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model/conv2d/Conv2D' defined at (most recent call last):\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/tmp/ipykernel_37652/555068419.py\", line 82, in <module>\n      pred_4xco2_norm = model.predict(TS_4xCO2_norm)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 2253, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 2041, in predict_function\n      return step_function(self, iterator)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 2027, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 2015, in run_step\n      outputs = model.predict_step(data)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 1983, in predict_step\n      return self(x, training=False)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/jet/home/ezhu3/.conda/envs/tf210/lib/python3.8/site-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n      return tf.nn.convolution(\nNode: 'model/conv2d/Conv2D'\nOOM when allocating tensor with shape[32,32,192,288] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/conv2d/Conv2D}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_predict_function_576]"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# FINAL OUT-OF-SAMPLE TEST SCRIPT FOR 2D ZONAL-MEAN MODELS\n",
    "# ==========================================================\n",
    "# This script contains all necessary code and corrected paths.\n",
    "# Please use this to replace your entire out-of-sample notebook.\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from keras.models import load_model\n",
    "import xarray as xr\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# --- 1. Configuration: Set ONE Correct Path for Model Results ---\n",
    "print(\"--- Setting up for Zonal-Mean Out-of-Sample Test ---\")\n",
    "\n",
    "# For CESM1:\n",
    "#path_model_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM1/trained_model_2D_changedLRandKS'\n",
    "# For CESM2:\n",
    "path_model_dir = '/ocean/projects/ees250004p/ezhu3/data/CESM2/trained_model_2D_changedLRandKS'\n",
    "\n",
    "# Define variable names for your test files\n",
    "In_name = \"TS\"\n",
    "Out_name = \"TOA_anom\"\n",
    "\n",
    "# Define paths to your TWO separate 4xCO2 test files\n",
    "# # For CESM1:\n",
    "# file_4xCO2_input = \"/ocean/projects/ees250004p/ezhu3/data/CESM1/test/test.4xCO2.ANN.new.nc\"\n",
    "# file_4xCO2_output = \"/ocean/projects/ees250004p/ezhu3/data/CESM1/test/test.4xCO2.zmean.ANN.new.nc\"\n",
    "\n",
    "# For CESM2:\n",
    "file_4xCO2_input = \"/ocean/projects/ees250004p/ezhu3/data/CESM2/test/test.CESM2-4xCO2.ANN.nc\"\n",
    "file_4xCO2_output = \"/ocean/projects/ees250004p/ezhu3/data/CESM2/test/test.CESM2-4xCO2.zmean.ANN.nc\"\n",
    "\n",
    "# --- 2. Load Normalization Data from the Training Run ---\n",
    "print(\"Loading normalization data...\")\n",
    "normalization_path = os.path.join(path_model_dir, 'Normalization_zonal.mat')\n",
    "normalization = sio.loadmat(normalization_path)\n",
    "X_mean = normalization['X_mean']\n",
    "X_std = normalization['X_std']\n",
    "y_mean = normalization['y_mean']\n",
    "y_std = normalization['y_std']\n",
    "print(\"✅ Normalization data loaded successfully.\")\n",
    "print(X_mean)\n",
    "print(X_std)\n",
    "print(y_mean)\n",
    "print(y_std)\n",
    "# --- 3. Load and Preprocess 4xCO2 Test Data ---\n",
    "print(\"\\nLoading and preprocessing 4xCO2 test data...\")\n",
    "ds_4xCO2_X = xr.open_dataset(file_4xCO2_input)\n",
    "ds_4xCO2_y = xr.open_dataset(file_4xCO2_output)\n",
    "\n",
    "# Extract variables from the correct files\n",
    "TS_4xCO2_raw = ds_4xCO2_X[In_name]\n",
    "TOA_4xCO2_truth = ds_4xCO2_y[Out_name].values\n",
    "lat = ds_4xCO2_X['lat'].values\n",
    "time_4xCO2 = ds_4xCO2_X['year'].values if 'year' in ds_4xCO2_X else ds_4xCO2_X['time'].values\n",
    "\n",
    "# Normalize inputs correctly by adding the channel dimension first\n",
    "TS_4xCO2_norm = (TS_4xCO2_raw.values[..., np.newaxis] - X_mean) / X_std\n",
    "print(\"✅ Test data preprocessed.\")\n",
    "\n",
    "# --- 4. Prediction Loop ---\n",
    "print(\"\\n--- Running Ensemble Predictions for 4xCO2 ---\")\n",
    "n_folds = 5\n",
    "predictions_from_folds = []\n",
    "\n",
    "for fold_no in range(1, n_folds + 1):\n",
    "    K.clear_session(); gc.collect()\n",
    "    \n",
    "    # Use the corrected, direct path to load the model\n",
    "    model_path = os.path.join(path_model_dir, f'model_fold{fold_no}_ens1.h5')\n",
    "    print(f\"    Loading and predicting with model: {model_path}\")\n",
    "    \n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    pred_4xco2_norm = model.predict(TS_4xCO2_norm)\n",
    "    pred_4xco2_unnorm = pred_4xco2_norm * y_std + y_mean\n",
    "    predictions_from_folds.append(pred_4xco2_unnorm)\n",
    "\n",
    "# Average predictions across the folds\n",
    "Model_pred_4xco2 = np.mean(np.stack(predictions_from_folds), axis=0)\n",
    "print(\"\\n✅ Prediction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c60dd36-d92c-4163-b95f-971daad94051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Out-of-Sample Analysis and Visualization (Corrected Plotting Axis)\n",
    "# ==========================================================\n",
    "print(\"\\n--- Starting Out-of-Sample Analysis ---\")\n",
    "\n",
    "# --- Task 1: Calculate Overall Pattern Correlation ---\n",
    "print(\"\\n    Calculating Overall Pattern Correlation...\")\n",
    "truth_flat = TOA_4xCO2_truth.flatten()\n",
    "pred_flat = Model_pred_4xco2.flatten()\n",
    "pattern_r, _ = pearsonr(truth_flat, pred_flat)\n",
    "print(f\"✅ Overall Pattern Correlation (r) = {pattern_r:.4f}\")\n",
    "\n",
    "# --- Task 2: Plot R-squared as a Function of Latitude ---\n",
    "print(\"\\n    Calculating and plotting R-squared per latitude...\")\n",
    "r2_by_latitude = [r2_score(TOA_4xCO2_truth[:, i], Model_pred_4xco2[:, i]) for i in range(len(lat))]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lat, r2_by_latitude, marker='o', linestyle='-')\n",
    "plt.title('Out-of-Sample Performance (R²) by Latitude - 4xCO2', fontsize=16)\n",
    "plt.xlabel('Latitude', fontsize=12)\n",
    "plt.ylabel('R-squared Score', fontsize=12)\n",
    "plt.grid(True, linestyle='--'); plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# --- Task 3: Plot Truth vs. Prediction as a 2D Contour Map ---\n",
    "print(\"\\n    Plotting Truth vs. Prediction as contour maps...\")\n",
    "\n",
    "# This creates a simple numerical axis [0, 1, 2, ...] for plotting\n",
    "time_axis_for_plot = np.arange(TOA_4xCO2_truth.shape[0])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6), sharey=True)\n",
    "vmax = np.percentile(np.abs(TOA_4xCO2_truth), 98)\n",
    "vmin = -vmax\n",
    "\n",
    "axes[0].set_title('Ground Truth TOA Zonal Mean', fontsize=16)\n",
    "# Use the new simple time axis for plotting\n",
    "cf1 = axes[0].contourf(time_axis_for_plot, lat, TOA_4xCO2_truth.T, levels=20, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
    "axes[0].set_xlabel('Time (Model Years)', fontsize=12)\n",
    "axes[0].set_ylabel('Latitude', fontsize=12)\n",
    "\n",
    "axes[1].set_title('Predicted TOA Zonal Mean', fontsize=16)\n",
    "# Use the new simple time axis for plotting here as well\n",
    "cf2 = axes[1].contourf(time_axis_for_plot, lat, Model_pred_4xco2.T, levels=20, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
    "axes[1].set_xlabel('Time (Model Years)', fontsize=12)\n",
    "\n",
    "fig.colorbar(cf1, ax=axes.ravel().tolist(), shrink=0.8, label='TOA Anomaly (W/m²)')\n",
    "fig.suptitle(\"Out-of-Sample Results - 4xCO2 Scenario\", fontsize=18, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470f59c-3312-4733-9169-81bb557753ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Task 4: Calculate and Compare Global Mean Time Series\n",
    "# =========================================================\n",
    "print(\"\\n--- Calculating and Comparing Weighted Global Means ---\")\n",
    "\n",
    "# --- Step 1: Calculate Latitude Weights ---\n",
    "# To get a true global mean, we must weight each latitude by the cosine\n",
    "# of its angle to account for the smaller grid cell areas near the poles.\n",
    "lat_radians = np.deg2rad(lat)\n",
    "weights = np.cos(lat_radians)\n",
    "# Ensure weights have the correct shape for broadcasting during the average\n",
    "weights = weights[np.newaxis, :]\n",
    "\n",
    "# --- Step 2: Calculate Weighted Average for Truth and Prediction ---\n",
    "# We average over the latitude axis (axis=1) to get a single global\n",
    "# mean value for each time step.\n",
    "global_mean_truth = np.average(TOA_4xCO2_truth, axis=1, weights=weights.flatten())\n",
    "global_mean_pred = np.average(Model_pred_4xco2, axis=1, weights=weights.flatten())\n",
    "\n",
    "print(\"✅ Weighted global means calculated.\")\n",
    "\n",
    "# --- Step 3: Print and Compare the Overall Mean Values ---\n",
    "# This gives a single number summary of the entire time series\n",
    "print(f\"    Overall Mean of Ground Truth: {np.mean(global_mean_truth):.4f} W/m²\")\n",
    "print(f\"    Overall Mean of Prediction:   {np.mean(global_mean_pred):.4f} W/m²\")\n",
    "\n",
    "# --- Step 4: Plot the Global Mean Time Series for Comparison ---\n",
    "print(\"\\n    Plotting global mean time series comparison...\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "time_axis_for_plot = np.arange(global_mean_truth.shape[0])\n",
    "\n",
    "plt.plot(time_axis_for_plot, global_mean_truth, label='Ground Truth', color='black', linewidth=2)\n",
    "plt.plot(time_axis_for_plot, global_mean_pred, label='Model Prediction', color='red', linestyle='--')\n",
    "\n",
    "plt.title('Out-of-Sample: Global Mean TOA Anomaly Time Series', fontsize=16)\n",
    "plt.xlabel('Time (Model Years)', fontsize=12)\n",
    "plt.ylabel('Global Mean Anomaly (W/m²)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025f18b-9ebb-4058-ae42-5357b6e20c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Task 4: Calculate and Compare Global Mean Time Series\n",
    "# =========================================================\n",
    "print(\"\\n--- Calculating and Comparing Weighted Global Means ---\")\n",
    "\n",
    "# --- Step 1: Calculate Latitude Weights ---\n",
    "# To get a true global mean, we must weight each latitude by the cosine\n",
    "# of its angle to account for the smaller grid cell areas near the poles.\n",
    "lat_radians = np.deg2rad(lat)\n",
    "weights = np.cos(lat_radians)\n",
    "# Ensure weights have the correct shape for broadcasting during the average\n",
    "weights = weights[np.newaxis, :]\n",
    "\n",
    "# --- Step 2: Calculate Weighted Average for Truth and Prediction ---\n",
    "# We average over the latitude axis (axis=1) to get a single global\n",
    "# mean value for each time step.\n",
    "global_mean_truth = np.average(TOA_4xCO2_truth, axis=1, weights=weights.flatten())\n",
    "global_mean_pred = np.average(Model_pred_4xco2, axis=1, weights=weights.flatten())\n",
    "\n",
    "print(\"✅ Weighted global means calculated.\")\n",
    "\n",
    "# --- Step 3: Print and Compare the Overall Mean Values ---\n",
    "# This gives a single number summary of the entire time series\n",
    "print(f\"    Overall Mean of Ground Truth: {np.mean(global_mean_truth):.4f} W/m²\")\n",
    "print(f\"    Overall Mean of Prediction:   {np.mean(global_mean_pred):.4f} W/m²\")\n",
    "\n",
    "# --- Step 4: Plot the Global Mean Time Series for Comparison ---\n",
    "print(\"\\n    Plotting global mean time series comparison...\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "time_axis_for_plot = np.arange(global_mean_truth.shape[0])\n",
    "\n",
    "plt.plot(time_axis_for_plot, global_mean_truth, label='Ground Truth', color='black', linewidth=2)\n",
    "plt.plot(time_axis_for_plot, global_mean_pred, label='Model Prediction', color='red', linestyle='--')\n",
    "\n",
    "plt.title('Out-of-Sample: Global Mean TOA Anomaly Time Series', fontsize=16)\n",
    "plt.xlabel('Time (Model Years)', fontsize=12)\n",
    "plt.ylabel('Global Mean Anomaly (W/m²)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dbe86a-9ace-4a66-893d-6231e0ecefc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# Task 5: Calculate and Plot Climate Feedback Parameter (λ)\n",
    "# =================================================================\n",
    "from scipy.stats import linregress\n",
    "\n",
    "print(\"\\n--- Calculating and Plotting Climate Feedback Parameter (λ) ---\")\n",
    "\n",
    "# --- Step 1: Calculate the X-axis data (Area-Weighted Global Mean TS Anomaly) ---\n",
    "# We already have the latitude weights from the previous task.\n",
    "# First, take the mean across the longitude axis of the raw input data.\n",
    "TS_zonal_mean_truth = np.mean(TS_4xCO2_raw.values, axis=2)\n",
    "\n",
    "# Now, calculate the latitude-weighted average to get the global mean.\n",
    "global_mean_TS_truth = np.average(TS_zonal_mean_truth, axis=1, weights=weights.flatten())\n",
    "\n",
    "# --- Step 2: Set up the side-by-side plots ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8), sharey=True)\n",
    "fig.suptitle('Climate Feedback Parameter (λ) - 4xCO2 Out-of-Sample', fontsize=20, weight='bold')\n",
    "\n",
    "# --- Step 3: Plot for the Ground Truth ---\n",
    "ax1 = axes[0]\n",
    "# Perform linear regression to find the slope (lambda)\n",
    "slope_truth, intercept_truth, r_value_truth, _, _ = linregress(global_mean_TS_truth, global_mean_truth)\n",
    "lambda_truth = slope_truth\n",
    "r2_truth = r_value_truth**2\n",
    "\n",
    "# Scatter plot\n",
    "ax1.scatter(global_mean_TS_truth, global_mean_truth, alpha=0.6, label='Yearly Data (Ground Truth)')\n",
    "# Best-fit line\n",
    "fit_line_truth = slope_truth * global_mean_TS_truth + intercept_truth\n",
    "ax1.plot(global_mean_TS_truth, fit_line_truth, color='red', linestyle='--', label='Linear Best Fit')\n",
    "\n",
    "# Add text box with results\n",
    "text_truth = (f'λ = {lambda_truth:.3f} W/m²/K\\n'\n",
    "              f'$R^2$ = {r2_truth:.3f}')\n",
    "ax1.text(0.05, 0.95, text_truth, transform=ax1.transAxes, fontsize=14,\n",
    "         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "ax1.set_title('Ground Truth Data', fontsize=16)\n",
    "ax1.set_xlabel('Area-Weighted Global Mean TS Anomaly (K)', fontsize=12)\n",
    "ax1.set_ylabel('Global Mean TOA Anomaly (W/m²)', fontsize=12)\n",
    "ax1.grid(True, linestyle=':')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "# --- Step 4: Plot for the Model Prediction ---\n",
    "ax2 = axes[1]\n",
    "# Perform linear regression\n",
    "slope_pred, intercept_pred, r_value_pred, _, _ = linregress(global_mean_TS_truth, global_mean_pred)\n",
    "lambda_pred = slope_pred\n",
    "r2_pred = r_value_pred**2\n",
    "\n",
    "# Scatter plot\n",
    "ax2.scatter(global_mean_TS_truth, global_mean_pred, alpha=0.6, label='Yearly Data (Model Prediction)')\n",
    "# Best-fit line\n",
    "fit_line_pred = slope_pred * global_mean_TS_truth + intercept_pred\n",
    "ax2.plot(global_mean_TS_truth, fit_line_pred, color='red', linestyle='--', label='Linear Best Fit')\n",
    "\n",
    "# Add text box with results\n",
    "text_pred = (f'λ = {lambda_pred:.3f} W/m²/K\\n'\n",
    "             f'$R^2$ = {r2_pred:.3f}')\n",
    "ax2.text(0.05, 0.95, text_pred, transform=ax2.transAxes, fontsize=14,\n",
    "         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "ax2.set_title('Model Prediction', fontsize=16)\n",
    "ax2.set_xlabel('Area-Weighted Global Mean TS Anomaly (K)', fontsize=12)\n",
    "# No Y-label needed as it's shared with ax1\n",
    "ax2.grid(True, linestyle=':')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to make room for suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e74817-43af-4398-a960-f0860a2e8879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# NEW ANALYSIS: Global Mean of Zonal-Mean Prediction vs. Truth\n",
    "# ==========================================================\n",
    "print(\"\\n--- Starting new analysis: Comparing the area-weighted global mean ---\")\n",
    "\n",
    "# --- 1. Helper Function for Area-Weighted Mean ---\n",
    "# This function correctly calculates the global mean from zonal-mean [time, lat] data.\n",
    "def calculate_area_weighted_global_mean_zonal(data_2d, lat_coords):\n",
    "    \"\"\"\n",
    "    Calculates the area-weighted global mean from a [time, lat] array.\n",
    "    \"\"\"\n",
    "    # The weights for each latitude band are proportional to the cosine of the latitude.\n",
    "    weights = np.cos(np.deg2rad(lat_coords))\n",
    "    # np.average calculates the weighted average over the latitude axis (axis=1).\n",
    "    global_mean_timeseries = np.average(data_2d, axis=1, weights=weights)\n",
    "    return global_mean_timeseries\n",
    "\n",
    "# --- 2. Calculate the Global Mean Time Series ---\n",
    "# This assumes 'TOA_4xCO2_truth' and 'Model_pred_4xco2' are in memory from the previous cell.\n",
    "# It also assumes 'lat' (your latitude coordinate array) is in memory.\n",
    "\n",
    "print(\"    Calculating global mean for both truth and prediction...\")\n",
    "global_mean_truth = calculate_area_weighted_global_mean_zonal(TOA_4xCO2_truth, lat)\n",
    "global_mean_pred = calculate_area_weighted_global_mean_zonal(Model_pred_4xco2, lat)\n",
    "\n",
    "# --- 3. Plot the Comparison Graph ---\n",
    "print(\"    Plotting the global mean comparison graph...\")\n",
    "\n",
    "# Calculate the R-squared score for the global mean time series\n",
    "r2_global_mean = r2_score(global_mean_truth, global_mean_pred)\n",
    "\n",
    "# === THIS IS THE FIX ===\n",
    "# Instead of using the complex time objects from the file, we create a simple numerical axis.\n",
    "# This will be an array like [0, 1, 2, ...] that has the correct length for plotting.\n",
    "time_axis_for_plot = np.arange(len(global_mean_truth))\n",
    "# ======================\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Use the new, simple time axis for plotting\n",
    "ax.plot(time_axis_for_plot, global_mean_truth, label=\"Truth (Global Mean)\", color=\"k\", linewidth=2.5)\n",
    "ax.plot(time_axis_for_plot, global_mean_pred, label=\"Prediction (Global Mean)\", color=\"C3\", linestyle='--')\n",
    "\n",
    "# Add R-squared annotation\n",
    "ax.text(0.02, 0.95, f\"$R^2$ = {r2_global_mean:.3f}\", transform=ax.transAxes,\n",
    "        fontsize=16, bbox=dict(facecolor=\"white\", edgecolor=\"black\", alpha=0.7))\n",
    "\n",
    "# Style the plot\n",
    "ax.set_xlabel(\"Time (Model Years)\", fontsize=16) # Label updated for clarity\n",
    "ax.set_ylabel(\"Global Mean TOA Anomaly (W/m²)\", fontsize=16)\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "ax.tick_params(axis='both', labelsize=14)\n",
    "ax.set_title(\"Model Performance on Global Mean (from Zonal-Mean Prediction)\", fontsize=18, pad=15)\n",
    "ax.legend(fontsize=14, loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf210)",
   "language": "python",
   "name": "tf210"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
